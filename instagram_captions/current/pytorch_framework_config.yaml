# =============================================================================
# PYTORCH PRIMARY FRAMEWORK CONFIGURATION
# =============================================================================
# Configuration file for PyTorch Primary Framework System
# This file contains all the settings and parameters for optimal PyTorch performance

# =============================================================================
# FRAMEWORK CONFIGURATION
# =============================================================================
framework:
  # Primary framework settings
  use_cuda: true
  use_mixed_precision: true
  use_distributed: false
  use_data_parallel: false
  use_gradient_checkpointing: true
  
  # PyTorch version compatibility
  min_pytorch_version: "1.12.0"
  target_pytorch_version: "2.1.0"
  
  # Framework type
  framework_type: "pytorch"
  framework_priority: "primary"

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================
performance:
  # CUDNN optimizations
  enable_cudnn_benchmark: true
  enable_cudnn_deterministic: false
  enable_tf32: true
  memory_efficient_attention: true
  
  # Memory optimizations
  use_channels_last: false
  enable_memory_efficient_forward: true
  use_amp: true
  
  # Model compilation (PyTorch 2.0+)
  compile_model: true
  compile_mode: "reduce-overhead"  # Options: "default", "reduce-overhead", "max-autotune"
  
  # Gradient optimization
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  use_gradient_scaling: true

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  # Data loading
  batch_size: 32
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  
  # Training loop
  epochs: 100
  steps_per_epoch: null  # Auto-calculate if null
  validation_steps: null  # Auto-calculate if null
  
  # Early stopping
  early_stopping_patience: 10
  early_stopping_min_delta: 0.001
  
  # Checkpointing
  save_checkpoint_every: 5
  keep_checkpoints: 3
  checkpoint_dir: "./checkpoints/pytorch_framework"

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
optimizer:
  # Optimizer type and settings
  type: "adamw"  # Options: "adamw", "adam", "sgd", "lion", "adafactor"
  
  # Learning rate settings
  learning_rate: 1e-4
  min_lr: 1e-6
  max_lr: 1e-3
  
  # Weight decay
  weight_decay: 1e-5
  
  # Beta parameters (for Adam-based optimizers)
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  
  # SGD specific settings
  momentum: 0.9
  nesterov: true

# =============================================================================
# SCHEDULER CONFIGURATION
# =============================================================================
scheduler:
  # Scheduler type
  type: "cosine"  # Options: "cosine", "step", "plateau", "onecycle", "warmup_cosine"
  
  # Cosine annealing settings
  T_max: 100
  eta_min: 0
  
  # Step LR settings
  step_size: 30
  gamma: 0.1
  
  # Plateau settings
  mode: "min"
  factor: 0.5
  patience: 10
  threshold: 1e-4
  
  # OneCycle settings
  max_lr: 1e-3
  steps_per_epoch: 100
  
  # Warmup settings
  warmup_epochs: 5
  warmup_start_lr: 1e-6

# =============================================================================
# MODEL ARCHITECTURE
# =============================================================================
model:
  # Model type
  type: "transformer"  # Options: "transformer", "cnn", "rnn", "mlp"
  
  # Transformer specific settings
  transformer:
    vocab_size: 30000
    d_model: 512
    n_heads: 8
    n_layers: 6
    d_ff: 2048
    max_seq_len: 512
    dropout: 0.1
    activation: "gelu"
    norm_first: true
    use_relative_position: false
    
  # CNN specific settings
  cnn:
    input_channels: 3
    num_classes: 1000
    base_channels: 64
    num_layers: 5
    use_batch_norm: true
    use_dropout: true
    
  # RNN specific settings
  rnn:
    input_size: 512
    hidden_size: 512
    num_layers: 2
    dropout: 0.1
    bidirectional: false
    rnn_type: "lstm"  # Options: "lstm", "gru", "rnn"

# =============================================================================
# LOSS FUNCTION
# =============================================================================
loss:
  # Loss type
  type: "cross_entropy"  # Options: "cross_entropy", "mse", "mae", "focal", "dice"
  
  # Cross entropy specific
  cross_entropy:
    ignore_index: -100
    reduction: "mean"
    label_smoothing: 0.1
    
  # Focal loss specific
  focal:
    alpha: 1.0
    gamma: 2.0
    
  # Dice loss specific
  dice:
    smooth: 1.0
    square_denominator: false

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
data:
  # Dataset settings
  dataset_type: "custom"  # Options: "custom", "torchvision", "huggingface"
  
  # Data augmentation
  augmentation:
    use_augmentation: true
    random_crop: true
    random_horizontal_flip: true
    random_rotation: 15
    color_jitter: true
    normalize: true
    
  # Data preprocessing
  preprocessing:
    resize: [224, 224]
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
    
  # Data splitting
  split_ratio: [0.8, 0.1, 0.1]  # Train, Val, Test
  random_seed: 42

# =============================================================================
# MONITORING AND LOGGING
# =============================================================================
monitoring:
  # TensorBoard
  use_tensorboard: true
  tensorboard_log_dir: "./runs/pytorch_framework"
  
  # Profiling
  use_profiler: false
  profiler_log_dir: "./runs/profiler"
  
  # Memory monitoring
  log_memory_usage: true
  memory_log_interval: 100
  
  # Metrics logging
  log_metrics_every: 10
  log_gradients: false
  log_learning_rate: true
  
  # Visualization
  save_plots: true
  plot_interval: 50

# =============================================================================
# EXPORT AND DEPLOYMENT
# =============================================================================
export:
  # Model export formats
  formats:
    - "torchscript"
    - "onnx"
    - "pytorch"
  
  # Export settings
  torchscript:
    use_trace: true
    use_script: false
    
  onnx:
    opset_version: 11
    do_constant_folding: true
    dynamic_axes: true
    
  # Export directory
  export_dir: "./exports"

# =============================================================================
# DISTRIBUTED TRAINING
# =============================================================================
distributed:
  # Distributed training settings
  use_distributed: false
  backend: "nccl"  # Options: "nccl", "gloo", "mpi"
  
  # Multi-GPU settings
  num_gpus: 1
  use_data_parallel: false
  use_distributed_data_parallel: false
  
  # Distributed optimizer
  use_distributed_optimizer: false
  bucket_cap_mb: 25
  
  # Communication
  find_unused_parameters: false
  broadcast_buffers: true

# =============================================================================
# DEBUGGING AND TESTING
# =============================================================================
debug:
  # Debug mode
  debug_mode: false
  
  # Deterministic training
  deterministic: false
  seed: 42
  
  # Gradient checking
  check_gradients: false
  grad_check_eps: 1e-6
  
  # Memory debugging
  track_memory: false
  memory_fraction: 0.9
  
  # Exception handling
  catch_exceptions: true
  verbose_logging: false

# =============================================================================
# ENVIRONMENT SPECIFIC SETTINGS
# =============================================================================
environment:
  # Development environment
  dev:
    use_cuda: true
    batch_size: 16
    num_workers: 2
    use_profiler: true
    
  # Production environment
  prod:
    use_cuda: true
    batch_size: 64
    num_workers: 8
    use_profiler: false
    
  # Testing environment
  test:
    use_cuda: false
    batch_size: 8
    num_workers: 1
    use_profiler: false

# =============================================================================
# ADVANCED SETTINGS
# =============================================================================
advanced:
  # Mixed precision
  mixed_precision:
    use_amp: true
    dtype: "float16"
    scaler_type: "grad_scaler"
    
  # Gradient checkpointing
  gradient_checkpointing:
    use_checkpointing: true
    checkpoint_every: 1
    
  # Memory optimization
  memory_optimization:
    use_memory_efficient_attention: true
    use_flash_attention: false
    use_xformers: false
    
  # Performance tuning
  performance_tuning:
    use_channels_last: false
    use_tf32: true
    use_cudnn_benchmark: true
    use_jit_compilation: true

# =============================================================================
# VALIDATION RULES
# =============================================================================
validation:
  # Configuration validation
  validate_config: true
  
  # Required fields
  required_fields:
    - "framework.framework_type"
    - "training.batch_size"
    - "optimizer.type"
    - "model.type"
    
  # Value ranges
  value_ranges:
    learning_rate:
      min: 1e-8
      max: 1.0
    batch_size:
      min: 1
      max: 1024
    epochs:
      min: 1
      max: 10000

# =============================================================================
# NOTES AND COMMENTS
# =============================================================================
# This configuration file is designed for PyTorch as the primary framework
# All settings are optimized for PyTorch best practices and performance
# 
# Key features:
# - Comprehensive PyTorch optimization settings
# - Multiple model architecture support
# - Advanced training configurations
# - Memory and performance optimizations
# - Export and deployment options
# - Environment-specific configurations
# 
# Usage:
# 1. Load this configuration in your PyTorch framework system
# 2. Modify values according to your specific needs
# 3. Validate configuration before training
# 4. Use environment-specific overrides as needed


