# Deep Learning and Model Development System

Sistema completo para implementar flujos de trabajo de deep learning y mejores pr√°cticas para desarrollo de modelos.

## üöÄ Caracter√≠sticas

### **Comprehensive Model Development**
- **Multiple Architectures**: Transformer, CNN, RNN, Hybrid models
- **Flexible Configuration**: Configuraci√≥n configurable para diferentes tipos de modelos
- **Modular Design**: Dise√±o modular y reutilizable
- **Production Ready**: Sistema listo para producci√≥n

### **Advanced Training Features**
- **Mixed Precision Training**: Entrenamiento con precisi√≥n mixta (AMP)
- **Gradient Accumulation**: Acumulaci√≥n de gradientes para batches grandes
- **Gradient Clipping**: Clipping de gradientes para estabilidad
- **Learning Rate Scheduling**: Programaci√≥n autom√°tica de learning rate
- **Multiple Optimizers**: Adam, SGD, AdamW con configuraci√≥n flexible

### **Monitoring and Logging**
- **TensorBoard Integration**: Integraci√≥n completa con TensorBoard
- **Weights & Biases**: Soporte para experiment tracking con W&B
- **Real-time Metrics**: M√©tricas en tiempo real durante entrenamiento
- **Training Visualization**: Visualizaci√≥n completa del historial de entrenamiento

### **Model Architectures**
- **Transformer Models**: Modelos transformer para procesamiento de secuencias
- **CNN Models**: Modelos CNN para procesamiento de im√°genes
- **RNN Models**: Modelos RNN/LSTM para secuencias
- **Hybrid Models**: Modelos h√≠bridos combinando m√∫ltiples arquitecturas

## üì¶ Instalaci√≥n

```bash
pip install -r requirements_deep_learning_model_development.txt
```

## üéØ Uso B√°sico

### Configuraci√≥n del Sistema

```python
from deep_learning_model_development_system import (
    ModelDevelopmentConfig, DeepLearningModelDevelopmentSystem
)

# Configuraci√≥n para modelo Transformer
config = ModelDevelopmentConfig(
    model_type="transformer",
    architecture_config={
        'vocab_size': 30000,
        'd_model': 512,
        'n_heads': 8,
        'n_layers': 6,
        'd_ff': 2048,
        'max_seq_len': 512,
        'dropout': 0.1
    },
    batch_size=32,
    learning_rate=1e-4,
    num_epochs=100,
    mixed_precision=True,
    gradient_accumulation_steps=2,
    use_tensorboard=True,
    use_wandb=False
)

# Crear sistema de desarrollo
dl_system = DeepLearningModelDevelopmentSystem(config)
```

### Entrenamiento de Modelo

```python
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss

# Crear dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Funci√≥n de p√©rdida
criterion = CrossEntropyLoss()

# Entrenar modelo
history = dl_system.train_model(train_loader, val_loader, criterion)

# Evaluar modelo
metrics = dl_system.evaluate_model(test_loader, criterion)

# Visualizar historial de entrenamiento
dl_system.plot_training_history(history)
```

### Diferentes Tipos de Modelos

```python
# Modelo CNN
cnn_config = ModelDevelopmentConfig(
    model_type="cnn",
    architecture_config={
        'input_channels': 3,
        'num_classes': 1000,
        'base_channels': 64,
        'num_layers': 5
    },
    batch_size=64,
    learning_rate=1e-3
)

# Modelo RNN
rnn_config = ModelDevelopmentConfig(
    model_type="rnn",
    architecture_config={
        'input_size': 512,
        'hidden_size': 256,
        'num_layers': 2,
        'num_classes': 1000,
        'bidirectional': True
    },
    batch_size=32,
    learning_rate=1e-4
)

# Modelo H√≠brido
hybrid_config = ModelDevelopmentConfig(
    model_type="hybrid",
    architecture_config={
        'transformer': {
            'vocab_size': 30000,
            'd_model': 256,
            'n_heads': 4,
            'n_layers': 3
        },
        'cnn': {
            'input_channels': 3,
            'num_classes': 1000,
            'base_channels': 32,
            'num_layers': 3
        },
        'fusion_method': 'concatenate'
    },
    batch_size=16,
    learning_rate=1e-4
)
```

## üîß Componentes Principales

### **DeepLearningModelDevelopmentSystem**
Sistema principal para desarrollo de modelos:

```python
from deep_learning_model_development_system import DeepLearningModelDevelopmentSystem

# Crear sistema
dl_system = DeepLearningModelDevelopmentSystem(config)

# Entrenar modelo
history = dl_system.train_model(train_loader, val_loader, criterion)

# Evaluar modelo
metrics = dl_system.evaluate_model(test_loader, criterion)

# Guardar checkpoint
dl_system._save_checkpoint('best_model.pth')

# Cargar checkpoint
dl_system.load_checkpoint('best_model.pth')

# Limpiar recursos
dl_system.cleanup()
```

### **ModelDevelopmentConfig**
Configuraci√≥n completa para desarrollo de modelos:

```python
from deep_learning_model_development_system import ModelDevelopmentConfig

config = ModelDevelopmentConfig(
    # Arquitectura del modelo
    model_type="transformer",  # transformer, cnn, rnn, hybrid
    architecture_config={
        'vocab_size': 30000,
        'd_model': 512,
        'n_heads': 8,
        'n_layers': 6
    },
    
    # Configuraci√≥n de entrenamiento
    batch_size=32,
    learning_rate=1e-4,
    num_epochs=100,
    weight_decay=1e-5,
    gradient_clip_norm=1.0,
    
    # Optimizaci√≥n
    optimizer_type="adamw",  # adam, sgd, adamw
    scheduler_type="cosine",  # step, cosine, plateau
    mixed_precision=True,
    gradient_accumulation_steps=2,
    
    # Datos
    train_split=0.8,
    val_split=0.1,
    test_split=0.1,
    num_workers=4,
    pin_memory=True,
    
    # Monitoreo
    log_interval=100,
    eval_interval=1000,
    save_interval=5000,
    use_tensorboard=True,
    use_wandb=False
)
```

### **Model Architectures**

#### **TransformerModel**
```python
from deep_learning_model_development_system import TransformerModel

transformer = TransformerModel(
    vocab_size=30000,
    d_model=512,
    n_heads=8,
    n_layers=6,
    d_ff=2048,
    max_seq_len=512,
    dropout=0.1
)

# Forward pass
input_tokens = torch.randint(0, 30000, (32, 100))
output_logits = transformer(input_tokens)
```

#### **CNNModel**
```python
from deep_learning_model_development_system import CNNModel

cnn = CNNModel(
    input_channels=3,
    num_classes=1000,
    base_channels=64,
    num_layers=5
)

# Forward pass
input_images = torch.randn(32, 3, 224, 224)
output_logits = cnn(input_images)
```

#### **RNNModel**
```python
from deep_learning_model_development_system import RNNModel

rnn = RNNModel(
    input_size=512,
    hidden_size=256,
    num_layers=2,
    num_classes=1000,
    bidirectional=True
)

# Forward pass
input_sequences = torch.randn(32, 100, 512)
output_logits = rnn(input_sequences)
```

#### **HybridModel**
```python
from deep_learning_model_development_system import HybridModel

hybrid = HybridModel(
    transformer_config={
        'vocab_size': 30000,
        'd_model': 256,
        'n_heads': 4,
        'n_layers': 3
    },
    cnn_config={
        'input_channels': 3,
        'num_classes': 1000,
        'base_channels': 32,
        'num_layers': 3
    },
    fusion_method='concatenate'
)
```

## üìä Caracter√≠sticas Avanzadas

### **Mixed Precision Training**
```python
# Configurar precisi√≥n mixta
config = ModelDevelopmentConfig(
    mixed_precision=True,  # Habilitar AMP
    gradient_accumulation_steps=4
)

# El sistema autom√°ticamente:
# - Usa autocast para forward pass
# - Escala la p√©rdida con GradScaler
# - Maneja la acumulaci√≥n de gradientes
# - Aplica clipping de gradientes
```

### **Gradient Accumulation**
```python
# Para batches grandes que no caben en memoria
config = ModelDevelopmentConfig(
    batch_size=8,  # Batch size peque√±o
    gradient_accumulation_steps=4,  # Acumular 4 pasos
    # Efectivo: batch_size = 8 * 4 = 32
)
```

### **Learning Rate Scheduling**
```python
# Diferentes tipos de schedulers
config = ModelDevelopmentConfig(
    scheduler_type="cosine",  # Cosine annealing
    # Alternativas: "step", "plateau"
)

# El sistema autom√°ticamente:
# - Crea el scheduler apropiado
# - Actualiza el learning rate
# - Maneja ReduceLROnPlateau con m√©tricas de validaci√≥n
```

### **Advanced Monitoring**
```python
# Configurar monitoreo completo
config = ModelDevelopmentConfig(
    use_tensorboard=True,
    use_wandb=True,
    log_interval=100,
    eval_interval=1000,
    save_interval=5000
)

# El sistema autom√°ticamente:
# - Registra m√©tricas en TensorBoard
# - Sincroniza con Weights & Biases
# - Guarda checkpoints regulares
# - Monitorea el mejor modelo
```

## üìà Visualizaci√≥n y An√°lisis

### **Training History Plots**
```python
# Visualizar historial completo de entrenamiento
history = dl_system.train_model(train_loader, val_loader, criterion)

dl_system.plot_training_history(
    history,
    save_path='training_history.png'
)

# Genera 4 gr√°ficos:
# 1. Training vs Validation Loss
# 2. Learning Rate over time
# 3. Train-Val Loss Difference
# 4. Validation Loss Trend
```

### **TensorBoard Integration**
```python
# Monitoreo en tiempo real
config = ModelDevelopmentConfig(use_tensorboard=True)

# M√©tricas registradas autom√°ticamente:
# - Loss/Train
# - Loss/Validation
# - Learning_Rate
# - Loss/Training_Step

# Acceder a TensorBoard:
# tensorboard --logdir=./runs
```

### **Weights & Biases Integration**
```python
# Experiment tracking
config = ModelDevelopmentConfig(use_wandb=True)

# El sistema autom√°ticamente:
# - Inicializa W&B
# - Registra hiperpar√°metros
# - Logs m√©tricas en tiempo real
# - Sincroniza checkpoints
```

## üèóÔ∏è Arquitectura del Sistema

```
DeepLearningModelDevelopmentSystem
‚îú‚îÄ‚îÄ ModelDevelopmentConfig
‚îÇ   ‚îú‚îÄ‚îÄ Model Architecture Configuration
‚îÇ   ‚îú‚îÄ‚îÄ Training Configuration
‚îÇ   ‚îú‚îÄ‚îÄ Optimization Configuration
‚îÇ   ‚îú‚îÄ‚îÄ Data Configuration
‚îÇ   ‚îî‚îÄ‚îÄ Monitoring Configuration
‚îú‚îÄ‚îÄ Model Creation
‚îÇ   ‚îú‚îÄ‚îÄ TransformerModel
‚îÇ   ‚îú‚îÄ‚îÄ CNNModel
‚îÇ   ‚îú‚îÄ‚îÄ RNNModel
‚îÇ   ‚îî‚îÄ‚îÄ HybridModel
‚îú‚îÄ‚îÄ Training Pipeline
‚îÇ   ‚îú‚îÄ‚îÄ Mixed Precision Training
‚îÇ   ‚îú‚îÄ‚îÄ Gradient Accumulation
‚îÇ   ‚îú‚îÄ‚îÄ Learning Rate Scheduling
‚îÇ   ‚îî‚îÄ‚îÄ Checkpoint Management
‚îú‚îÄ‚îÄ Monitoring & Logging
‚îÇ   ‚îú‚îÄ‚îÄ TensorBoard Integration
‚îÇ   ‚îú‚îÄ‚îÄ Weights & Biases
‚îÇ   ‚îú‚îÄ‚îÄ Real-time Metrics
‚îÇ   ‚îî‚îÄ‚îÄ Training Visualization
‚îî‚îÄ‚îÄ Model Evaluation
    ‚îú‚îÄ‚îÄ Test Set Evaluation
    ‚îú‚îÄ‚îÄ Metrics Calculation
    ‚îî‚îÄ‚îÄ Performance Analysis
```

## üöÄ Casos de Uso

### **Large Language Models**
```python
# Configuraci√≥n para LLMs grandes
llm_config = ModelDevelopmentConfig(
    model_type="transformer",
    architecture_config={
        'vocab_size': 50000,
        'd_model': 1024,
        'n_heads': 16,
        'n_layers': 12,
        'd_ff': 4096,
        'max_seq_len': 1024,
        'dropout': 0.1
    },
    batch_size=8,
    gradient_accumulation_steps=8,  # Efectivo: 64
    mixed_precision=True,
    learning_rate=1e-5
)
```

### **Computer Vision Models**
```python
# Configuraci√≥n para modelos de visi√≥n
vision_config = ModelDevelopmentConfig(
    model_type="cnn",
    architecture_config={
        'input_channels': 3,
        'num_classes': 1000,
        'base_channels': 128,
        'num_layers': 6
    },
    batch_size=64,
    learning_rate=1e-3,
    optimizer_type="sgd",
    scheduler_type="step"
)
```

### **Sequence Models**
```python
# Configuraci√≥n para modelos de secuencia
sequence_config = ModelDevelopmentConfig(
    model_type="rnn",
    architecture_config={
        'input_size': 512,
        'hidden_size': 512,
        'num_layers': 4,
        'num_classes': 1000,
        'bidirectional': True
    },
    batch_size=32,
    learning_rate=1e-4,
    scheduler_type="plateau"
)
```

### **Hybrid Models**
```python
# Configuraci√≥n para modelos h√≠bridos
hybrid_config = ModelDevelopmentConfig(
    model_type="hybrid",
    architecture_config={
        'transformer': {
            'vocab_size': 30000,
            'd_model': 256,
            'n_heads': 4,
            'n_layers': 3
        },
        'cnn': {
            'input_channels': 3,
            'num_classes': 1000,
            'base_channels': 32,
            'num_layers': 3
        },
        'fusion_method': 'attention'
    },
    batch_size=16,
    learning_rate=1e-4
)
```

## üìö Mejores Pr√°cticas

### **Model Development Workflow**
1. **Configuration**: Definir configuraci√≥n completa del modelo
2. **Data Preparation**: Preparar dataloaders con splits apropiados
3. **Training**: Usar mixed precision y gradient accumulation
4. **Monitoring**: Monitorear m√©tricas en tiempo real
5. **Evaluation**: Evaluar en conjunto de test
6. **Analysis**: Analizar resultados y visualizaciones

### **Performance Optimization**
- **Mixed Precision**: Usar AMP para acelerar entrenamiento
- **Gradient Accumulation**: Para batches grandes
- **Learning Rate Scheduling**: Para convergencia estable
- **Gradient Clipping**: Para estabilidad de entrenamiento
- **Checkpointing**: Guardar modelos regularmente

### **Monitoring Best Practices**
- **Real-time Logging**: Logs cada 100 pasos
- **Validation**: Evaluar cada √©poca
- **Checkpointing**: Guardar cada 10 √©pocas
- **Visualization**: Gr√°ficos autom√°ticos de progreso
- **Experiment Tracking**: Usar TensorBoard y/o W&B

## üîç Troubleshooting

### **Common Issues**
1. **Out of Memory**: Reducir batch_size o usar gradient_accumulation_steps
2. **Training Instability**: Ajustar gradient_clip_norm y learning_rate
3. **Slow Convergence**: Verificar learning_rate y scheduler
4. **Overfitting**: Aumentar dropout y weight_decay

### **Performance Tips**
- Usar mixed precision para acelerar entrenamiento
- Ajustar num_workers para DataLoader
- Usar pin_memory=True para GPU
- Monitorear GPU memory usage

## üìà Ejemplos de Uso

### **Entrenamiento Completo**
```python
def complete_training_workflow():
    """Flujo de trabajo completo de entrenamiento"""
    
    # 1. Configuraci√≥n
    config = ModelDevelopmentConfig(
        model_type="transformer",
        architecture_config={
            'vocab_size': 30000,
            'd_model': 512,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 2048,
            'max_seq_len': 512,
            'dropout': 0.1
        },
        batch_size=32,
        learning_rate=1e-4,
        num_epochs=100,
        mixed_precision=True,
        gradient_accumulation_steps=2,
        use_tensorboard=True
    )
    
    # 2. Crear sistema
    dl_system = DeepLearningModelDevelopmentSystem(config)
    
    # 3. Preparar datos
    train_loader = create_dataloader(train_dataset, batch_size=32)
    val_loader = create_dataloader(val_dataset, batch_size=32)
    test_loader = create_dataloader(test_dataset, batch_size=32)
    
    # 4. Entrenar modelo
    criterion = nn.CrossEntropyLoss()
    history = dl_system.train_model(train_loader, val_loader, criterion)
    
    # 5. Evaluar modelo
    metrics = dl_system.evaluate_model(test_loader, criterion)
    
    # 6. Visualizar resultados
    dl_system.plot_training_history(history, 'training_results.png')
    
    # 7. Limpiar recursos
    dl_system.cleanup()
    
    return metrics, history
```

### **Fine-tuning de Modelo Existente**
```python
def fine_tune_existing_model():
    """Fine-tuning de modelo pre-entrenado"""
    
    # 1. Cargar modelo existente
    dl_system = DeepLearningModelDevelopmentSystem(config)
    dl_system.load_checkpoint('pretrained_model.pth')
    
    # 2. Ajustar configuraci√≥n para fine-tuning
    dl_system.config.learning_rate = 1e-5  # Learning rate m√°s bajo
    dl_system.config.num_epochs = 20        # Menos √©pocas
    
    # 3. Entrenar con nuevos datos
    history = dl_system.train_model(new_train_loader, new_val_loader, criterion)
    
    # 4. Guardar modelo fine-tuneado
    dl_system._save_checkpoint('fine_tuned_model.pth')
    
    return history
```

## ü§ù Contribuci√≥n

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/DeepLearning`)
3. Commit tus cambios (`git commit -m 'Add deep learning features'`)
4. Push a la rama (`git push origin feature/DeepLearning`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo `LICENSE` para m√°s detalles.

## üÜò Soporte

Para soporte t√©cnico o preguntas:
- Abre un issue en GitHub
- Consulta la documentaci√≥n
- Revisa los ejemplos de uso

---

**Sistema de Deep Learning and Model Development implementado exitosamente** ‚úÖ

**Caracter√≠sticas implementadas:**
- ‚úÖ Comprehensive model development
- ‚úÖ Multiple architectures (Transformer, CNN, RNN, Hybrid)
- ‚úÖ Advanced training features (AMP, gradient accumulation)
- ‚úÖ Flexible configuration system
- ‚úÖ Monitoring and logging (TensorBoard, W&B)
- ‚úÖ Training visualization
- ‚úÖ Checkpoint management
- ‚úÖ Model evaluation
- ‚úÖ Production ready code
- ‚úÖ Best practices implementation

**SISTEMA COMPLETAMENTE FUNCIONAL PARA PRODUCCI√ìN** üöÄ


