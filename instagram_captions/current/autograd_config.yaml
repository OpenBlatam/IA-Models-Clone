# PyTorch Autograd System Configuration
# This file configures the automatic differentiation capabilities

autograd_system:
  # Basic autograd settings
  enable_gradients: true
  create_graph: false
  retain_graph: false
  
  # Gradient monitoring
  gradient_monitoring:
    enabled: true
    save_gradient_history: true
    compute_statistics: true
    hook_registration: true
  
  # Advanced autograd features
  advanced_features:
    higher_order_gradients: true
    custom_gradients: true
    gradient_checkpointing: false
    mixed_precision: false
  
  # Performance settings
  performance:
    use_amp: false  # Automatic Mixed Precision
    use_compile: false  # torch.compile()
    use_channels_last: false
    memory_efficient: false

# Custom loss function configurations
custom_loss_functions:
  mse_l1_combined:
    alpha: 1.0
    beta: 0.1
    learnable_weights: true
  
  focal_loss:
    alpha: 1.0
    gamma: 2.0
    reduction: "mean"
  
  huber_loss:
    delta: 1.0
    reduction: "mean"

# Gradient monitoring configurations
gradient_monitoring:
  # Statistics to compute
  statistics:
    - mean
    - std
    - min
    - max
    - norm
    - norm_l1
  
  # History settings
  history:
    max_entries: 1000
    save_parameter_values: false
    save_gradient_values: false
  
  # Hook settings
  hooks:
    register_on_forward: false
    register_on_backward: true
    save_intermediate_gradients: false

# Training configurations with autograd
training_configs:
  basic_training:
    optimizer: "adam"
    learning_rate: 0.001
    weight_decay: 0.0001
    gradient_clipping: 1.0
    
  advanced_training:
    optimizer: "adamw"
    learning_rate: 0.0001
    weight_decay: 0.01
    gradient_clipping: 0.5
    use_amp: true
    
  research_training:
    optimizer: "lion"
    learning_rate: 0.00001
    weight_decay: 0.001
    gradient_clipping: 0.1
    higher_order_gradients: true

# Autograd debugging configurations
debugging:
  # Gradient checks
  gradient_checks:
    check_nan: true
    check_inf: true
    check_exploding: true
    check_vanishing: true
  
  # Thresholds
  thresholds:
    max_gradient_norm: 10.0
    min_gradient_norm: 1e-8
    max_gradient_value: 100.0
    min_gradient_value: -100.0
  
  # Logging
  logging:
    log_gradient_norms: true
    log_gradient_statistics: true
    log_gradient_histograms: false
    log_parameter_updates: false

# Integration with custom models
model_integration:
  # Transformer models
  transformer:
    enable_attention_gradients: true
    save_attention_weights: false
    compute_attention_gradients: false
  
  # CNN models
  cnn:
    enable_convolution_gradients: true
    save_feature_maps: false
    compute_feature_gradients: false
  
  # RNN models
  rnn:
    enable_hidden_gradients: true
    save_hidden_states: false
    compute_hidden_gradients: false
  
  # Hybrid models
  hybrid:
    enable_cross_modal_gradients: true
    save_modality_gradients: false
    compute_modality_gradients: false

# Example configurations
examples:
  # Basic autograd example
  basic:
    tensor_sizes: [2, 3]
    operations: ["power", "addition"]
    compute_gradients: true
  
  # Custom function example
  custom_function:
    input_shape: [3, 2]
    weight_shape: [2, 4]
    activation: "tanh"
    compute_gradients: true
  
  # Higher-order gradients
  higher_order:
    max_order: 3
    function: "polynomial"
    coefficients: [1, 2, 1, 0]
    compute_gradients: true
  
  # Gradient flow
  gradient_flow:
    network_layers: [2, 4, 1]
    activation: "relu"
    batch_size: 5
    compute_gradients: true

# Validation rules
validation:
  # Input validation
  input_validation:
    check_tensor_shapes: true
    check_data_types: true
    check_device_consistency: true
  
  # Output validation
  output_validation:
    check_gradient_shapes: true
    check_gradient_values: true
    check_gradient_consistency: true
  
  # Model validation
  model_validation:
    check_parameter_gradients: true
    check_model_consistency: true
    check_training_mode: true

# Export and deployment
export:
  # TorchScript
  torchscript:
    enable: true
    optimize: true
    strict: false
  
  # ONNX
  onnx:
    enable: true
    opset_version: 17
    dynamic_axes: true
  
  # Custom formats
  custom:
    enable: false
    format: "custom"
    options: {}

# Performance benchmarks
benchmarks:
  # Gradient computation
  gradient_computation:
    measure_time: true
    measure_memory: true
    measure_throughput: true
  
  # Model training
  model_training:
    measure_epoch_time: true
    measure_step_time: true
    measure_memory_usage: true
  
  # Comparison metrics
  comparison:
    compare_with_baseline: true
    save_results: true
    generate_report: true


