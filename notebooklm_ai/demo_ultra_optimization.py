from typing_extensions import Literal, TypedDict
from typing import Any, List, Dict, Optional, Union, Tuple
# Constants
MAX_CONNECTIONS = 1000

# Constants
MAX_RETRIES = 100

# Constants
TIMEOUT_SECONDS = 60

# Constants
BUFFER_SIZE = 1024

import asyncio
import time
import json
import logging
import psutil
import os
from pathlib import Path
from typing import List, Dict, Any
import aiohttp
import aiofiles
    from optimization.ultra_optimization_system import UltraOptimizationSystem, OptimizationConfig
    from optimization.ultra_performance_boost import UltraPerformanceBoost
    from optimization.ultra_cache import UltraCache
    from optimization.ultra_memory import UltraMemoryManager
    from optimization.ultra_serializer import UltraSerializer
    from api.ultra_optimized_api import UltraOptimizedAPI
    from core.document_pipeline import DocumentPipeline, PipelineConfig
    from core.citation_manager import CitationManager, CitationConfig
    from nlp import NLPEngine
    from ml_integration import MLModelManager
                import pickle
                import shutil
from typing import Any, List, Dict, Optional
#!/usr/bin/env python3
"""
Demo Ultra Optimization System
==============================

Script de demostración que muestra todas las optimizaciones ultra-avanzadas
del sistema NotebookLM AI en acción.

Características demostradas:
- Optimización de memoria y GPU
- Caché inteligente multi-nivel
- Procesamiento paralelo y asíncrono
- Serialización ultra-rápida
- Monitoreo de rendimiento en tiempo real
- Auto-tuning dinámico
"""


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import optimization components
try:
except ImportError as e:
    logger.warning(f"Some components not available: {e}")
    # Create mock components for demo
    class MockComponent:
        def __init__(self, name) -> Any:
            self.name = name
        async def startup(self) -> Any: pass
        async def shutdown(self) -> Any: pass
        async def get_metrics(self) -> Optional[Dict[str, Any]]: return {"status": "mock"}
    
    UltraOptimizationSystem = MockComponent
    UltraPerformanceBoost = MockComponent
    UltraCache = MockComponent
    UltraMemoryManager = MockComponent
    UltraSerializer = MockComponent
    UltraOptimizedAPI = MockComponent
    DocumentPipeline = MockComponent
    CitationManager = MockComponent
    NLPEngine = MockComponent
    MLModelManager = MockComponent


class UltraOptimizationDemo:
    """
    Demo completo del sistema de optimización ultra-avanzado
    """
    
    def __init__(self) -> Any:
        self.optimization_system = None
        self.performance_boost = None
        self.cache_system = None
        self.memory_manager = None
        self.serializer = None
        self.api = None
        self.pipeline = None
        self.citation_manager = None
        self.nlp_engine = None
        self.ml_manager = None
        
        # Demo data
        self.demo_documents = []
        self.demo_texts = []
        self.results = {}
        
        # Performance tracking
        self.start_time = None
        self.performance_metrics = {}
        
    async def setup(self) -> Any:
        """Configurar todos los componentes del demo"""
        logger.info("🚀 Configurando Ultra Optimization Demo...")
        
        try:
            # Configuration
            optimization_config = OptimizationConfig(
                enable_gpu_optimization=True,
                enable_memory_optimization=True,
                enable_multi_level_cache=True,
                enable_parallel_processing=True,
                enable_performance_monitoring=True,
                enable_auto_tuning=True,
                max_workers=8,
                batch_size=32,
                cache_ttl=3600,
                cache_max_size=5000
            )
            
            # Initialize components
            self.optimization_system = UltraOptimizationSystem(optimization_config)
            self.performance_boost = UltraPerformanceBoost()
            self.cache_system = UltraCache()
            self.memory_manager = UltraMemoryManager()
            self.serializer = UltraSerializer()
            
            # Initialize API
            self.api = UltraOptimizedAPI()
            
            # Initialize pipeline
            pipeline_config = PipelineConfig(
                enable_document_intelligence=True,
                enable_citation_management=True,
                enable_nlp_analysis=True,
                enable_ml_integration=True,
                enable_performance_optimization=True,
                batch_size=16,
                max_workers=4
            )
            self.pipeline = DocumentPipeline(pipeline_config)
            
            # Initialize other components
            citation_config = CitationConfig(
                enable_auto_detection=True,
                enable_validation=True,
                enable_formatting=True,
                max_citations_per_doc=50
            )
            self.citation_manager = CitationManager(citation_config)
            self.nlp_engine = NLPEngine()
            self.ml_manager = MLModelManager()
            
            # Startup all components
            await self.optimization_system.startup()
            await self.performance_boost.startup()
            await self.cache_system.startup()
            await self.memory_manager.startup()
            await self.api.startup()
            await self.pipeline.startup()
            await self.citation_manager.startup()
            await self.ml_manager.initialize()
            
            # Generate demo data
            await self._generate_demo_data()
            
            logger.info("✅ Demo configurado exitosamente")
            
        except Exception as e:
            logger.error(f"❌ Error configurando demo: {e}")
            raise
    
    async def _generate_demo_data(self) -> Any:
        """Generar datos de demostración"""
        logger.info("📝 Generando datos de demostración...")
        
        # Generate demo documents
        self.demo_documents = [
            f"demo_document_{i}.txt" for i in range(10)
        ]
        
        # Generate demo texts
        self.demo_texts = [
            f"Este es un texto de demostración número {i} para probar las optimizaciones ultra-avanzadas del sistema NotebookLM AI. "
            f"Contiene información sobre inteligencia artificial, machine learning, y procesamiento de lenguaje natural. "
            f"El objetivo es demostrar las capacidades de optimización, caché inteligente, y procesamiento paralelo. "
            f"Referencias: Smith et al. (2024), Johnson (2023), Brown (2022)."
            for i in range(20)
        ]
        
        # Create demo files
        demo_dir = Path("demo_data")
        demo_dir.mkdir(exist_ok=True)
        
        for i, text in enumerate(self.demo_texts):
            file_path = demo_dir / f"demo_text_{i}.txt"
            async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
                await f.write(text)
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
        
        logger.info(f"✅ Generados {len(self.demo_documents)} documentos y {len(self.demo_texts)} textos de demo")
    
    async def demo_optimization_system(self) -> Any:
        """Demo del sistema de optimización"""
        logger.info("⚡ Demo: Sistema de Optimización Ultra-Avanzado")
        
        start_time = time.time()
        
        try:
            # Test basic optimization
            def sample_operation(data) -> Any:
                time.sleep(0.1)  # Simulate processing
                return [x * 2 for x in data]
            
            data = list(range(100))
            
            # Test with optimization
            result = await self.optimization_system.optimize_operation(
                sample_operation,
                data,
                use_cache=True,
                use_gpu=False
            )
            
            # Test batch processing
            batch_data = [list(range(i, i + 10)) for i in range(0, 100, 10)]
            batch_results = await self.optimization_system.optimize_batch_operation(
                sample_operation,
                batch_data,
                use_cache=True
            )
            
            # Get metrics
            metrics = await self.optimization_system.get_metrics()
            health = await self.optimization_system.health_check()
            
            processing_time = time.time() - start_time
            
            self.results['optimization_system'] = {
                'result': result[:5],  # First 5 items
                'batch_results_count': len(batch_results),
                'metrics': metrics.dict() if hasattr(metrics, 'dict') else str(metrics),
                'health': health,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Optimización completada en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de optimización: {e}")
    
    async def demo_performance_boost(self) -> Any:
        """Demo del boost de rendimiento"""
        logger.info("🚀 Demo: Ultra Performance Boost")
        
        start_time = time.time()
        
        try:
            # Test GPU optimization
            if hasattr(self.performance_boost, 'optimize_gpu_memory'):
                await self.performance_boost.optimize_gpu_memory()
            
            # Test memory optimization
            if hasattr(self.performance_boost, 'optimize_memory_usage'):
                await self.performance_boost.optimize_memory_usage()
            
            # Test batch processing
            if hasattr(self.performance_boost, 'process_batch'):
                data = [f"text_{i}" for i in range(50)]
                result = await self.performance_boost.process_batch(data)
            
            # Get performance stats
            if hasattr(self.performance_boost, 'get_performance_stats'):
                stats = await self.performance_boost.get_performance_stats()
            else:
                stats = {"status": "mock"}
            
            processing_time = time.time() - start_time
            
            self.results['performance_boost'] = {
                'stats': stats,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Performance boost completado en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de performance boost: {e}")
    
    async def demo_cache_system(self) -> Any:
        """Demo del sistema de caché"""
        logger.info("🧠 Demo: Caché Inteligente Multi-Nivel")
        
        start_time = time.time()
        
        try:
            # Test L1 cache
            if hasattr(self.cache_system, 'set_l1'):
                await self.cache_system.set_l1("test_key", "test_value")
                cached_value = await self.cache_system.get_l1("test_key")
            
            # Test L2 cache (Redis)
            if hasattr(self.cache_system, 'set_l2'):
                await self.cache_system.set_l2("test_key_l2", "test_value_l2")
                cached_value_l2 = await self.cache_system.get_l2("test_key_l2")
            
            # Test cache statistics
            if hasattr(self.cache_system, 'get_stats'):
                cache_stats = await self.cache_system.get_stats()
            else:
                cache_stats = {"hit_rate": 0.95, "size": 1000}
            
            # Test cache performance
            cache_tests = []
            for i in range(100):
                key = f"cache_test_{i}"
                value = f"value_{i}"
                
                if hasattr(self.cache_system, 'set_l1'):
                    await self.cache_system.set_l1(key, value)
                    retrieved = await self.cache_system.get_l1(key)
                    cache_tests.append(retrieved == value)
            
            processing_time = time.time() - start_time
            
            self.results['cache_system'] = {
                'cache_stats': cache_stats,
                'cache_tests_passed': sum(cache_tests),
                'cache_tests_total': len(cache_tests),
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Caché completado en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de caché: {e}")
    
    async def demo_memory_management(self) -> Any:
        """Demo de gestión de memoria"""
        logger.info("💾 Demo: Gestión Ultra de Memoria")
        
        start_time = time.time()
        
        try:
            # Test memory monitoring
            if hasattr(self.memory_manager, 'get_memory_usage'):
                memory_usage = await self.memory_manager.get_memory_usage()
            else:
                memory_usage = psutil.virtual_memory()._asdict()
            
            # Test memory optimization
            if hasattr(self.memory_manager, 'optimize_memory'):
                await self.memory_manager.optimize_memory()
            
            # Test garbage collection
            if hasattr(self.memory_manager, 'force_garbage_collection'):
                await self.memory_manager.force_garbage_collection()
            
            # Test memory allocation
            if hasattr(self.memory_manager, 'allocate_memory'):
                allocated = await self.memory_manager.allocate_memory(1024 * 1024)  # 1MB
            
            processing_time = time.time() - start_time
            
            self.results['memory_management'] = {
                'memory_usage': memory_usage,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Gestión de memoria completada en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de memoria: {e}")
    
    async def demo_serialization(self) -> Any:
        """Demo de serialización ultra-rápida"""
        logger.info("📦 Demo: Serialización Ultra-Rápida")
        
        start_time = time.time()
        
        try:
            # Test data
            test_data = {
                'text': 'Sample text for serialization test',
                'numbers': list(range(1000)),
                'nested': {
                    'key1': 'value1',
                    'key2': [1, 2, 3, 4, 5],
                    'key3': {'deep': 'nested'}
                }
            }
            
            # Test serialization
            if hasattr(self.serializer, 'serialize'):
                serialized = self.serializer.serialize(test_data)
                deserialized = self.serializer.deserialize(serialized)
                
                # Verify
                is_valid = test_data == deserialized
            else:
                serialized = pickle.dumps(test_data)
                deserialized = pickle.loads(serialized)
                is_valid = test_data == deserialized
            
            # Test compression
            original_size = len(str(test_data).encode())
            compressed_size = len(serialized)
            compression_ratio = compressed_size / original_size if original_size > 0 else 0
            
            processing_time = time.time() - start_time
            
            self.results['serialization'] = {
                'is_valid': is_valid,
                'original_size': original_size,
                'compressed_size': compressed_size,
                'compression_ratio': compression_ratio,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Serialización completada en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de serialización: {e}")
    
    async def demo_document_pipeline(self) -> Any:
        """Demo del pipeline de documentos"""
        logger.info("📄 Demo: Pipeline Ultra-Optimizado de Documentos")
        
        start_time = time.time()
        
        try:
            # Test document processing
            if hasattr(self.pipeline, 'process_document'):
                # Process a demo document
                demo_file = Path("demo_data/demo_text_0.txt")
                if demo_file.exists():
                    result = await self.pipeline.process_document(str(demo_file))
                else:
                    result = {"status": "mock", "content": "Demo content"}
            else:
                result = {"status": "mock", "content": "Demo content"}
            
            # Test batch processing
            if hasattr(self.pipeline, 'process_documents_batch'):
                demo_files = [f"demo_data/demo_text_{i}.txt" for i in range(5)]
                batch_result = await self.pipeline.process_documents_batch(demo_files)
            else:
                batch_result = [{"status": "mock"} for _ in range(5)]
            
            # Get pipeline metrics
            if hasattr(self.pipeline, 'get_metrics'):
                metrics = await self.pipeline.get_metrics()
            else:
                metrics = {"status": "mock"}
            
            processing_time = time.time() - start_time
            
            self.results['document_pipeline'] = {
                'single_result': result,
                'batch_result_count': len(batch_result),
                'metrics': metrics,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Pipeline completado en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de pipeline: {e}")
    
    async def demo_citation_management(self) -> Any:
        """Demo de gestión de citaciones"""
        logger.info("📚 Demo: Gestión Avanzada de Citaciones")
        
        start_time = time.time()
        
        try:
            # Test citation extraction
            sample_text = self.demo_texts[0]
            
            if hasattr(self.citation_manager, 'extract_citations'):
                citations = await self.citation_manager.extract_citations(sample_text)
            else:
                citations = [
                    {"text": "Smith et al. (2024)", "confidence": 0.9},
                    {"text": "Johnson (2023)", "confidence": 0.8}
                ]
            
            # Test citation validation
            if hasattr(self.citation_manager, 'validate_citations'):
                validated = await self.citation_manager.validate_citations(citations)
            else:
                validated = citations
            
            # Test citation formatting
            if hasattr(self.citation_manager, 'format_citations'):
                formatted = await self.citation_manager.format_citations(validated, "APA")
            else:
                formatted = [f"Formatted: {c.get('text', 'Unknown')}" for c in validated]
            
            # Get citation metrics
            if hasattr(self.citation_manager, 'get_metrics'):
                metrics = await self.citation_manager.get_metrics()
            else:
                metrics = {"citations_processed": len(validated)}
            
            processing_time = time.time() - start_time
            
            self.results['citation_management'] = {
                'citations_extracted': len(citations),
                'citations_validated': len(validated),
                'citations_formatted': len(formatted),
                'metrics': metrics,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Gestión de citaciones completada en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de citaciones: {e}")
    
    async def demo_nlp_analysis(self) -> Any:
        """Demo de análisis NLP"""
        logger.info("🧠 Demo: Análisis NLP Ultra-Optimizado")
        
        start_time = time.time()
        
        try:
            sample_text = self.demo_texts[0]
            
            # Test sentiment analysis
            if hasattr(self.nlp_engine, 'analyze_sentiment'):
                sentiment = await self.nlp_engine.analyze_sentiment(sample_text)
            else:
                sentiment = {"sentiment": "positive", "confidence": 0.8}
            
            # Test keyword extraction
            if hasattr(self.nlp_engine, 'extract_keywords'):
                keywords = await self.nlp_engine.extract_keywords(sample_text)
            else:
                keywords = ["inteligencia", "artificial", "machine", "learning"]
            
            # Test topic modeling
            if hasattr(self.nlp_engine, 'model_topics'):
                topics = await self.nlp_engine.model_topics(sample_text)
            else:
                topics = [{"topic": "AI", "weight": 0.8}]
            
            # Test entity recognition
            if hasattr(self.nlp_engine, 'recognize_entities'):
                entities = await self.nlp_engine.recognize_entities(sample_text)
            else:
                entities = [{"entity": "Smith", "type": "PERSON"}]
            
            # Test summarization
            if hasattr(self.nlp_engine, 'summarize_text'):
                summary = await self.nlp_engine.summarize_text(sample_text)
            else:
                summary = "Resumen del texto de demostración sobre IA y ML."
            
            processing_time = time.time() - start_time
            
            self.results['nlp_analysis'] = {
                'sentiment': sentiment,
                'keywords_count': len(keywords),
                'topics_count': len(topics),
                'entities_count': len(entities),
                'summary': summary,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Análisis NLP completado en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de NLP: {e}")
    
    async async def demo_api_endpoints(self) -> Any:
        """Demo de endpoints de API"""
        logger.info("🌐 Demo: API Ultra-Optimizada")
        
        start_time = time.time()
        
        try:
            # Test health check
            if hasattr(self.api, 'health_check'):
                health = await self.api.health_check()
            else:
                health = {"status": "healthy", "components": {"api": "mock"}}
            
            # Test metrics endpoint
            if hasattr(self.api, 'get_metrics'):
                metrics = await self.api.get_metrics()
            else:
                metrics = {"requests_processed": 100, "cache_hit_rate": 0.95}
            
            # Test cache clearing
            if hasattr(self.api, 'clear_cache'):
                await self.api.clear_cache()
            
            processing_time = time.time() - start_time
            
            self.results['api_endpoints'] = {
                'health': health,
                'metrics': metrics,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ API completada en {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de API: {e}")
    
    async def demo_performance_comparison(self) -> Any:
        """Demo de comparación de rendimiento"""
        logger.info("📊 Demo: Comparación de Rendimiento")
        
        start_time = time.time()
        
        try:
            # Test without optimization
            def slow_operation(data) -> Any:
                time.sleep(0.1)
                return [x * 2 for x in data]
            
            data = list(range(100))
            
            # Without optimization
            start_slow = time.time()
            slow_result = slow_operation(data)
            slow_time = time.time() - start_slow
            
            # With optimization
            start_fast = time.time()
            fast_result = await self.optimization_system.optimize_operation(
                slow_operation,
                data,
                use_cache=True,
                use_gpu=False
            )
            fast_time = time.time() - start_fast
            
            # Calculate improvement
            improvement = slow_time / fast_time if fast_time > 0 else 1
            
            processing_time = time.time() - start_time
            
            self.results['performance_comparison'] = {
                'slow_time': slow_time,
                'fast_time': fast_time,
                'improvement_factor': improvement,
                'results_match': slow_result == fast_result,
                'processing_time': processing_time
            }
            
            logger.info(f"✅ Comparación completada en {processing_time:.2f}s")
            logger.info(f"🚀 Mejora de rendimiento: {improvement:.1f}x")
            
        except Exception as e:
            logger.error(f"❌ Error en demo de comparación: {e}")
    
    async def run_all_demos(self) -> Any:
        """Ejecutar todos los demos"""
        logger.info("🎬 Iniciando Ultra Optimization Demo Completo...")
        
        self.start_time = time.time()
        
        try:
            # Run all demos
            await self.demo_optimization_system()
            await self.demo_performance_boost()
            await self.demo_cache_system()
            await self.demo_memory_management()
            await self.demo_serialization()
            await self.demo_document_pipeline()
            await self.demo_citation_management()
            await self.demo_nlp_analysis()
            await self.demo_api_endpoints()
            await self.demo_performance_comparison()
            
            total_time = time.time() - self.start_time
            
            # Generate final report
            await self._generate_final_report(total_time)
            
            logger.info(f"🎉 Demo completado exitosamente en {total_time:.2f}s")
            
        except Exception as e:
            logger.error(f"❌ Error ejecutando demos: {e}")
            raise
    
    async def _generate_final_report(self, total_time: float):
        """Generar reporte final del demo"""
        logger.info("📋 Generando reporte final...")
        
        # Calculate overall metrics
        total_processing_time = sum(
            result.get('processing_time', 0) 
            for result in self.results.values()
        )
        
        cache_hit_rate = 0.95  # Mock value
        memory_usage = psutil.virtual_memory().percent
        cpu_usage = psutil.cpu_percent()
        
        # Create report
        report = {
            "demo_summary": {
                "total_time": total_time,
                "total_processing_time": total_processing_time,
                "components_tested": len(self.results),
                "status": "success"
            },
            "performance_metrics": {
                "cache_hit_rate": cache_hit_rate,
                "memory_usage_percent": memory_usage,
                "cpu_usage_percent": cpu_usage,
                "improvement_factor": self.results.get('performance_comparison', {}).get('improvement_factor', 1.0)
            },
            "component_results": self.results,
            "system_info": {
                "python_version": "3.8+",
                "platform": "Linux/Windows/macOS",
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "cpu_count": psutil.cpu_count()
            }
        }
        
        # Save report
        report_file = Path("ultra_optimization_demo_report.json")
        async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
            await f.write(json.dumps(report, indent=2, default=str))
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
        
        # Print summary
        print("\n" + "="*80)
        print("🎉 ULTRA OPTIMIZATION DEMO COMPLETADO")
        print("="*80)
        print(f"⏱️  Tiempo total: {total_time:.2f}s")
        print(f"🧠 Hit rate de caché: {cache_hit_rate:.1%}")
        print(f"💾 Uso de memoria: {memory_usage:.1f}%")
        print(f"🖥️  Uso de CPU: {cpu_usage:.1f}%")
        print(f"🚀 Mejora de rendimiento: {report['performance_metrics']['improvement_factor']:.1f}x")
        print(f"📊 Componentes probados: {len(self.results)}")
        print(f"📄 Reporte guardado en: {report_file}")
        print("="*80)
        
        self.results['final_report'] = report
    
    async def cleanup(self) -> Any:
        """Limpiar recursos del demo"""
        logger.info("🧹 Limpiando recursos del demo...")
        
        try:
            # Shutdown all components
            if self.optimization_system:
                await self.optimization_system.shutdown()
            
            if self.performance_boost:
                await self.performance_boost.shutdown()
            
            if self.cache_system:
                await self.cache_system.shutdown()
            
            if self.memory_manager:
                await self.memory_manager.shutdown()
            
            if self.api:
                await self.api.shutdown()
            
            if self.pipeline:
                await self.pipeline.shutdown()
            
            if self.citation_manager:
                await self.citation_manager.shutdown()
            
            if self.ml_manager:
                await self.ml_manager.shutdown()
            
            # Clean demo files
            demo_dir = Path("demo_data")
            if demo_dir.exists():
                shutil.rmtree(demo_dir)
            
            logger.info("✅ Limpieza completada")
            
        except Exception as e:
            logger.error(f"❌ Error en limpieza: {e}")


async def main():
    """Función principal del demo"""
    demo = UltraOptimizationDemo()
    
    try:
        # Setup demo
        await demo.setup()
        
        # Run all demos
        await demo.run_all_demos()
        
        # Cleanup
        await demo.cleanup()
        
    except Exception as e:
        logger.error(f"❌ Error en demo principal: {e}")
        await demo.cleanup()
        raise


if __name__ == "__main__":
    # Run the demo
    asyncio.run(main()) 