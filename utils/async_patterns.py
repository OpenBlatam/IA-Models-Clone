from typing_extensions import Literal, TypedDict
from typing import Any, List, Dict, Optional, Union, Tuple
# Constants
MAX_CONNECTIONS = 1000

# Constants
MAX_RETRIES = 100

# Constants
TIMEOUT_SECONDS = 60

import asyncio
import time
import logging
import functools
import inspect
from typing import Any, Dict, List, Optional, Callable, Union, Tuple, TypeVar, Generic, Awaitable, AsyncIterator, AsyncGenerator
from dataclasses import dataclass, field
from collections import defaultdict, deque
from datetime import datetime, timedelta
from enum import Enum
import json
import weakref
import contextlib
from abc import ABC, abstractmethod
import structlog
from pydantic import BaseModel, Field
import numpy as np
        import aiofiles
from typing import Any, List, Dict, Optional
"""
ðŸ”„ Async Patterns Library
=========================

Comprehensive collection of async patterns and best practices for:
- Async generators and iterators
- Async context managers
- Async queues and pipelines
- Async event-driven patterns
- Async batching and streaming
- Async retry and circuit breaker patterns
- Async caching patterns
- Async resource pooling
- Async background task patterns
- Async error handling patterns
"""



logger = structlog.get_logger(__name__)

T = TypeVar('T')
InputT = TypeVar('InputT')
OutputT = TypeVar('OutputT')

class AsyncPatternType(Enum):
    """Types of async patterns"""
    GENERATOR = "generator"
    CONTEXT_MANAGER = "context_manager"
    QUEUE = "queue"
    PIPELINE = "pipeline"
    EVENT_DRIVEN = "event_driven"
    BATCHING = "batching"
    STREAMING = "streaming"
    RETRY = "retry"
    CIRCUIT_BREAKER = "circuit_breaker"
    CACHING = "caching"
    POOLING = "pooling"
    BACKGROUND = "background"
    ERROR_HANDLING = "error_handling"

# 1. Async Generators and Iterators

class AsyncDataGenerator:
    """Async generator for processing data in batches"""
    
    def __init__(self, data: List[Any], batch_size: int = 100, delay: float = 0.01):
        
    """__init__ function."""
self.data = data
        self.batch_size = batch_size
        self.delay = delay
        self.index = 0
    
    def __aiter__(self) -> Any:
        return self
    
    async def __anext__(self) -> Any:
        if self.index >= len(self.data):
            raise StopAsyncIteration
        
        # Get batch
        end_index = min(self.index + self.batch_size, len(self.data))
        batch = self.data[self.index:end_index]
        self.index = end_index
        
        # Small delay to prevent blocking
        if self.delay > 0:
            await asyncio.sleep(self.delay)
        
        return batch

class AsyncFileReader:
    """Async generator for reading files in chunks"""
    
    def __init__(self, file_path: str, chunk_size: int = 8192):
        
    """__init__ function."""
self.file_path = file_path
        self.chunk_size = chunk_size
        self.file = None
    
    async def __aenter__(self) -> Any:
        self.file = await aiofiles.open(self.file_path, 'r')
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> Any:
        if self.file:
            await self.file.close()
    
    def __aiter__(self) -> Any:
        return self
    
    async def __anext__(self) -> Any:
        if not self.file:
            raise StopAsyncIteration
        
        chunk = await self.file.read(self.chunk_size)
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
        if not chunk:
            raise StopAsyncIteration
        
        return chunk

class AsyncStreamProcessor:
    """Async stream processor with backpressure handling"""
    
    def __init__(self, max_buffer_size: int = 1000):
        
    """__init__ function."""
self.max_buffer_size = max_buffer_size
        self.buffer = asyncio.Queue(maxsize=max_buffer_size)
        self.processing = False
    
    async def add_item(self, item: Any):
        """Add item to stream buffer"""
        await self.buffer.put(item)
    
    async def process_stream(self, processor: Callable[[Any], Awaitable[Any]]):
        """Process stream with backpressure handling"""
        self.processing = True
        
        while self.processing:
            try:
                # Get item with timeout
                item = await asyncio.wait_for(self.buffer.get(), timeout=1.0)
                
                # Process item
                result = await processor(item)
                
                # Mark as done
                self.buffer.task_done()
                
                yield result
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Error processing stream item: {e}")
                self.buffer.task_done()
    
    def stop(self) -> Any:
        """Stop stream processing"""
        self.processing = False

# 2. Async Context Managers

class AsyncResourcePool:
    """Async resource pool with automatic cleanup"""
    
    def __init__(self, factory: Callable[[], Awaitable[Any]], max_size: int = 10):
        
    """__init__ function."""
self.factory = factory
        self.max_size = max_size
        self.pool = asyncio.Queue(maxsize=max_size)
        self.created_resources = 0
        self.lock = asyncio.Lock()
    
    async def __aenter__(self) -> Any:
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> Any:
        await self.cleanup()
    
    async def get_resource(self) -> Optional[Dict[str, Any]]:
        """Get resource from pool"""
        try:
            # Try to get from pool
            return self.pool.get_nowait()
        except asyncio.QueueEmpty:
            # Create new resource if pool is not full
            async with self.lock:
                if self.created_resources < self.max_size:
                    resource = await self.factory()
                    self.created_resources += 1
                    return resource
                else:
                    # Wait for resource to become available
                    return await self.pool.get()
    
    async def return_resource(self, resource: Any):
        """Return resource to pool"""
        try:
            self.pool.put_nowait(resource)
        except asyncio.QueueFull:
            # Pool is full, cleanup resource
            await self._cleanup_resource(resource)
    
    async def _cleanup_resource(self, resource: Any):
        """Cleanup individual resource"""
        if hasattr(resource, 'close'):
            await resource.close()
        elif hasattr(resource, 'aclose'):
            await resource.aclose()
        self.created_resources -= 1
    
    async def cleanup(self) -> Any:
        """Cleanup all resources"""
        while not self.pool.empty():
            resource = await self.pool.get()
            await self._cleanup_resource(resource)

class AsyncTransaction:
    """Async transaction context manager"""
    
    def __init__(self, connection) -> Any:
        self.connection = connection
        self.transaction = None
    
    async def __aenter__(self) -> Any:
        self.transaction = await self.connection.begin()
        return self.transaction
    
    async def __aexit__(self, exc_type, exc_val, exc_tb) -> Any:
        if exc_type is not None:
            await self.transaction.rollback()
        else:
            await self.transaction.commit()

# 3. Async Queues and Pipelines

class AsyncPipeline:
    """Async pipeline for processing data through multiple stages"""
    
    def __init__(self, stages: List[Callable[[Any], Awaitable[Any]]]):
        
    """__init__ function."""
self.stages = stages
        self.queues = [asyncio.Queue() for _ in range(len(stages) + 1)]
        self.tasks = []
    
    async def process(self, input_data: List[Any]) -> List[Any]:
        """Process data through pipeline"""
        # Start pipeline stages
        for i, stage in enumerate(self.stages):
            task = asyncio.create_task(self._run_stage(i, stage))
            self.tasks.append(task)
        
        # Feed input data
        for item in input_data:
            await self.queues[0].put(item)
        
        # Close input queue
        await self.queues[0].put(None)
        
        # Collect results
        results = []
        while True:
            result = await self.queues[-1].get()
            if result is None:
                break
            results.append(result)
        
        # Wait for all stages to complete
        await asyncio.gather(*self.tasks)
        
        return results
    
    async def _run_stage(self, stage_index: int, stage_func: Callable):
        """Run a pipeline stage"""
        input_queue = self.queues[stage_index]
        output_queue = self.queues[stage_index + 1]
        
        while True:
            item = await input_queue.get()
            if item is None:
                # End of input, signal next stage
                await output_queue.put(None)
                break
            
            try:
                # Process item
                result = await stage_func(item)
                await output_queue.put(result)
            except Exception as e:
                logger.error(f"Error in pipeline stage {stage_index}: {e}")
                # Continue processing other items

class AsyncEventBus:
    """Async event bus for event-driven patterns"""
    
    def __init__(self) -> Any:
        self.subscribers = defaultdict(list)
        self.event_queue = asyncio.Queue()
        self.running = False
        self.task = None
    
    async def start(self) -> Any:
        """Start event bus"""
        self.running = True
        self.task = asyncio.create_task(self._event_loop())
    
    async def stop(self) -> Any:
        """Stop event bus"""
        self.running = False
        if self.task:
            await self.task
    
    def subscribe(self, event_type: str, handler: Callable[[Any], Awaitable[None]]):
        """Subscribe to event type"""
        self.subscribers[event_type].append(handler)
    
    async def publish(self, event_type: str, event_data: Any):
        """Publish event"""
        await self.event_queue.put((event_type, event_data))
    
    async def _event_loop(self) -> Any:
        """Event processing loop"""
        while self.running:
            try:
                event_type, event_data = await asyncio.wait_for(
                    self.event_queue.get(), timeout=1.0
                )
                
                # Notify all subscribers
                handlers = self.subscribers[event_type]
                if handlers:
                    tasks = [handler(event_data) for handler in handlers]
                    await asyncio.gather(*tasks, return_exceptions=True)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Error in event loop: {e}")

# 4. Async Batching and Streaming

class AsyncBatchProcessor:
    """Async batch processor with configurable batching strategies"""
    
    def __init__(self, processor: Callable[[List[Any]], Awaitable[List[Any]]], 
                 batch_size: int = 100, max_wait_time: float = 1.0):
        
    """__init__ function."""
self.processor = processor
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.batch_queue = asyncio.Queue()
        self.result_queue = asyncio.Queue()
        self.running = False
        self.task = None
    
    async def start(self) -> Any:
        """Start batch processor"""
        self.running = True
        self.task = asyncio.create_task(self._batch_loop())
    
    async def stop(self) -> Any:
        """Stop batch processor"""
        self.running = False
        if self.task:
            await self.task
    
    async def add_item(self, item: Any) -> str:
        """Add item for batch processing"""
        batch_id = f"batch_{time.time()}_{id(item)}"
        await self.batch_queue.put((batch_id, item))
        return batch_id
    
    async def get_result(self, batch_id: str, timeout: float = 30.0) -> Optional[Dict[str, Any]]:
        """Get result for batch item"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                result_batch_id, result = self.result_queue.get_nowait()
                if result_batch_id == batch_id:
                    return result
                else:
                    # Put back other results
                    await self.result_queue.put((result_batch_id, result))
            except asyncio.QueueEmpty:
                await asyncio.sleep(0.1)
        
        raise TimeoutError(f"Timeout waiting for result: {batch_id}")
    
    async def _batch_loop(self) -> Any:
        """Batch processing loop"""
        current_batch = []
        current_batch_ids = []
        last_batch_time = time.time()
        
        while self.running:
            try:
                # Try to get item with timeout
                batch_id, item = await asyncio.wait_for(
                    self.batch_queue.get(), timeout=0.1
                )
                
                current_batch.append(item)
                current_batch_ids.append(batch_id)
                
                # Check if we should process batch
                should_process = (
                    len(current_batch) >= self.batch_size or
                    time.time() - last_batch_time >= self.max_wait_time
                )
                
                if should_process and current_batch:
                    # Process batch
                    try:
                        results = await self.processor(current_batch)
                        
                        # Put results back
                        for batch_id, result in zip(current_batch_ids, results):
                            await self.result_queue.put((batch_id, result))
                    
                    except Exception as e:
                        logger.error(f"Error processing batch: {e}")
                        # Put error results
                        for batch_id in current_batch_ids:
                            await self.result_queue.put((batch_id, {"error": str(e)}))
                    
                    # Reset batch
                    current_batch = []
                    current_batch_ids = []
                    last_batch_time = time.time()
                
            except asyncio.TimeoutError:
                # Process remaining items if timeout reached
                if current_batch and time.time() - last_batch_time >= self.max_wait_time:
                    try:
                        results = await self.processor(current_batch)
                        for batch_id, result in zip(current_batch_ids, results):
                            await self.result_queue.put((batch_id, result))
                    except Exception as e:
                        logger.error(f"Error processing final batch: {e}")
                    
                    current_batch = []
                    current_batch_ids = []
                    last_batch_time = time.time()

# 5. Async Retry and Circuit Breaker

class AsyncRetry:
    """Async retry pattern with exponential backoff"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 1.0, 
                 max_delay: float = 60.0, exponential_base: float = 2.0):
        
    """__init__ function."""
self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.exponential_base = exponential_base
    
    async def __call__(self, func: Callable[[], Awaitable[T]], *args, **kwargs) -> T:
        """Execute function with retry logic"""
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    raise last_exception
                
                # Calculate delay with exponential backoff
                delay = min(
                    self.base_delay * (self.exponential_base ** attempt),
                    self.max_delay
                )
                
                logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s")
                await asyncio.sleep(delay)
        
        raise last_exception

class AsyncCircuitBreaker:
    """Async circuit breaker pattern"""
    
    def __init__(self, failure_threshold: int = 5, recovery_timeout: float = 60.0,
                 expected_exception: type = Exception):
        
    """__init__ function."""
self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    async def __call__(self, func: Callable[[], Awaitable[T]], *args, **kwargs) -> T:
        """Execute function with circuit breaker logic"""
        if self.state == "OPEN":
            if time.time() - self.last_failure_time >= self.recovery_timeout:
                self.state = "HALF_OPEN"
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            
            return result
            
        except self.expected_exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
            
            raise e

# 6. Async Caching Patterns

class AsyncCache:
    """Async cache with TTL and LRU eviction"""
    
    def __init__(self, max_size: int = 1000, default_ttl: float = 3600.0):
        
    """__init__ function."""
self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache = {}
        self.access_order = deque()
        self.lock = asyncio.Lock()
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        async with self.lock:
            if key in self.cache:
                value, expiry = self.cache[key]
                
                # Check if expired
                if time.time() > expiry:
                    del self.cache[key]
                    self.access_order.remove(key)
                    return None
                
                # Update access order
                self.access_order.remove(key)
                self.access_order.append(key)
                
                return value
            
            return None
    
    async def set(self, key: str, value: Any, ttl: float = None):
        """Set value in cache"""
        if ttl is None:
            ttl = self.default_ttl
        
        async with self.lock:
            # Check if key exists
            if key in self.cache:
                self.access_order.remove(key)
            
            # Add to cache
            expiry = time.time() + ttl
            self.cache[key] = (value, expiry)
            self.access_order.append(key)
            
            # Evict if necessary
            if len(self.cache) > self.max_size:
                oldest_key = self.access_order.popleft()
                del self.cache[oldest_key]
    
    async def delete(self, key: str):
        """Delete key from cache"""
        async with self.lock:
            if key in self.cache:
                del self.cache[key]
                self.access_order.remove(key)
    
    async def clear(self) -> Any:
        """Clear all cache"""
        async with self.lock:
            self.cache.clear()
            self.access_order.clear()

# 7. Async Background Task Patterns

class AsyncBackgroundTaskManager:
    """Manager for async background tasks"""
    
    def __init__(self, max_concurrent_tasks: int = 10):
        
    """__init__ function."""
self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.tasks = {}
        self.task_results = {}
        self.running = True
    
    async def submit_task(self, task_id: str, task_func: Callable[[], Awaitable[Any]]) -> str:
        """Submit background task"""
        if not self.running:
            raise Exception("Task manager is not running")
        
        # Create task
        task = asyncio.create_task(self._execute_task(task_id, task_func))
        self.tasks[task_id] = task
        
        return task_id
    
    async def _execute_task(self, task_id: str, task_func: Callable[[], Awaitable[Any]]):
        """Execute task with semaphore control"""
        async with self.semaphore:
            try:
                result = await task_func()
                self.task_results[task_id] = {"status": "completed", "result": result}
            except Exception as e:
                self.task_results[task_id] = {"status": "failed", "error": str(e)}
            finally:
                # Clean up task
                if task_id in self.tasks:
                    del self.tasks[task_id]
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get task status"""
        if task_id in self.tasks:
            task = self.tasks[task_id]
            if task.done():
                return self.task_results.get(task_id, {"status": "unknown"})
            else:
                return {"status": "running"}
        else:
            return self.task_results.get(task_id)
    
    async def cancel_task(self, task_id: str) -> bool:
        """Cancel task"""
        if task_id in self.tasks:
            task = self.tasks[task_id]
            task.cancel()
            return True
        return False
    
    async def shutdown(self) -> Any:
        """Shutdown task manager"""
        self.running = False
        
        # Cancel all tasks
        for task in self.tasks.values():
            task.cancel()
        
        # Wait for all tasks to complete
        if self.tasks:
            await asyncio.gather(*self.tasks.values(), return_exceptions=True)

# 8. Async Error Handling Patterns

class AsyncErrorHandler:
    """Async error handling patterns"""
    
    def __init__(self) -> Any:
        self.error_handlers = {}
        self.error_counts = defaultdict(int)
        self.error_thresholds = {}
    
    def register_handler(self, exception_type: type, handler: Callable[[Exception], Awaitable[Any]]):
        """Register error handler for exception type"""
        self.error_handlers[exception_type] = handler
    
    def set_error_threshold(self, exception_type: type, threshold: int):
        """Set error threshold for exception type"""
        self.error_thresholds[exception_type] = threshold
    
    async def handle_error(self, exception: Exception) -> Any:
        """Handle exception with registered handler"""
        exception_type = type(exception)
        self.error_counts[exception_type] += 1
        
        # Check if threshold exceeded
        threshold = self.error_thresholds.get(exception_type)
        if threshold and self.error_counts[exception_type] >= threshold:
            logger.error(f"Error threshold exceeded for {exception_type}: {self.error_counts[exception_type]}")
        
        # Find handler
        handler = self.error_handlers.get(exception_type)
        if handler:
            return await handler(exception)
        else:
            # Default handler
            logger.error(f"Unhandled exception: {exception}")
            raise exception
    
    async def execute_with_error_handling(self, func: Callable[[], Awaitable[T]]) -> T:
        """Execute function with error handling"""
        try:
            return await func()
        except Exception as e:
            return await self.handle_error(e)

# Decorators for async patterns

def async_retry(max_retries: int = 3, base_delay: float = 1.0):
    """Decorator for async retry pattern"""
    retry = AsyncRetry(max_retries, base_delay)
    
    def decorator(func: Callable[[], Awaitable[T]]) -> Callable[[], Awaitable[T]]:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            return await retry(func, *args, **kwargs)
        return wrapper
    return decorator

def async_circuit_breaker(failure_threshold: int = 5, recovery_timeout: float = 60.0):
    """Decorator for async circuit breaker pattern"""
    circuit_breaker = AsyncCircuitBreaker(failure_threshold, recovery_timeout)
    
    def decorator(func: Callable[[], Awaitable[T]]) -> Callable[[], Awaitable[T]]:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            return await circuit_breaker(func, *args, **kwargs)
        return wrapper
    return decorator

def async_cache(ttl: float = 3600.0):
    """Decorator for async caching pattern"""
    cache = AsyncCache()
    
    def decorator(func: Callable[[], Awaitable[T]]) -> Callable[[], Awaitable[T]]:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # Create cache key
            cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # Try to get from cache
            cached_result = await cache.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Execute function
            result = await func(*args, **kwargs)
            
            # Cache result
            await cache.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

def async_background_task():
    """Decorator for async background task pattern"""
    def decorator(func: Callable[[], Awaitable[T]]) -> Callable[[], Awaitable[str]]:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs) -> Any:
            # This would integrate with a task manager
            task_id = f"task_{time.time()}_{id(func)}"
            # Submit to background task manager
            return task_id
        return wrapper
    return decorator

# Example usage

async def example_async_patterns():
    """Example usage of async patterns"""
    
    # 1. Async Generator
    data = list(range(1000))
    async for batch in AsyncDataGenerator(data, batch_size=100):
        print(f"Processing batch: {len(batch)} items")
        await asyncio.sleep(0.01)
    
    # 2. Async Pipeline
    async def stage1(data) -> Any:
        return [x * 2 for x in data]
    
    async def stage2(data) -> Any:
        return [x + 1 for x in data]
    
    pipeline = AsyncPipeline([stage1, stage2])
    results = await pipeline.process([1, 2, 3, 4, 5])
    print(f"Pipeline results: {results}")
    
    # 3. Async Event Bus
    event_bus = AsyncEventBus()
    await event_bus.start()
    
    async def event_handler(data) -> Any:
        print(f"Handling event: {data}")
    
    event_bus.subscribe("test_event", event_handler)
    await event_bus.publish("test_event", {"message": "Hello World"})
    
    await asyncio.sleep(0.1)
    await event_bus.stop()
    
    # 4. Async Batch Processor
    async def batch_processor(items) -> Any:
        return [item * 2 for item in items]
    
    batch_processor = AsyncBatchProcessor(batch_processor)
    await batch_processor.start()
    
    # Submit items
    batch_ids = []
    for i in range(10):
        batch_id = await batch_processor.add_item(i)
        batch_ids.append(batch_id)
    
    # Get results
    for batch_id in batch_ids:
        result = await batch_processor.get_result(batch_id)
        print(f"Batch result: {result}")
    
    await batch_processor.stop()
    
    # 5. Async Retry
    @async_retry(max_retries=3)
    async def unreliable_function():
        
    """unreliable_function function."""
if np.random.random() < 0.7:
            raise Exception("Random failure")
        return "Success"
    
    try:
        result = await unreliable_function()
        print(f"Retry result: {result}")
    except Exception as e:
        print(f"Retry failed: {e}")
    
    # 6. Async Circuit Breaker
    @async_circuit_breaker(failure_threshold=3)
    async def failing_function():
        
    """failing_function function."""
raise Exception("Always fails")
    
    for i in range(5):
        try:
            await failing_function()
        except Exception as e:
            print(f"Attempt {i+1}: {e}")
    
    # 7. Async Cache
    @async_cache(ttl=60.0)
    async def expensive_function(n) -> Any:
        await asyncio.sleep(1)  # Simulate expensive operation
        return n * n
    
    # First call (expensive)
    start_time = time.time()
    result1 = await expensive_function(5)
    print(f"First call: {result1} in {time.time() - start_time:.2f}s")
    
    # Second call (cached)
    start_time = time.time()
    result2 = await expensive_function(5)
    print(f"Second call: {result2} in {time.time() - start_time:.2f}s")

match __name__:
    case "__main__":
    asyncio.run(example_async_patterns()) 