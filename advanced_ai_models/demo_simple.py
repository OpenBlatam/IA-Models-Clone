from typing_extensions import Literal, TypedDict
from typing import Any, List, Dict, Optional, Union, Tuple
# Constants
MAX_CONNECTIONS = 1000

# Constants
MAX_RETRIES = 100

# Constants
TIMEOUT_SECONDS = 60

# Constants
BUFFER_SIZE = 1024

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import math
        import traceback
from typing import Any, List, Dict, Optional
import asyncio
#!/usr/bin/env python3
"""
Simple Advanced AI Models Demo - Basic Version
Works with minimal dependencies
"""


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")


class SimpleTransformerModel(nn.Module):
    """Simple transformer model for demo purposes."""
    
    def __init__(self, vocab_size: int = 1000, d_model: int = 256, n_layers: int = 4):
        
    """__init__ function."""
super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_layers = n_layers
        
        # Token embedding
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        
        # Positional encoding
        self.pos_encoding = nn.Parameter(torch.randn(1, 100, d_model))
        
        # Transformer layers
        self.transformer_layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=8,
                dim_feedforward=d_model * 4,
                dropout=0.1,
                batch_first=True
            ) for _ in range(n_layers)
        ])
        
        # Output projection
        self.output_projection = nn.Linear(d_model, vocab_size)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self) -> Any:
        """Initialize model weights."""
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.output_projection.weight, std=0.02)
        nn.init.zeros_(self.output_projection.bias)
    
    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Forward pass."""
        batch_size, seq_len = input_ids.size()
        
        # Token embeddings
        embeddings = self.token_embedding(input_ids)
        
        # Add positional encoding
        if seq_len <= self.pos_encoding.size(1):
            embeddings = embeddings + self.pos_encoding[:, :seq_len, :]
        
        # Pass through transformer layers
        hidden_states = embeddings
        for layer in self.transformer_layers:
            hidden_states = layer(hidden_states)
        
        # Output projection
        logits = self.output_projection(hidden_states)
        
        return {"logits": logits, "hidden_states": hidden_states}


class SimpleVisionModel(nn.Module):
    """Simple vision model for demo purposes."""
    
    def __init__(self, num_classes: int = 10):
        
    """__init__ function."""
super().__init__()
        self.num_classes = num_classes
        
        # Simple CNN
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass."""
        features = self.features(x)
        return self.classifier(features)


class SimpleDiffusionModel(nn.Module):
    """Simple diffusion model for demo purposes."""
    
    def __init__(self, image_size: int = 32, channels: int = 3):
        
    """__init__ function."""
super().__init__()
        self.image_size = image_size
        self.channels = channels
        
        # Simple UNet-like architecture
        self.encoder = nn.Sequential(
            nn.Conv2d(channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        
        self.middle = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU()
        )
        
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 64, 2, stride=2),
            nn.Conv2d(64, channels, 3, padding=1)
        )
    
    def forward(self, x: torch.Tensor, timestep: torch.Tensor) -> torch.Tensor:
        """Forward pass."""
        # Add timestep information
        timestep_emb = timestep.view(-1, 1, 1, 1).expand(-1, 1, x.size(2), x.size(3))
        x = torch.cat([x, timestep_emb], dim=1)
        
        # Encode
        encoded = self.encoder(x)
        
        # Middle
        middle = self.middle(encoded)
        
        # Decode
        decoded = self.decoder(middle)
        
        return decoded


class SimpleLLMModel(nn.Module):
    """Simple LLM model for demo purposes."""
    
    def __init__(self, vocab_size: int = 1000, hidden_size: int = 256):
        
    """__init__ function."""
super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # Embedding
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        
        # LSTM layers
        self.lstm = nn.LSTM(
            hidden_size,
            hidden_size,
            num_layers=2,
            dropout=0.1,
            batch_first=True
        )
        
        # Output projection
        self.output = nn.Linear(hidden_size, vocab_size)
    
    def forward(self, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Forward pass."""
        # Embeddings
        embeddings = self.embedding(input_ids)
        
        # LSTM
        lstm_out, _ = self.lstm(embeddings)
        
        # Output projection
        logits = self.output(lstm_out)
        
        return {"logits": logits, "hidden_states": lstm_out}
    
    def generate(self, prompt: str, max_length: int = 50) -> str:
        """Simple text generation."""
        # Convert prompt to tokens (simplified)
        tokens = [ord(c) % self.vocab_size for c in prompt[:10]]
        input_ids = torch.tensor([tokens], device=device)
        
        # Generate
        with torch.no_grad():
            for _ in range(max_length):
                outputs = self.forward(input_ids)
                next_token = torch.argmax(outputs["logits"][:, -1, :], dim=-1, keepdim=True)
                input_ids = torch.cat([input_ids, next_token], dim=1)
        
        # Convert back to text (simplified)
        result = "".join([chr(t % 128) for t in input_ids[0].tolist()])
        return result


class SimpleAIModelsDemo:
    """Simple demo for advanced AI models."""
    
    def __init__(self) -> Any:
        self.models = {}
        self.results = {}
        self.performance_metrics = {}
        
        # Initialize models
        self._initialize_models()
        
        logger.info("üöÄ Simple Advanced AI Models Demo initialized")
    
    def _initialize_models(self) -> Any:
        """Initialize all models."""
        logger.info("Initializing models...")
        
        # Transformer Model
        self.models["transformer"] = SimpleTransformerModel(
            vocab_size=1000,
            d_model=256,
            n_layers=4
        ).to(device)
        
        # Vision Model
        self.models["vision"] = SimpleVisionModel(
            num_classes=10
        ).to(device)
        
        # Diffusion Model
        self.models["diffusion"] = SimpleDiffusionModel(
            image_size=32,
            channels=3
        ).to(device)
        
        # LLM Model
        self.models["llm"] = SimpleLLMModel(
            vocab_size=1000,
            hidden_size=256
        ).to(device)
        
        logger.info("‚úÖ All models initialized")
    
    def demo_transformer_models(self) -> Dict[str, Any]:
        """Demo transformer models functionality."""
        logger.info("üîÑ Running Transformer Models Demo...")
        
        results = {
            "transformer_forward": {},
            "transformer_generation": {}
        }
        
        try:
            # Test Transformer Forward Pass
            logger.info("Testing Transformer Forward Pass...")
            
            batch_size, seq_len = 2, 10
            input_ids = torch.randint(0, 1000, (batch_size, seq_len)).to(device)
            
            start_time = time.time()
            transformer_model = self.models["transformer"]
            outputs = transformer_model(input_ids)
            forward_time = time.time() - start_time
            
            results["transformer_forward"] = {
                "success": True,
                "input_shape": list(input_ids.shape),
                "output_shape": list(outputs["logits"].shape),
                "execution_time": forward_time,
                "vocab_size": 1000
            }
            
            # Test Simple Generation
            logger.info("Testing Simple Generation...")
            
            start_time = time.time()
            # Simple generation simulation
            generated_tokens = torch.randint(0, 1000, (1, 20)).to(device)
            generation_time = time.time() - start_time
            
            results["transformer_generation"] = {
                "success": True,
                "generated_length": 20,
                "execution_time": generation_time,
                "sample_tokens": generated_tokens[0][:5].tolist()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Transformer demo failed: {e}")
            results["error"] = str(e)
        
        self.results["transformer"] = results
        return results
    
    def demo_vision_models(self) -> Dict[str, Any]:
        """Demo vision models functionality."""
        logger.info("üîÑ Running Vision Models Demo...")
        
        results = {
            "vision_forward": {},
            "image_classification": {}
        }
        
        try:
            # Test Vision Model Forward Pass
            logger.info("Testing Vision Model Forward Pass...")
            
            batch_size, channels, height, width = 2, 3, 32, 32
            dummy_image = torch.randn(batch_size, channels, height, width).to(device)
            
            start_time = time.time()
            vision_model = self.models["vision"]
            outputs = vision_model(dummy_image)
            forward_time = time.time() - start_time
            
            results["vision_forward"] = {
                "success": True,
                "input_shape": list(dummy_image.shape),
                "output_shape": list(outputs.shape),
                "execution_time": forward_time,
                "num_classes": 10
            }
            
            # Test Image Classification
            logger.info("Testing Image Classification...")
            
            start_time = time.time()
            predictions = F.softmax(outputs, dim=1)
            top_predictions = torch.topk(predictions, 3, dim=1)
            classification_time = time.time() - start_time
            
            results["image_classification"] = {
                "success": True,
                "top_predictions": top_predictions.indices[0].tolist(),
                "probabilities": top_predictions.values[0].tolist(),
                "execution_time": classification_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå Vision demo failed: {e}")
            results["error"] = str(e)
        
        self.results["vision"] = results
        return results
    
    def demo_diffusion_models(self) -> Dict[str, Any]:
        """Demo diffusion models functionality."""
        logger.info("üîÑ Running Diffusion Models Demo...")
        
        results = {
            "diffusion_forward": {},
            "noise_generation": {}
        }
        
        try:
            # Test Diffusion Model Forward Pass
            logger.info("Testing Diffusion Model Forward Pass...")
            
            batch_size, channels, height, width = 1, 3, 32, 32
            noisy_image = torch.randn(batch_size, channels, height, width).to(device)
            timestep = torch.randint(0, 1000, (batch_size,)).to(device)
            
            start_time = time.time()
            diffusion_model = self.models["diffusion"]
            outputs = diffusion_model(noisy_image, timestep)
            forward_time = time.time() - start_time
            
            results["diffusion_forward"] = {
                "success": True,
                "input_shape": list(noisy_image.shape),
                "output_shape": list(outputs.shape),
                "execution_time": forward_time,
                "timestep": timestep.item()
            }
            
            # Test Noise Generation
            logger.info("Testing Noise Generation...")
            
            start_time = time.time()
            # Simulate noise addition
            clean_image = torch.randn(batch_size, channels, height, width).to(device)
            noise = torch.randn_like(clean_image)
            noisy_result = clean_image + 0.1 * noise
            noise_time = time.time() - start_time
            
            results["noise_generation"] = {
                "success": True,
                "clean_shape": list(clean_image.shape),
                "noisy_shape": list(noisy_result.shape),
                "noise_level": 0.1,
                "execution_time": noise_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå Diffusion demo failed: {e}")
            results["error"] = str(e)
        
        self.results["diffusion"] = results
        return results
    
    def demo_llm_models(self) -> Dict[str, Any]:
        """Demo LLM models functionality."""
        logger.info("üîÑ Running LLM Models Demo...")
        
        results = {
            "llm_forward": {},
            "text_generation": {}
        }
        
        try:
            # Test LLM Forward Pass
            logger.info("Testing LLM Forward Pass...")
            
            batch_size, seq_len = 1, 10
            input_ids = torch.randint(0, 1000, (batch_size, seq_len)).to(device)
            
            start_time = time.time()
            llm_model = self.models["llm"]
            outputs = llm_model(input_ids)
            forward_time = time.time() - start_time
            
            results["llm_forward"] = {
                "success": True,
                "input_shape": list(input_ids.shape),
                "output_shape": list(outputs["logits"].shape),
                "execution_time": forward_time,
                "vocab_size": 1000
            }
            
            # Test Text Generation
            logger.info("Testing Text Generation...")
            
            start_time = time.time()
            generated_text = llm_model.generate("Hello", max_length=20)
            generation_time = time.time() - start_time
            
            results["text_generation"] = {
                "success": True,
                "input_prompt": "Hello",
                "generated_text": generated_text,
                "execution_time": generation_time,
                "text_length": len(generated_text)
            }
            
        except Exception as e:
            logger.error(f"‚ùå LLM demo failed: {e}")
            results["error"] = str(e)
        
        self.results["llm"] = results
        return results
    
    def run_performance_benchmarks(self) -> Dict[str, Any]:
        """Run performance benchmarks."""
        logger.info("üîÑ Running Performance Benchmarks...")
        
        benchmarks = {
            "memory_usage": {},
            "inference_speed": {},
            "model_sizes": {}
        }
        
        try:
            # Memory Usage Benchmark
            logger.info("Testing Memory Usage...")
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                initial_memory = torch.cuda.memory_allocated()
                
                # Test with different model sizes
                model_sizes = [256, 512, 1024]
                memory_results = {}
                
                for size in model_sizes:
                    try:
                        # Create a simple model
                        model = nn.Sequential(
                            nn.Linear(size, size),
                            nn.ReLU(),
                            nn.Linear(size, size)
                        ).to(device)
                        
                        # Test input
                        x = torch.randn(1, size).to(device)
                        _ = model(x)
                        
                        current_memory = torch.cuda.memory_allocated()
                        memory_used = current_memory - initial_memory
                        
                        memory_results[f"size_{size}"] = {
                            "memory_mb": memory_used / 1024 / 1024,
                            "parameters": sum(p.numel() for p in model.parameters())
                        }
                        
                        del model
                        torch.cuda.empty_cache()
                        
                    except Exception as e:
                        memory_results[f"size_{size}"] = {"error": str(e)}
                
                benchmarks["memory_usage"] = memory_results
            else:
                benchmarks["memory_usage"] = {"note": "GPU not available"}
            
            # Inference Speed Benchmark
            logger.info("Testing Inference Speed...")
            
            # Test different batch sizes
            batch_sizes = [1, 4, 8]
            speed_results = {}
            
            for batch_size in batch_sizes:
                try:
                    # Create test model
                    model = nn.Sequential(
                        nn.Linear(512, 512),
                        nn.ReLU(),
                        nn.Linear(512, 512)
                    ).to(device)
                    
                    x = torch.randn(batch_size, 512).to(device)
                    
                    # Warmup
                    for _ in range(10):
                        _ = model(x)
                    
                    # Benchmark
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    start_time = time.time()
                    
                    for _ in range(100):
                        _ = model(x)
                    
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    end_time = time.time()
                    
                    avg_time = (end_time - start_time) / 100
                    throughput = batch_size / avg_time
                    
                    speed_results[f"batch_{batch_size}"] = {
                        "avg_inference_time_ms": avg_time * 1000,
                        "throughput_samples_per_sec": throughput,
                        "batch_size": batch_size
                    }
                    
                    del model
                    
                except Exception as e:
                    speed_results[f"batch_{batch_size}"] = {"error": str(e)}
            
            benchmarks["inference_speed"] = speed_results
            
            # Model Sizes
            logger.info("Calculating Model Sizes...")
            
            model_sizes = {}
            for name, model in self.models.items():
                try:
                    total_params = sum(p.numel() for p in model.parameters())
                    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                    
                    # Calculate size in MB
                    param_size = 0
                    for param in model.parameters():
                        param_size += param.nelement() * param.element_size()
                    
                    size_mb = param_size / 1024 / 1024
                    
                    model_sizes[name] = {
                        "total_parameters": total_params,
                        "trainable_parameters": trainable_params,
                        "size_mb": size_mb
                    }
                    
                except Exception as e:
                    model_sizes[name] = {"error": str(e)}
            
            benchmarks["model_sizes"] = model_sizes
            
        except Exception as e:
            logger.error(f"‚ùå Performance benchmarks failed: {e}")
            benchmarks["error"] = str(e)
        
        self.performance_metrics = benchmarks
        return benchmarks
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive demo report."""
        logger.info("üìä Generating Comprehensive Report...")
        
        report = {
            "demo_summary": {
                "total_models": len(self.models),
                "available_models": list(self.models.keys()),
                "device": str(device),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            "model_results": self.results,
            "performance_metrics": self.performance_metrics,
            "recommendations": [],
            "system_info": {
                "torch_version": torch.__version__,
                "cuda_available": torch.cuda.is_available(),
                "cuda_version": torch.version.cuda if torch.cuda.is_available() else "N/A",
                "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
        }
        
        # Generate recommendations
        recommendations = []
        
        # Check model availability
        available_models = sum(1 for results in self.results.values() 
                             if any(test.get("success", False) for test in results.values() 
                                   if isinstance(test, dict)))
        
        if available_models == 0:
            recommendations.append("‚ö†Ô∏è No models are currently available. Check dependencies and model loading.")
        elif available_models < len(self.results):
            recommendations.append("‚ö†Ô∏è Some models failed to load. Consider using smaller models or checking GPU memory.")
        
        # Performance recommendations
        if "performance_metrics" in report and "inference_speed" in report["performance_metrics"]:
            speed_data = report["performance_metrics"]["inference_speed"]
            avg_times = [data.get("avg_inference_time_ms", 0) for data in speed_data.values() 
                        if "error" not in data]
            
            if avg_times:
                avg_time = sum(avg_times) / len(avg_times)
                if avg_time > 100:
                    recommendations.append("üêå Inference is slow. Consider using model optimization techniques.")
                elif avg_time < 10:
                    recommendations.append("‚ö° Excellent inference speed! Consider increasing batch size for better throughput.")
        
        # Memory recommendations
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            memory_usage = memory_used / memory_total
            
            if memory_usage > 0.8:
                recommendations.append("üíæ High GPU memory usage. Consider using model quantization or smaller models.")
            elif memory_usage < 0.3:
                recommendations.append("üíæ Low GPU memory usage. Consider using larger models for better performance.")
        
        report["recommendations"] = recommendations
        
        return report
    
    def run_complete_demo(self) -> Dict[str, Any]:
        """Run complete demo with all components."""
        logger.info("üöÄ Starting Complete Simple Advanced AI Models Demo...")
        
        # Run all demos
        logger.info("=" * 60)
        logger.info("üîÑ TRANSFORMER MODELS DEMO")
        logger.info("=" * 60)
        transformer_results = self.demo_transformer_models()
        
        logger.info("=" * 60)
        logger.info("üîÑ VISION MODELS DEMO")
        logger.info("=" * 60)
        vision_results = self.demo_vision_models()
        
        logger.info("=" * 60)
        logger.info("üîÑ DIFFUSION MODELS DEMO")
        logger.info("=" * 60)
        diffusion_results = self.demo_diffusion_models()
        
        logger.info("=" * 60)
        logger.info("üîÑ LLM MODELS DEMO")
        logger.info("=" * 60)
        llm_results = self.demo_llm_models()
        
        logger.info("=" * 60)
        logger.info("üîÑ PERFORMANCE BENCHMARKS")
        logger.info("=" * 60)
        performance_results = self.run_performance_benchmarks()
        
        logger.info("=" * 60)
        logger.info("üìä GENERATING REPORT")
        logger.info("=" * 60)
        report = self.generate_comprehensive_report()
        
        logger.info("=" * 60)
        logger.info("‚úÖ COMPLETE DEMO FINISHED")
        logger.info("=" * 60)
        
        return {
            "transformer_results": transformer_results,
            "vision_results": vision_results,
            "diffusion_results": diffusion_results,
            "llm_results": llm_results,
            "performance_results": performance_results,
            "comprehensive_report": report
        }


def main():
    """Main demo execution."""
    print("üöÄ Simple Advanced AI Models Demo")
    print("=" * 50)
    print("Deep Learning, Transformers, Diffusion Models & LLMs")
    print("=" * 50)
    
    # Initialize demo
    demo = SimpleAIModelsDemo()
    
    # Run complete demo
    results = demo.run_complete_demo()
    
    # Print summary
    print("\nüìä DEMO SUMMARY")
    print("=" * 50)
    
    # Model availability
    total_tests = 0
    successful_tests = 0
    
    for model_type, model_results in results.items():
        if model_type != "comprehensive_report" and model_type != "performance_results":
            print(f"\nüîß {model_type.upper().replace('_', ' ')}:")
            
            for test_name, test_result in model_results.items():
                if isinstance(test_result, dict) and "success" in test_result:
                    total_tests += 1
                    if test_result["success"]:
                        successful_tests += 1
                        print(f"  ‚úÖ {test_name}: Success")
                    else:
                        print(f"  ‚ùå {test_name}: Failed")
    
    success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
    print(f"\nüìà Overall Success Rate: {success_rate:.1f}% ({successful_tests}/{total_tests})")
    
    # Performance summary
    if "performance_results" in results:
        perf = results["performance_results"]
        if "inference_speed" in perf:
            speed_data = perf["inference_speed"]
            avg_times = [data.get("avg_inference_time_ms", 0) for data in speed_data.values() 
                        if "error" not in data]
            if avg_times:
                avg_time = sum(avg_times) / len(avg_times)
                print(f"‚ö° Average Inference Time: {avg_time:.2f} ms")
    
    # System info
    print(f"\nüíª System Info:")
    print(f"  Device: {device}")
    print(f"  PyTorch: {torch.__version__}")
    print(f"  CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name()}")
        print(f"  Memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    print("\nüéâ Demo completed successfully!")
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        
        # Save results to file
        output_file = "simple_advanced_ai_models_demo_results.json"
        with open(output_file, 'w') as f:
    try:
        pass
    except Exception as e:
        print(f"Error: {e}")
            # Convert tensors to lists for JSON serialization
            def convert_tensors(obj) -> Any:
                if isinstance(obj, torch.Tensor):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_tensors(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_tensors(item) for item in obj]
                else:
                    return obj
            
            json.dump(convert_tensors(results), f, indent=2)
        
        print(f"\nüíæ Results saved to: {output_file}")
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        traceback.print_exc() 