# Advanced Training Optimization System Configuration
# HeyGen AI Enterprise - Training Optimization Settings

# =============================================================================
# CURRICULUM LEARNING CONFIGURATION
# =============================================================================
curriculum_learning:
  enabled: true
  strategy: "adaptive"  # Options: "linear", "exponential", "adaptive"
  curriculum_epochs: 100
  difficulty_threshold: 0.8
  adaptive_settings:
    performance_window: 5  # Number of epochs to consider for trend analysis
    difficulty_increase_fast: 0.02  # Fast difficulty increase for improving performance
    difficulty_increase_slow: 0.01  # Slow difficulty increase for stable performance
    difficulty_decrease: -0.01      # Difficulty decrease for declining performance
    min_difficulty: 0.1
    max_difficulty: 1.0

# =============================================================================
# META-LEARNING CONFIGURATION
# =============================================================================
meta_learning:
  enabled: true
  meta_learning_rate: 0.001
  meta_update_steps: 5
  enable_few_shot: true
  few_shot_settings:
    support_set_size: 5
    query_set_size: 10
    adaptation_steps: 3
    meta_batch_size: 4
  optimization_settings:
    inner_loop_lr: 0.01
    outer_loop_lr: 0.001
    gradient_clip_norm: 1.0

# =============================================================================
# MULTI-TASK LEARNING CONFIGURATION
# =============================================================================
multi_task_learning:
  enabled: true
  task_weighting: "uncertainty"  # Options: "equal", "uncertainty", "dynamic"
  enable_task_scheduling: true
  uncertainty_settings:
    performance_history_length: 10
    min_uncertainty: 0.01  # Avoid division by zero
    uncertainty_decay: 0.95
  dynamic_settings:
    improvement_threshold: 0.01
    weight_increase_factor: 1.1
    weight_decrease_factor: 0.9
    min_weight: 0.1
    max_weight: 2.0
  task_scheduling:
    priority_update_interval: 5  # epochs
    priority_increase_factor: 1.1
    priority_decrease_factor: 0.9
    min_priority: 0.1
    max_priority: 2.0

# =============================================================================
# ADAPTIVE TRAINING SCHEDULING CONFIGURATION
# =============================================================================
adaptive_scheduling:
  enabled: true
  adaptive_metric: "loss"  # Options: "loss", "accuracy", "gradient_norm"
  adaptation_threshold: 0.1
  learning_rate_adaptation:
    enabled: true
    increase_factor: 1.1
    decrease_factor: 0.9
    min_lr: 1e-6
    max_lr: 0.1
    adaptation_window: 3  # epochs
  batch_size_adaptation:
    enabled: true
    memory_under_utilization_threshold: 0.7
    memory_over_utilization_threshold: 0.9
    batch_size_increase_factor: 2
    batch_size_decrease_factor: 0.5
    min_batch_size: 8
    max_batch_size: 128
  optimizer_adaptation:
    enabled: true
    switch_to_adam_threshold: 0.05
    switch_to_sgd_threshold: 0.02

# =============================================================================
# ADVANCED LOSS FUNCTIONS AND REGULARIZATION
# =============================================================================
advanced_loss:
  enabled: true
  enable_regularization: true
  regularization_strength: 0.01
  loss_functions:
    classification: "cross_entropy"  # Options: "cross_entropy", "focal_loss", "label_smoothing"
    regression: "mse"               # Options: "mse", "mae", "huber_loss"
    custom_losses: []
  regularization:
    l1_strength: 0.001
    l2_strength: 0.01
    dropout_rate: 0.1
    label_smoothing: 0.1
    mixup_alpha: 0.2

# =============================================================================
# TRAINING ACCELERATION CONFIGURATION
# =============================================================================
training_acceleration:
  enable_mixed_precision: true
  enable_gradient_accumulation: true
  accumulation_steps: 4
  enable_early_stopping: true
  early_stopping:
    patience: 10
    min_delta: 0.001
    restore_best_weights: true
  gradient_accumulation:
    sync_batch_norm: true
    accumulate_grad_before_optimizer_step: true
  mixed_precision:
    dtype: "float16"  # Options: "float16", "bfloat16"
    scaler: "grad_scaler"  # Options: "grad_scaler", "apex"
    loss_scaling: true

# =============================================================================
# PERFORMANCE MONITORING CONFIGURATION
# =============================================================================
performance_monitoring:
  enabled: true
  monitoring_interval: 2  # seconds
  metrics_to_track:
    - "loss"
    - "accuracy"
    - "learning_rate"
    - "gradient_norm"
    - "memory_usage"
    - "gpu_utilization"
    - "training_time"
  alerting:
    enabled: true
    loss_increase_threshold: 0.1
    memory_usage_threshold: 0.9
    gpu_utilization_threshold: 0.95

# =============================================================================
# INTEGRATION SETTINGS
# =============================================================================
integration:
  with_performance_optimizer: true
  with_memory_management: true
  with_analytics_engine: true
  with_cross_platform_optimization: true
  data_sharing:
    enable_metrics_sharing: true
    enable_config_sharing: true
    enable_model_sharing: false

# =============================================================================
# LOGGING AND DEBUGGING CONFIGURATION
# =============================================================================
logging:
  level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"
  enable_tensorboard: true
  enable_wandb: false
  log_interval: 100  # steps
  save_checkpoints: true
  checkpoint_interval: 1000  # steps
  max_checkpoints: 5

# =============================================================================
# EXPERIMENTAL FEATURES
# =============================================================================
experimental:
  enable_curriculum_sampling: false
  enable_dynamic_architecture: false
  enable_neural_architecture_search: false
  enable_hyperparameter_optimization: false
  enable_automated_data_augmentation: false

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================
deployment:
  environment: "development"  # Options: "development", "staging", "production"
  enable_auto_scaling: false
  max_workers: 4
  enable_distributed_training: false
  distributed_settings:
    backend: "nccl"  # Options: "nccl", "gloo", "mpi"
    init_method: "env://"
    world_size: 1
    rank: 0
