# Advanced AutoML System Configuration for HeyGen AI Enterprise
# Comprehensive settings for Neural Architecture Search, Hyperparameter Optimization,
# and Intelligent Model Selection

# General AutoML Configuration
general:
  enabled: true
  log_level: "INFO"
  enable_parallel_processing: true
  max_parallel_jobs: 4
  enable_early_stopping: true
  enable_checkpointing: true
  checkpoint_dir: "automl_checkpoints"
  results_dir: "automl_results"

# Neural Architecture Search (NAS) Configuration
neural_architecture_search:
  enabled: true
  strategy: "evolutionary"  # evolutionary, reinforcement, bayesian
  max_trials: 100
  population_size: 20
  generations: 10
  
  # Evolutionary Strategy Settings
  evolutionary:
    crossover_rate: 0.8
    mutation_rate: 0.2
    selection_method: "tournament"  # tournament, roulette, rank
    tournament_size: 3
    elitism_rate: 0.1
    
  # Reinforcement Learning Strategy Settings
  reinforcement:
    agent_type: "ppo"  # ppo, a3c, dqn
    learning_rate: 0.001
    exploration_rate: 0.1
    memory_size: 10000
    
  # Bayesian Strategy Settings
  bayesian:
    acquisition_function: "ei"  # ei, pi, ucb
    n_initial_points: 10
    random_state: 42
    
  # Architecture Constraints
  constraints:
    max_layers: 20
    max_parameters: 100000000  # 100M
    min_accuracy: 0.7
    max_inference_time: 0.1  # seconds
    max_memory_usage: 8.0  # GB

# Hyperparameter Optimization (HPO) Configuration
hyperparameter_optimization:
  enabled: true
  strategy: "optuna"  # optuna, ray_tune, hyperopt
  max_trials: 200
  timeout_hours: 24
  enable_pruning: true
  enable_parallel_trials: true
  
  # Optuna Settings
  optuna:
    sampler: "tpe"  # tpe, random, cmaes, nsgaii
    pruner: "median"  # median, hyperband, threshold
    study_name: "heygen_ai_hpo"
    storage: "sqlite:///optuna_studies.db"
    
  # Ray Tune Settings
  ray_tune:
    scheduler: "asha"  # asha, hyperband, pbt
    search_algorithm: "optuna"
    num_samples: 200
    resources_per_trial:
      cpu: 2
      gpu: 0.5
    max_concurrent_trials: 4
    
  # Hyperopt Settings
  hyperopt:
    algorithm: "tpe"  # tpe, random, anneal
    max_evals: 200
    trials_timeout: 86400  # 24 hours
    
  # Search Space Templates
  search_spaces:
    transformer:
      learning_rate:
        type: "float"
        low: 1e-5
        high: 1e-2
        log: true
      batch_size:
        type: "categorical"
        choices: [16, 32, 64, 128, 256]
      weight_decay:
        type: "float"
        low: 0.0
        high: 0.1
      warmup_steps:
        type: "int"
        low: 100
        high: 10000
      dropout:
        type: "float"
        low: 0.0
        high: 0.5
      num_layers:
        type: "int"
        low: 2
        high: 12
      embed_dim:
        type: "categorical"
        choices: [128, 256, 512, 768, 1024]
      num_heads:
        type: "categorical"
        choices: [4, 8, 12, 16, 32]
        
    cnn:
      learning_rate:
        type: "float"
        low: 1e-4
        high: 1e-2
        log: true
      batch_size:
        type: "categorical"
        choices: [32, 64, 128, 256, 512]
      dropout:
        type: "float"
        low: 0.0
        high: 0.5
      num_conv_layers:
        type: "int"
        low: 3
        high: 10
      conv_channels:
        type: "categorical"
        choices: [16, 32, 64, 128, 256, 512]
      kernel_size:
        type: "categorical"
        choices: [3, 5, 7, 9]
        
    rnn:
      learning_rate:
        type: "float"
        low: 1e-4
        high: 1e-2
        log: true
      batch_size:
        type: "categorical"
        choices: [16, 32, 64, 128]
      hidden_size:
        type: "categorical"
        choices: [32, 64, 128, 256, 512]
      num_layers:
        type: "int"
        low: 1
        high: 6
      dropout:
        type: "float"
        low: 0.0
        high: 0.5

# Model Selection Configuration
model_selection:
  enabled: true
  selection_metric: "balanced"  # balanced, accuracy, speed, memory, custom
  max_models_to_select: 5
  enable_ensemble_creation: true
  
  # Selection Metrics
  metrics:
    balanced:
      accuracy_weight: 0.5
      speed_weight: 0.25
      memory_weight: 0.25
    accuracy:
      accuracy_weight: 1.0
      speed_weight: 0.0
      memory_weight: 0.0
    speed:
      accuracy_weight: 0.25
      speed_weight: 0.5
      memory_weight: 0.25
    memory:
      accuracy_weight: 0.25
      speed_weight: 0.25
      memory_weight: 0.5
      
  # Ensemble Methods
  ensemble_methods:
    stacking:
      enabled: true
      meta_learner: "logistic_regression"  # logistic_regression, random_forest, xgboost
      cross_validation: 5
      use_proba: true
      
    voting:
      enabled: true
      voting_strategy: "soft"  # soft, hard
      weights: "uniform"  # uniform, custom
      
    blending:
      enabled: true
      validation_split: 0.2
      meta_learner: "linear_regression"
      
  # Model Diversity
  diversity:
    enable_diversity_penalty: true
    diversity_threshold: 0.3
    diversity_metrics: ["architecture", "hyperparameters", "performance"]

# Performance Integration Configuration
performance_integration:
  enabled: true
  performance_threshold: 0.8
  memory_constraint_gb: 16.0
  enable_automatic_optimization: true
  
  # Integration with Performance Systems
  systems:
    advanced_performance_optimizer: true
    performance_benchmarking_suite: true
    advanced_neural_network_optimizer: true
    performance_monitoring_system: true
    
  # Optimization Strategies
  optimization:
    enable_quantization: true
    enable_pruning: true
    enable_kernel_fusion: true
    enable_memory_optimization: true
    enable_compilation: true

# Multi-Objective Optimization
multi_objective:
  enabled: true
  objectives: ["accuracy", "speed", "memory", "energy"]
  optimization_method: "nsga2"  # nsga2, spea2, moead
  
  # Objective Weights
  weights:
    accuracy: 0.4
    speed: 0.3
    memory: 0.2
    energy: 0.1
    
  # Pareto Front
  pareto:
    enable_pareto_front: true
    max_pareto_solutions: 20
    diversity_metric: "crowding_distance"

# Advanced Features
advanced_features:
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001
    monitor: "val_loss"
    
  # Adaptive Optimization
  adaptive_optimization:
    enabled: true
    adaptation_strategy: "dynamic"
    adaptation_threshold: 0.1
    max_adaptations: 5
    
  # Transfer Learning
  transfer_learning:
    enabled: true
    source_tasks: ["imagenet", "coco", "voc"]
    adaptation_method: "fine_tuning"
    
  # Meta-Learning
  meta_learning:
    enabled: true
    meta_learner: "maml"
    few_shot_samples: 5
    meta_update_steps: 3

# Logging and Monitoring
logging:
  level: "INFO"
  enable_file_logging: true
  log_file: "automl.log"
  enable_console_logging: true
  enable_mlflow: true
  
  # MLflow Configuration
  mlflow:
    tracking_uri: "sqlite:///mlflow.db"
    experiment_name: "heygen_ai_automl"
    enable_artifact_logging: true
    log_models: true
    log_parameters: true
    log_metrics: true
    
  # Ray Dashboard
  ray:
    enable_dashboard: true
    dashboard_host: "0.0.0.0"
    dashboard_port: 8265
    enable_logging: true

# Data Management
data:
  enable_data_augmentation: true
  enable_data_validation: true
  enable_data_caching: true
  cache_dir: "automl_cache"
  
  # Data Augmentation
  augmentation:
    enable_random_rotation: true
    enable_random_flip: true
    enable_color_jitter: true
    enable_random_crop: true
    
  # Data Validation
  validation:
    enable_schema_validation: true
    enable_data_quality_checks: true
    enable_outlier_detection: true

# Security and Privacy
security:
  enable_secure_training: false
  enable_differential_privacy: false
  enable_secure_aggregation: false
  
  # Differential Privacy
  differential_privacy:
    epsilon: 1.0
    delta: 1e-5
    noise_scale: 1.0

# Deployment and Export
deployment:
  enable_model_export: true
  export_formats: ["onnx", "torchscript", "tensorrt"]
  enable_model_serving: false
  
  # Model Export
  export:
    onnx:
      enable_optimization: true
      opset_version: 11
    torchscript:
      enable_tracing: true
      enable_scripting: false
    tensorrt:
      enable_fp16: true
      enable_int8: false

# Experimental Features
experimental:
  enable_quantum_enhanced: false
  enable_federated_learning: false
  enable_edge_optimization: false
  
  # Quantum Enhancement
  quantum:
    backend: "qiskit"
    num_qubits: 4
    optimization_level: 2
    
  # Federated Learning
  federated:
    num_clients: 5
    aggregation_method: "fedavg"
    privacy_budget: 1.0

# Performance Monitoring
monitoring:
  enable_real_time_monitoring: true
  enable_performance_tracking: true
  enable_resource_monitoring: true
  
  # Metrics to Track
  metrics:
    - "accuracy"
    - "inference_time"
    - "memory_usage"
    - "gpu_utilization"
    - "cpu_utilization"
    - "training_time"
    - "convergence_rate"
    
  # Alerts
  alerts:
    enable_performance_alerts: true
    performance_threshold: 0.8
    enable_resource_alerts: true
    memory_threshold_gb: 14.0
    gpu_threshold_percent: 90.0
