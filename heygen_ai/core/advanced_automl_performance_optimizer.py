"""
Advanced AutoML Performance Optimizer for HeyGen AI Enterprise

This module implements cutting-edge AutoML capabilities with performance optimization:
- Neural Architecture Search (NAS) with performance constraints
- Automated hyperparameter optimization
- Multi-objective optimization (accuracy, speed, memory)
- Performance-aware model selection
- Automated model compression and quantization
- Cross-platform model optimization
- Real-time performance validation
"""

import logging
import os
import time
import gc
import warnings
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from pathlib import Path
from contextlib import contextmanager
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import json
import pickle
from collections import defaultdict, deque
import random

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
from tqdm import tqdm

# AutoML and optimization libraries
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    warnings.warn("optuna not available. Install for advanced AutoML capabilities.")

try:
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    warnings.warn("scikit-learn not available. Install for ML evaluation.")

try:
    import ray
    from ray import tune
    from ray.tune.schedulers import ASHAScheduler
    from ray.tune.search.optuna import OptunaSearch
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False
    warnings.warn("ray[tune] not available. Install for distributed hyperparameter tuning.")

logger = logging.getLogger(__name__)


@dataclass
class AutoMLPerformanceConfig:
    """Configuration for AutoML performance optimization."""
    
    # Search settings
    max_trials: int = 100
    max_time_hours: float = 24.0
    population_size: int = 50
    generations: int = 20
    
    # Performance constraints
    min_accuracy: float = 0.8
    max_inference_time_ms: float = 100.0
    max_memory_mb: float = 1000.0
    max_model_size_mb: float = 100.0
    
    # Optimization objectives
    primary_objective: str = "accuracy"  # accuracy, speed, memory, balanced
    secondary_objectives: List[str] = field(default_factory=lambda: ["speed", "memory"])
    
    # Search strategies
    search_algorithm: str = "evolutionary"  # evolutionary, bayesian, random, grid
    enable_early_stopping: bool = True
    enable_pruning: bool = True
    
    # Model constraints
    min_layers: int = 2
    max_layers: int = 20
    min_neurons: int = 32
    max_neurons: int = 2048
    allowed_activations: List[str] = field(default_factory=lambda: ["relu", "gelu", "swish", "mish"])
    allowed_regularization: List[str] = field(default_factory=lambda: ["dropout", "batch_norm", "layer_norm"])
    
    # Performance optimization
    enable_quantization: bool = True
    enable_pruning: bool = True
    enable_knowledge_distillation: bool = True
    enable_mixed_precision: bool = True
    
    # Advanced features
    enable_multi_gpu_search: bool = False
    enable_distributed_search: bool = False
    enable_meta_learning: bool = False
    enable_transfer_learning: bool = True


class ModelArchitecture:
    """Represents a neural network architecture configuration."""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.architecture_id = self._generate_id()
        self.performance_metrics = {}
        self.training_history = []
        
    def _generate_id(self) -> str:
        """Generate unique architecture ID."""
        return f"arch_{int(time.time() * 1000)}_{random.randint(1000, 9999)}"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert architecture to dictionary."""
        return {
            "architecture_id": self.architecture_id,
            "config": self.config,
            "performance_metrics": self.performance_metrics,
            "training_history": self.training_history
        }
    
    def get_complexity_score(self) -> float:
        """Calculate architecture complexity score."""
        try:
            total_params = 0
            total_layers = len(self.config.get("layers", []))
            
            for layer in self.config.get("layers", []):
                if layer["type"] == "linear":
                    input_size = layer.get("input_size", 0)
                    output_size = layer.get("output_size", 0)
                    total_params += input_size * output_size + output_size
                elif layer["type"] == "conv":
                    in_channels = layer.get("in_channels", 0)
                    out_channels = layer.get("out_channels", 0)
                    kernel_size = layer.get("kernel_size", 3)
                    total_params += in_channels * out_channels * kernel_size * kernel_size + out_channels
            
            # Normalize by number of layers
            complexity = total_params / max(total_layers, 1)
            return complexity
            
        except Exception as e:
            logger.warning(f"Failed to calculate complexity score: {e}")
            return 0.0


class PerformanceEvaluator:
    """Evaluates model performance across multiple metrics."""
    
    def __init__(self, config: AutoMLPerformanceConfig):
        self.config = config
        self.evaluation_history = []
        
    def evaluate_model(self, model: nn.Module, test_data: DataLoader, 
                      device: torch.device) -> Dict[str, float]:
        """Comprehensive model evaluation."""
        try:
            model.eval()
            model = model.to(device)
            
            # Performance metrics
            inference_time = self._measure_inference_time(model, test_data, device)
            memory_usage = self._measure_memory_usage(model, device)
            model_size = self._measure_model_size(model)
            accuracy = self._measure_accuracy(model, test_data, device)
            
            metrics = {
                "inference_time_ms": inference_time,
                "memory_usage_mb": memory_usage,
                "model_size_mb": model_size,
                "accuracy": accuracy,
                "complexity_score": self._calculate_complexity_score(model),
                "efficiency_score": self._calculate_efficiency_score(accuracy, inference_time, memory_usage)
            }
            
            # Store evaluation
            self.evaluation_history.append({
                "timestamp": time.time(),
                "metrics": metrics,
                "model_config": self._extract_model_config(model)
            })
            
            return metrics
            
        except Exception as e:
            logger.error(f"Model evaluation failed: {e}")
            return {}
    
    def _measure_inference_time(self, model: nn.Module, test_data: DataLoader, 
                               device: torch.device) -> float:
        """Measure average inference time."""
        try:
            model.eval()
            total_time = 0.0
            num_samples = 0
            
            with torch.no_grad():
                for batch_idx, (data, _) in enumerate(test_data):
                    if batch_idx >= 10:  # Limit to 10 batches for speed
                        break
                    
                    data = data.to(device)
                    
                    # Warm up
                    if batch_idx == 0:
                        for _ in range(3):
                            _ = model(data)
                    
                    # Measure inference time
                    torch.cuda.synchronize() if device.type == 'cuda' else None
                    start_time = time.time()
                    
                    _ = model(data)
                    
                    torch.cuda.synchronize() if device.type == 'cuda' else None
                    end_time = time.time()
                    
                    total_time += (end_time - start_time) * 1000  # Convert to ms
                    num_samples += data.size(0)
            
            avg_time = total_time / max(num_samples, 1)
            return avg_time
            
        except Exception as e:
            logger.warning(f"Inference time measurement failed: {e}")
            return 1000.0  # Default high value
    
    def _measure_memory_usage(self, model: nn.Module, device: torch.device) -> float:
        """Measure model memory usage."""
        try:
            if device.type == 'cuda':
                memory_allocated = torch.cuda.memory_allocated(device) / (1024**2)  # MB
                memory_reserved = torch.cuda.memory_reserved(device) / (1024**2)    # MB
                return max(memory_allocated, memory_reserved)
            else:
                # CPU memory estimation
                param_size = sum(p.numel() * p.element_size() for p in model.parameters())
                buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
                return (param_size + buffer_size) / (1024**2)  # MB
                
        except Exception as e:
            logger.warning(f"Memory usage measurement failed: {e}")
            return 1000.0  # Default high value
    
    def _measure_model_size(self, model: nn.Module) -> float:
        """Measure model file size."""
        try:
            param_size = sum(p.numel() * p.element_size() for p in model.parameters())
            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
            return (param_size + buffer_size) / (1024**2)  # MB
            
        except Exception as e:
            logger.warning(f"Model size measurement failed: {e}")
            return 100.0  # Default value
    
    def _measure_accuracy(self, model: nn.Module, test_data: DataLoader, 
                         device: torch.device) -> float:
        """Measure model accuracy."""
        try:
            model.eval()
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_idx, (data, targets) in enumerate(test_data):
                    if batch_idx >= 20:  # Limit to 20 batches for speed
                        break
                    
                    data, targets = data.to(device), targets.to(device)
                    outputs = model(data)
                    
                    _, predicted = torch.max(outputs.data, 1)
                    total += targets.size(0)
                    correct += (predicted == targets).sum().item()
            
            accuracy = correct / max(total, 1)
            return accuracy
            
        except Exception as e:
            logger.warning(f"Accuracy measurement failed: {e}")
            return 0.0
    
    def _calculate_complexity_score(self, model: nn.Module) -> float:
        """Calculate model complexity score."""
        try:
            total_params = sum(p.numel() for p in model.parameters())
            total_layers = len(list(model.modules()))
            
            # Normalize complexity
            complexity = total_params / max(total_layers, 1)
            return min(complexity / 1000, 1.0)  # Normalize to 0-1
            
        except Exception as e:
            logger.warning(f"Complexity score calculation failed: {e}")
            return 0.5
    
    def _calculate_efficiency_score(self, accuracy: float, inference_time: float, 
                                  memory_usage: float) -> float:
        """Calculate overall efficiency score."""
        try:
            # Normalize metrics to 0-1 scale
            acc_score = accuracy
            time_score = max(0, 1 - (inference_time / 1000))  # 1000ms = 0 score
            memory_score = max(0, 1 - (memory_usage / 1000))  # 1000MB = 0 score
            
            # Weighted average
            efficiency = (0.5 * acc_score + 0.3 * time_score + 0.2 * memory_score)
            return max(0, min(1, efficiency))
            
        except Exception as e:
            logger.warning(f"Efficiency score calculation failed: {e}")
            return 0.0
    
    def _extract_model_config(self, model: nn.Module) -> Dict[str, Any]:
        """Extract model configuration for analysis."""
        try:
            config = {
                "num_layers": 0,
                "num_parameters": 0,
                "layer_types": [],
                "activation_functions": [],
                "regularization_methods": []
            }
            
            for name, module in model.named_modules():
                if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d)):
                    config["num_layers"] += 1
                    config["layer_types"].append(type(module).__name__)
                
                if isinstance(module, (nn.ReLU, nn.GELU, nn.SiLU, nn.Mish)):
                    config["activation_functions"].append(type(module).__name__)
                
                if isinstance(module, (nn.Dropout, nn.BatchNorm1d, nn.BatchNorm2d, nn.LayerNorm)):
                    config["regularization_methods"].append(type(module).__name__)
            
            config["num_parameters"] = sum(p.numel() for p in model.parameters())
            
            return config
            
        except Exception as e:
            logger.warning(f"Model config extraction failed: {e}")
            return {}


class EvolutionaryArchitectureSearch:
    """Evolutionary algorithm for neural architecture search."""
    
    def __init__(self, config: AutoMLPerformanceConfig):
        self.config = config
        self.population = []
        self.generation = 0
        self.best_architectures = []
        self.search_history = []
        
    def initialize_population(self, input_size: int, output_size: int):
        """Initialize random population of architectures."""
        try:
            self.population = []
            
            for _ in range(self.config.population_size):
                architecture = self._generate_random_architecture(input_size, output_size)
                self.population.append(architecture)
            
            logger.info(f"Initialized population of {len(self.population)} architectures")
            
        except Exception as e:
            logger.error(f"Population initialization failed: {e}")
    
    def _generate_random_architecture(self, input_size: int, output_size: int) -> ModelArchitecture:
        """Generate a random neural architecture."""
        try:
            num_layers = random.randint(self.config.min_layers, self.config.max_layers)
            layers = []
            
            current_size = input_size
            
            for i in range(num_layers):
                if i == num_layers - 1:
                    # Output layer
                    layer_size = output_size
                else:
                    # Hidden layer
                    layer_size = random.randint(self.config.min_neurons, self.config.max_neurons)
                
                layer_config = {
                    "type": "linear",
                    "input_size": current_size,
                    "output_size": layer_size,
                    "activation": random.choice(self.config.allowed_activations),
                    "regularization": random.choice(self.config.allowed_regularization) if random.random() > 0.5 else None
                }
                
                layers.append(layer_config)
                current_size = layer_size
            
            config = {
                "input_size": input_size,
                "output_size": output_size,
                "layers": layers,
                "optimizer": random.choice(["adam", "sgd", "adamw"]),
                "learning_rate": random.uniform(1e-5, 1e-2),
                "batch_size": random.choice([16, 32, 64, 128])
            }
            
            return ModelArchitecture(config)
            
        except Exception as e:
            logger.error(f"Random architecture generation failed: {e}")
            # Return simple fallback architecture
            return ModelArchitecture({
                "input_size": input_size,
                "output_size": output_size,
                "layers": [
                    {"type": "linear", "input_size": input_size, "output_size": 64, "activation": "relu"},
                    {"type": "linear", "input_size": 64, "output_size": output_size, "activation": "none"}
                ]
            })
    
    def evolve_population(self, fitness_scores: List[float]):
        """Evolve population based on fitness scores."""
        try:
            if len(fitness_scores) != len(self.population):
                logger.warning("Fitness scores length mismatch")
                return
            
            # Sort population by fitness
            sorted_population = [x for _, x in sorted(zip(fitness_scores, self.population), reverse=True)]
            
            # Keep top performers
            elite_size = max(1, len(self.population) // 4)
            new_population = sorted_population[:elite_size]
            
            # Generate offspring through crossover and mutation
            while len(new_population) < self.config.population_size:
                parent1 = random.choice(sorted_population[:len(sorted_population)//2])
                parent2 = random.choice(sorted_population[:len(sorted_population)//2])
                
                offspring = self._crossover(parent1, parent2)
                offspring = self._mutate(offspring)
                
                new_population.append(offspring)
            
            self.population = new_population
            self.generation += 1
            
            # Store best architectures
            best_arch = sorted_population[0]
            self.best_architectures.append(best_arch)
            
            logger.info(f"Generation {self.generation} completed. Population evolved.")
            
        except Exception as e:
            logger.error(f"Population evolution failed: {e}")
    
    def _crossover(self, parent1: ModelArchitecture, parent2: ModelArchitecture) -> ModelArchitecture:
        """Perform crossover between two parent architectures."""
        try:
            config1 = parent1.config
            config2 = parent2.config
            
            # Crossover layers
            layers1 = config1["layers"]
            layers2 = config2["layers"]
            
            if len(layers1) == 0 or len(layers2) == 0:
                return parent1
            
            # Random crossover point
            crossover_point = random.randint(1, min(len(layers1), len(layers2)) - 1)
            
            new_layers = layers1[:crossover_point] + layers2[crossover_point:]
            
            # Crossover other parameters
            new_config = {
                "input_size": config1["input_size"],
                "output_size": config1["output_size"],
                "layers": new_layers,
                "optimizer": random.choice([config1["optimizer"], config2["optimizer"]]),
                "learning_rate": random.choice([config1["learning_rate"], config2["learning_rate"]]),
                "batch_size": random.choice([config1["batch_size"], config2["batch_size"]])
            }
            
            return ModelArchitecture(new_config)
            
        except Exception as e:
            logger.warning(f"Crossover failed: {e}")
            return parent1
    
    def _mutate(self, architecture: ModelArchitecture) -> ModelArchitecture:
        """Apply random mutations to architecture."""
        try:
            config = architecture.config.copy()
            
            # Random mutations
            if random.random() < 0.1:  # 10% chance to change number of layers
                layers = config["layers"]
                if len(layers) > 1 and random.random() < 0.5:
                    # Remove random layer
                    remove_idx = random.randint(0, len(layers) - 1)
                    layers.pop(remove_idx)
                else:
                    # Add random layer
                    insert_idx = random.randint(1, len(layers))
                    new_layer = {
                        "type": "linear",
                        "input_size": layers[insert_idx-1]["output_size"] if insert_idx > 0 else config["input_size"],
                        "output_size": random.randint(self.config.min_neurons, self.config.max_neurons),
                        "activation": random.choice(self.config.allowed_activations),
                        "regularization": random.choice(self.config.allowed_regularization) if random.random() > 0.5 else None
                    }
                    layers.insert(insert_idx, new_layer)
            
            # Mutate layer properties
            for layer in config["layers"]:
                if random.random() < 0.05:  # 5% chance per layer
                    layer["activation"] = random.choice(self.config.allowed_activations)
                if random.random() < 0.05:  # 5% chance per layer
                    layer["regularization"] = random.choice(self.config.allowed_regularization) if random.random() > 0.5 else None
            
            # Mutate hyperparameters
            if random.random() < 0.1:
                config["learning_rate"] *= random.uniform(0.5, 2.0)
            if random.random() < 0.1:
                config["batch_size"] = random.choice([16, 32, 64, 128])
            
            return ModelArchitecture(config)
            
        except Exception as e:
            logger.warning(f"Mutation failed: {e}")
            return architecture
    
    def get_best_architecture(self) -> Optional[ModelArchitecture]:
        """Get the best architecture found so far."""
        if self.best_architectures:
            return self.best_architectures[-1]
        return None


class AdvancedAutoMLPerformanceOptimizer:
    """Main AutoML performance optimizer orchestrating all components."""
    
    def __init__(self, config: AutoMLPerformanceConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.optimizer")
        
        # Initialize components
        self.evaluator = PerformanceEvaluator(config)
        self.evolutionary_search = EvolutionaryArchitectureSearch(config)
        
        # Search state
        self.search_active = False
        self.search_results = []
        self.optimization_history = []
        
    async def optimize_architecture(self, input_size: int, output_size: int,
                                  train_data: DataLoader, test_data: DataLoader,
                                  device: torch.device) -> ModelArchitecture:
        """Main optimization method."""
        try:
            self.logger.info("ðŸš€ Starting AutoML Architecture Optimization...")
            start_time = time.time()
            
            # Initialize search
            self.evolutionary_search.initialize_population(input_size, output_size)
            
            # Main optimization loop
            for generation in range(self.config.generations):
                if time.time() - start_time > self.config.max_time_hours * 3600:
                    self.logger.info("â° Time limit reached, stopping optimization")
                    break
                
                self.logger.info(f"ðŸ” Generation {generation + 1}/{self.config.generations}")
                
                # Evaluate current population
                fitness_scores = await self._evaluate_population(train_data, test_data, device)
                
                # Store results
                self.search_results.append({
                    "generation": generation,
                    "fitness_scores": fitness_scores,
                    "best_score": max(fitness_scores) if fitness_scores else 0
                })
                
                # Evolve population
                self.evolutionary_search.evolve_population(fitness_scores)
                
                # Early stopping check
                if self.config.enable_early_stopping and self._should_stop_early():
                    self.logger.info("ðŸ›‘ Early stopping triggered")
                    break
            
            # Get best architecture
            best_architecture = self.evolutionary_search.get_best_architecture()
            
            if best_architecture:
                self.logger.info(f"âœ… Optimization completed. Best architecture found: {best_architecture.architecture_id}")
                
                # Apply final performance optimizations
                optimized_architecture = await self._apply_performance_optimizations(
                    best_architecture, train_data, test_data, device
                )
                
                return optimized_architecture
            else:
                self.logger.warning("âŒ No valid architecture found")
                return None
                
        except Exception as e:
            self.logger.error(f"Architecture optimization failed: {e}")
            return None
    
    async def _evaluate_population(self, train_data: DataLoader, test_data: DataLoader,
                                 device: torch.device) -> List[float]:
        """Evaluate all architectures in the current population."""
        try:
            fitness_scores = []
            
            for architecture in self.evolutionary_search.population:
                try:
                    # Build and train model
                    model = self._build_model_from_architecture(architecture)
                    
                    # Quick training (limited epochs for speed)
                    self._quick_train_model(model, train_data, device, max_epochs=3)
                    
                    # Evaluate performance
                    metrics = self.evaluator.evaluate_model(model, test_data, device)
                    
                    # Calculate fitness score
                    fitness = self._calculate_fitness_score(metrics)
                    fitness_scores.append(fitness)
                    
                    # Store optimization history
                    self.optimization_history.append({
                        "architecture_id": architecture.architecture_id,
                        "generation": self.evolutionary_search.generation,
                        "metrics": metrics,
                        "fitness": fitness
                    })
                    
                except Exception as e:
                    self.logger.warning(f"Architecture evaluation failed: {e}")
                    fitness_scores.append(0.0)  # Penalty for failed architectures
            
            return fitness_scores
            
        except Exception as e:
            self.logger.error(f"Population evaluation failed: {e}")
            return [0.0] * len(self.evolutionary_search.population)
    
    def _build_model_from_architecture(self, architecture: ModelArchitecture) -> nn.Module:
        """Build PyTorch model from architecture configuration."""
        try:
            config = architecture.config
            layers = []
            
            current_size = config["input_size"]
            
            for layer_config in config["layers"]:
                if layer_config["type"] == "linear":
                    # Add linear layer
                    layers.append(nn.Linear(current_size, layer_config["output_size"]))
                    current_size = layer_config["output_size"]
                    
                    # Add activation function
                    if layer_config["activation"] == "relu":
                        layers.append(nn.ReLU())
                    elif layer_config["activation"] == "gelu":
                        layers.append(nn.GELU())
                    elif layer_config["activation"] == "swish":
                        layers.append(nn.SiLU())
                    elif layer_config["activation"] == "mish":
                        layers.append(nn.Mish())
                    
                    # Add regularization
                    if layer_config["regularization"] == "dropout":
                        layers.append(nn.Dropout(0.2))
                    elif layer_config["regularization"] == "batch_norm":
                        layers.append(nn.BatchNorm1d(current_size))
                    elif layer_config["regularization"] == "layer_norm":
                        layers.append(nn.LayerNorm(current_size))
            
            # Add output layer if not already present
            if current_size != config["output_size"]:
                layers.append(nn.Linear(current_size, config["output_size"]))
            
            model = nn.Sequential(*layers)
            return model
            
        except Exception as e:
            self.logger.error(f"Model building failed: {e}")
            # Return simple fallback model
            return nn.Sequential(
                nn.Linear(config["input_size"], 64),
                nn.ReLU(),
                nn.Linear(64, config["output_size"])
            )
    
    def _quick_train_model(self, model: nn.Module, train_data: DataLoader,
                          device: torch.device, max_epochs: int = 3):
        """Quick training for evaluation purposes."""
        try:
            model = model.to(device)
            model.train()
            
            # Setup optimizer and loss
            optimizer = optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.CrossEntropyLoss()
            
            for epoch in range(max_epochs):
                for batch_idx, (data, targets) in enumerate(train_data):
                    if batch_idx >= 10:  # Limit batches for speed
                        break
                    
                    data, targets = data.to(device), targets.to(device)
                    
                    optimizer.zero_grad()
                    outputs = model(data)
                    loss = criterion(outputs, targets)
                    loss.backward()
                    optimizer.step()
                    
        except Exception as e:
            self.logger.warning(f"Quick training failed: {e}")
    
    def _calculate_fitness_score(self, metrics: Dict[str, float]) -> float:
        """Calculate fitness score based on performance metrics."""
        try:
            # Extract metrics
            accuracy = metrics.get("accuracy", 0.0)
            inference_time = metrics.get("inference_time_ms", 1000.0)
            memory_usage = metrics.get("memory_usage_mb", 1000.0)
            model_size = metrics.get("model_size_mb", 100.0)
            
            # Check constraints
            if (accuracy < self.config.min_accuracy or
                inference_time > self.config.max_inference_time_ms or
                memory_usage > self.config.max_memory_mb or
                model_size > self.config.max_model_size_mb):
                return 0.0  # Invalid architecture
            
            # Calculate fitness based on primary objective
            if self.config.primary_objective == "accuracy":
                fitness = accuracy
            elif self.config.primary_objective == "speed":
                fitness = max(0, 1 - (inference_time / self.config.max_inference_time_ms))
            elif self.config.primary_objective == "memory":
                fitness = max(0, 1 - (memory_usage / self.config.max_memory_mb))
            elif self.config.primary_objective == "balanced":
                # Multi-objective fitness
                acc_score = accuracy
                time_score = max(0, 1 - (inference_time / self.config.max_inference_time_ms))
                memory_score = max(0, 1 - (memory_usage / self.config.max_memory_mb))
                size_score = max(0, 1 - (model_size / self.config.max_model_size_mb))
                
                fitness = (0.4 * acc_score + 0.2 * time_score + 
                          0.2 * memory_score + 0.2 * size_score)
            else:
                fitness = accuracy
            
            return max(0, min(1, fitness))
            
        except Exception as e:
            self.logger.warning(f"Fitness calculation failed: {e}")
            return 0.0
    
    def _should_stop_early(self) -> bool:
        """Check if early stopping should be triggered."""
        try:
            if len(self.search_results) < 5:
                return False
            
            # Check if best score hasn't improved for 3 generations
            recent_results = self.search_results[-3:]
            best_scores = [r["best_score"] for r in recent_results]
            
            if len(best_scores) >= 3:
                improvement = max(best_scores) - min(best_scores)
                return improvement < 0.01  # Less than 1% improvement
            
            return False
            
        except Exception as e:
            self.logger.warning(f"Early stopping check failed: {e}")
            return False
    
    async def _apply_performance_optimizations(self, architecture: ModelArchitecture,
                                            train_data: DataLoader, test_data: DataLoader,
                                            device: torch.device) -> ModelArchitecture:
        """Apply final performance optimizations to the best architecture."""
        try:
            self.logger.info("ðŸ”§ Applying performance optimizations...")
            
            # Build model
            model = self._build_model_from_architecture(architecture)
            
            # Apply optimizations
            if self.config.enable_quantization:
                model = self._apply_quantization(model)
            
            if self.config.enable_pruning:
                model = self._apply_pruning(model)
            
            if self.config.enable_mixed_precision:
                model = self._apply_mixed_precision(model)
            
            # Re-evaluate optimized model
            optimized_metrics = self.evaluator.evaluate_model(model, test_data, device)
            
            # Update architecture with optimization results
            architecture.performance_metrics = optimized_metrics
            
            self.logger.info("âœ… Performance optimizations applied successfully")
            return architecture
            
        except Exception as e:
            self.logger.error(f"Performance optimization failed: {e}")
            return architecture
    
    def _apply_quantization(self, model: nn.Module) -> nn.Module:
        """Apply quantization to the model."""
        try:
            # Dynamic quantization for CPU
            if not torch.cuda.is_available():
                model = torch.quantization.quantize_dynamic(
                    model, {nn.Linear}, dtype=torch.qint8
                )
            else:
                # FP16 for GPU
                model = model.half()
            
            return model
            
        except Exception as e:
            self.logger.warning(f"Quantization failed: {e}")
            return model
    
    def _apply_pruning(self, model: nn.Module) -> nn.Module:
        """Apply pruning to the model."""
        try:
            # Simple magnitude-based pruning
            for name, module in model.named_modules():
                if isinstance(module, nn.Linear):
                    weight = module.weight.data
                    threshold = torch.quantile(torch.abs(weight), 0.1)  # 10% sparsity
                    mask = torch.abs(weight) > threshold
                    module.weight.data = weight * mask
            
            return model
            
        except Exception as e:
            self.logger.warning(f"Pruning failed: {e}")
            return model
    
    def _apply_mixed_precision(self, model: nn.Module) -> nn.Module:
        """Apply mixed precision training."""
        try:
            # Convert to FP16
            model = model.half()
            return model
            
        except Exception as e:
            self.logger.warning(f"Mixed precision failed: {e}")
            return model
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """Get comprehensive optimization summary."""
        return {
            "total_generations": self.evolutionary_search.generation,
            "total_evaluations": len(self.optimization_history),
            "best_architecture": self.evolutionary_search.get_best_architecture().architecture_id if self.evolutionary_search.get_best_architecture() else None,
            "search_results": self.search_results,
            "optimization_history": len(self.optimization_history)
        }


# Factory functions
def create_automl_performance_optimizer(config: Optional[AutoMLPerformanceConfig] = None) -> AdvancedAutoMLPerformanceOptimizer:
    """Create an AutoML performance optimizer."""
    if config is None:
        config = AutoMLPerformanceConfig()
    
    return AdvancedAutoMLPerformanceOptimizer(config)


def create_balanced_automl_config() -> AutoMLPerformanceConfig:
    """Create balanced AutoML configuration."""
    return AutoMLPerformanceConfig(
        primary_objective="balanced",
        max_trials=50,
        max_time_hours=12.0,
        population_size=30,
        generations=15
    )


def create_speed_optimized_automl_config() -> AutoMLPerformanceConfig:
    """Create speed-optimized AutoML configuration."""
    return AutoMLPerformanceConfig(
        primary_objective="speed",
        max_inference_time_ms=50.0,
        max_trials=30,
        max_time_hours=6.0,
        population_size=20,
        generations=10
    )


if __name__ == "__main__":
    # Test the AutoML performance optimizer
    config = create_balanced_automl_config()
    optimizer = create_automl_performance_optimizer(config)
    
    print(f"AutoML Performance Optimizer created with config: {config}")
    print(f"Optimizer ready for architecture search!")
