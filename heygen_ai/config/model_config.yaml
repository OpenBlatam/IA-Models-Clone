# üöÄ HeyGen AI - Model Configuration
# ===================================
# Centralized configuration for all AI models and training parameters

# üß† Model Architecture Settings
models:
  transformer:
    # GPT-2 Configuration
    gpt2:
      model_name: "gpt2"
      model_type: "causal_lm"
      hidden_size: 768
      num_attention_heads: 12
      num_hidden_layers: 12
      intermediate_size: 3072
      max_position_embeddings: 1024
      vocab_size: 50257
      dropout: 0.1
      attention_dropout: 0.1
      activation_dropout: 0.1
      layer_norm_epsilon: 1e-5
      initializer_range: 0.02
      scale_embedding: false
      use_cache: true
    
    # BERT Configuration
    bert:
      model_name: "bert-base-uncased"
      model_type: "encoder"
      hidden_size: 768
      num_attention_heads: 12
      num_hidden_layers: 12
      intermediate_size: 3072
      max_position_embeddings: 512
      vocab_size: 30522
      dropout: 0.1
      attention_dropout: 0.1
      hidden_dropout_prob: 0.1
      layer_norm_epsilon: 1e-12
      initializer_range: 0.02
      type_vocab_size: 2
      use_cache: true
    
    # T5 Configuration
    t5:
      model_name: "t5-base"
      model_type: "seq2seq"
      d_model: 768
      d_ff: 3072
      num_layers: 12
      num_decoder_layers: 12
      num_heads: 12
      dropout_rate: 0.1
      layer_norm_epsilon: 1e-6
      initializer_factor: 1.0
      vocab_size: 32128
      use_cache: true

  diffusion:
    # Stable Diffusion Configuration
    stable_diffusion:
      model_name: "runwayml/stable-diffusion-v1-5"
      model_type: "stable_diffusion"
      scheduler_type: "ddim"
      num_inference_steps: 50
      guidance_scale: 7.5
      width: 512
      height: 512
      batch_size: 1
      use_fp16: true
      enable_attention_slicing: true
      enable_vae_slicing: true
      enable_xformers: true
    
    # Stable Diffusion XL Configuration
    stable_diffusion_xl:
      model_name: "stabilityai/stable-diffusion-xl-base-1.0"
      model_type: "stable_diffusion_xl"
      scheduler_type: "euler"
      num_inference_steps: 30
      guidance_scale: 7.5
      width: 1024
      height: 1024
      batch_size: 1
      use_fp16: true
      enable_attention_slicing: true
      enable_vae_slicing: true
      enable_xformers: true

# ‚ö° Training Configuration
training:
  # General Training Settings
  general:
    seed: 42
    num_epochs: 10
    batch_size: 8
    gradient_accumulation_steps: 4
    max_grad_norm: 1.0
    warmup_steps: 100
    save_steps: 500
    eval_steps: 500
    logging_steps: 100
  
  # Learning Rate Settings
  learning_rate:
    initial_lr: 5e-5
    min_lr: 1e-6
    scheduler_type: "cosine"  # cosine, linear, constant
    warmup_ratio: 0.1
    weight_decay: 0.01
  
  # Mixed Precision Settings
  mixed_precision:
    enabled: true
    dtype: "fp16"  # fp16, bf16, fp32
    autocast: true
    scaler: true
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 3
    min_delta: 0.001
    monitor: "val_loss"
    mode: "min"

# üéØ Optimization Settings
optimization:
  # LoRA Fine-tuning
  lora:
    enabled: false
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "out_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"
  
  # Quantization
  quantization:
    enabled: false
    dtype: "int8"  # int8, fp16, bf16
    backend: "auto"  # auto, x86, arm, cuda
  
  # Gradient Checkpointing
  gradient_checkpointing:
    enabled: true
    memory_efficient: true

# üìä Data Configuration
data:
  # Dataset Settings
  dataset:
    train_file: "data/train.json"
    validation_file: "data/validation.json"
    test_file: "data/test.json"
    max_length: 512
    truncation: true
    padding: "max_length"
    return_tensors: "pt"
  
  # DataLoader Settings
  dataloader:
    num_workers: 4
    pin_memory: true
    shuffle: true
    drop_last: false
    persistent_workers: true

# üñ•Ô∏è Hardware Configuration
hardware:
  # Device Settings
  device:
    type: "auto"  # auto, cuda, cpu, mps
    cuda_device: 0
    mixed_precision: true
  
  # Memory Management
  memory:
    max_memory_usage: 0.9
    enable_memory_efficient_attention: true
    enable_attention_slicing: true
    enable_vae_slicing: true
  
  # Multi-GPU
  distributed:
    enabled: false
    backend: "nccl"  # nccl, gloo
    world_size: 1
    rank: 0

# üìà Monitoring and Logging
monitoring:
  # Experiment Tracking
  experiment_tracking:
    enabled: true
    backend: "wandb"  # wandb, tensorboard, mlflow
    project_name: "heygen-ai"
    run_name: "experiment-001"
    log_interval: 100
  
  # Logging
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file: "logs/training.log"
    console: true
  
  # Metrics
  metrics:
    save_metrics: true
    metrics_file: "metrics/training_metrics.json"
    plot_metrics: true

# üîí Security and Validation
security:
  # Input Validation
  input_validation:
    enabled: true
    max_input_length: 1000
    allowed_file_types: ["jpg", "jpeg", "png", "mp4", "avi"]
    max_file_size_mb: 100
  
  # Rate Limiting
  rate_limiting:
    enabled: true
    requests_per_minute: 60
    burst_size: 10

# üöÄ Performance Optimization
performance:
  # Torch Compile
  torch_compile:
    enabled: true
    mode: "max-autotune"  # default, reduce-overhead, max-autotune
    dynamic: true
    fullgraph: true
    backend: "inductor"
  
  # Attention Optimization
  attention:
    enable_flash_attention: true
    enable_xformers: true
    enable_memory_efficient_attention: true
    attention_backend: "auto"
  
  # Memory Optimization
  memory:
    enable_gradient_checkpointing: true
    enable_activation_checkpointing: true
    enable_selective_checkpointing: true
    memory_pool_optimization: true
  
  # Batch Optimization
  batch:
    enable_dynamic_batching: true
    max_batch_size: 32
    min_batch_size: 1
    optimization_interval: 100

# üß™ Testing and Evaluation
testing:
  # Unit Tests
  unit_tests:
    enabled: true
    coverage_threshold: 80
    test_pattern: "test_*.py"
  
  # Integration Tests
  integration_tests:
    enabled: true
    test_models: true
    test_pipelines: true
  
  # Performance Tests
  performance_tests:
    enabled: true
    benchmark_models: true
    memory_profiling: true
    speed_profiling: true

# üåê API Configuration
api:
  # FastAPI Settings
  fastapi:
    host: "0.0.0.0"
    port: 8000
    workers: 4
    reload: false
  
  # Gradio Interface
  gradio:
    enabled: true
    port: 7860
    share: false
    auth: false
    theme: "default"
  
  # CORS
  cors:
    enabled: true
    origins: ["*"]
    methods: ["GET", "POST", "PUT", "DELETE"]
    headers: ["*"]

# üîÑ Workflow Configuration
workflow:
  # Pipeline Settings
  pipeline:
    enable_cache: true
    cache_dir: "cache/"
    enable_streaming: true
  
  # Error Recovery
  error_recovery:
    enabled: true
    max_retries: 3
    retry_delay: 1.0
    fallback_strategy: "graceful"
  
  # Async Processing
  async:
    enabled: true
    max_concurrent: 10
    queue_size: 100
    timeout: 300

