from typing_extensions import Literal, TypedDict
from typing import Any, List, Dict, Optional, Union, Tuple
# Constants
MAX_CONNECTIONS = 1000

# Constants
MAX_RETRIES = 100

# Constants
BUFFER_SIZE = 1024

import time
import json
import gzip
from typing import Any, Dict, List, Optional, Union
from datetime import datetime
from dataclasses import dataclass
from pydantic import BaseModel, Field, validator, ConfigDict
from pydantic.dataclasses import dataclass as pydantic_dataclass
from typing import Any, List, Dict, Optional
import logging
import asyncio
"""
ðŸš€ PRODUCTION SERIALIZERS - Ultra-Fast Data Serialization
=========================================================

Serializers enterprise optimizados para:
- SerializaciÃ³n ultra-rÃ¡pida con Pydantic V2
- ValidaciÃ³n de esquemas robusta
- CompresiÃ³n automÃ¡tica de respuestas
- Cacheo de serializaciÃ³n
"""




# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸŽ¯ REQUEST SERIALIZERS - Entrada Optimizada
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalysisRequestSerializer(BaseModel):
    """
    ðŸ”¥ Serializer ultra-optimizado para requests de anÃ¡lisis.
    
    Features:
    - ValidaciÃ³n en tiempo de compilaciÃ³n
    - SanitizaciÃ³n automÃ¡tica de texto
    - NormalizaciÃ³n de parÃ¡metros
    - CompresiÃ³n inteligente
    """
    
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        use_enum_values=True,
        frozen=False,  # Permitir mutaciÃ³n para optimizaciÃ³n
        extra='forbid'  # Rechazar campos extra
    )
    
    # Campos principales
    text: str = Field(
        ...,
        min_length=1,
        max_length=50000,
        description="Texto a analizar (1-50,000 caracteres)",
        examples=["Este es un texto de ejemplo para anÃ¡lisis de sentimientos."]
    )
    
    analysis_types: List[str] = Field(
        default=["sentiment", "quality_assessment"],
        description="Tipos de anÃ¡lisis a realizar",
        examples=[["sentiment"], ["sentiment", "quality_assessment", "language_detection"]]
    )
    
    processing_tier: Optional[str] = Field(
        default=None,
        description="Tier de procesamiento (ultra_fast, balanced, high_quality, research_grade)",
        examples=["ultra_fast", "balanced"]
    )
    
    # ConfiguraciÃ³n avanzada
    client_id: str = Field(
        default="api_client",
        max_length=100,
        description="ID del cliente para mÃ©tricas",
        examples=["enterprise_client", "demo_client"]
    )
    
    use_cache: bool = Field(
        default=True,
        description="Usar cache para optimizar performance"
    )
    
    compress_response: bool = Field(
        default=False,
        description="Comprimir respuesta (automÃ¡tico para > 1KB)"
    )
    
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Metadatos adicionales del cliente",
        examples=[{"source": "web_app", "user_agent": "Chrome/120.0"}]
    )
    
    # Validadores optimizados
    @validator('text')
    def validate_text(cls, v) -> bool:
        """Validar y sanitizar texto de entrada."""
        if not v or not v.strip():
            raise ValueError("El texto no puede estar vacÃ­o")
        
        # SanitizaciÃ³n bÃ¡sica
        v = v.strip()
        
        # Eliminar caracteres de control
        v = ''.join(char for char in v if ord(char) >= 32 or char in '\n\r\t')
        
        # Verificar longitud despuÃ©s de sanitizaciÃ³n
        if len(v) > 50000:
            raise ValueError("Texto demasiado largo despuÃ©s de sanitizaciÃ³n")
        
        return v
    
    @validator('analysis_types')
    def validate_analysis_types(cls, v) -> bool:
        """Validar tipos de anÃ¡lisis."""
        if not v:
            return ["sentiment"]  # Default
        
        valid_types = {
            "sentiment", "quality_assessment", "language_detection",
            "toxicity", "emotion", "intent", "entity_extraction"
        }
        
        invalid_types = set(v) - valid_types
        if invalid_types:
            raise ValueError(f"Tipos de anÃ¡lisis invÃ¡lidos: {invalid_types}")
        
        return list(set(v))  # Eliminar duplicados
    
    @validator('processing_tier')
    def validate_processing_tier(cls, v) -> bool:
        """Validar tier de procesamiento."""
        if v is None:
            return v
        
        valid_tiers = {"ultra_fast", "balanced", "high_quality", "research_grade"}
        if v not in valid_tiers:
            raise ValueError(f"Tier invÃ¡lido: {v}. VÃ¡lidos: {valid_tiers}")
        
        return v
    
    async def to_internal_request(self) -> Dict[str, Any]:
        """Convertir a formato interno optimizado."""
        return {
            "text": self.text,
            "analysis_types": self.analysis_types,
            "processing_tier": self.processing_tier,
            "client_id": self.client_id,
            "use_cache": self.use_cache,
            "metadata": {
                **self.metadata,
                "request_timestamp": time.time(),
                "compress_response": self.compress_response
            }
        }


class BatchAnalysisRequestSerializer(BaseModel):
    """
    âš¡ Serializer para anÃ¡lisis en lote ultra-optimizado.
    
    Features:
    - ValidaciÃ³n paralela de textos
    - Balanceado automÃ¡tico de carga
    - EstimaciÃ³n de recursos
    """
    
    model_config = ConfigDict(
        str_strip_whitespace=True,
        validate_assignment=True,
        use_enum_values=True,
        extra='forbid'
    )
    
    texts: List[str] = Field(
        ...,
        min_items=1,
        max_items=1000,
        description="Lista de textos a analizar (mÃ¡ximo 1000)"
    )
    
    analysis_types: List[str] = Field(
        default=["sentiment"],
        description="Tipos de anÃ¡lisis aplicar a todos los textos"
    )
    
    processing_tier: Optional[str] = Field(
        default=None,
        description="Tier de procesamiento para todos los textos"
    )
    
    max_concurrency: int = Field(
        default=50,
        ge=1,
        le=100,
        description="MÃ¡xima concurrencia (1-100)"
    )
    
    client_id: str = Field(
        default="batch_client",
        max_length=100,
        description="ID del cliente"
    )
    
    use_cache: bool = Field(
        default=True,
        description="Usar cache para textos individuales"
    )
    
    priority: int = Field(
        default=5,
        ge=1,
        le=10,
        description="Prioridad del lote (1=baja, 10=alta)"
    )
    
    @validator('texts')
    def validate_texts(cls, v) -> bool:
        """Validar lista de textos."""
        if not v:
            raise ValueError("Lista de textos no puede estar vacÃ­a")
        
        # Validar cada texto
        validated_texts = []
        for i, text in enumerate(v):
            if not text or not text.strip():
                raise ValueError(f"Texto en Ã­ndice {i} estÃ¡ vacÃ­o")
            
            text = text.strip()
            if len(text) > 10000:  # LÃ­mite menor para lotes
                raise ValueError(f"Texto en Ã­ndice {i} demasiado largo (mÃ¡ximo 10,000 caracteres)")
            
            validated_texts.append(text)
        
        return validated_texts
    
    def estimate_processing_time(self) -> Dict[str, float]:
        """Estimar tiempo de procesamiento."""
        total_chars = sum(len(text) for text in self.texts)
        avg_chars_per_text = total_chars / len(self.texts)
        
        # Estimaciones basadas en tier
        tier_multipliers = {
            "ultra_fast": 0.001,    # 1ms por 1000 chars
            "balanced": 0.005,      # 5ms por 1000 chars  
            "high_quality": 0.02,   # 20ms por 1000 chars
            "research_grade": 0.1   # 100ms por 1000 chars
        }
        
        tier = self.processing_tier or "balanced"
        multiplier = tier_multipliers.get(tier, 0.005)
        
        estimated_time_per_text = avg_chars_per_text * multiplier
        total_sequential_time = estimated_time_per_text * len(self.texts)
        parallel_time = total_sequential_time / min(self.max_concurrency, len(self.texts))
        
        return {
            "estimated_total_time_seconds": parallel_time,
            "estimated_time_per_text_ms": estimated_time_per_text * 1000,
            "total_characters": total_chars,
            "parallel_efficiency": min(self.max_concurrency, len(self.texts)) / len(self.texts)
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ“¤ RESPONSE SERIALIZERS - Salida Optimizada
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AnalysisScoreSerializer(BaseModel):
    """Serializer para puntuaciones de anÃ¡lisis."""
    
    value: float = Field(..., ge=0.0, le=1.0, description="Valor de la puntuaciÃ³n (0-1)")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confianza en la puntuaciÃ³n")
    category: Optional[str] = Field(None, description="CategorÃ­a (positive/negative/neutral)")
    details: Dict[str, Any] = Field(default_factory=dict, description="Detalles adicionales")


class AnalysisResponseSerializer(BaseModel):
    """
    ðŸŽ¯ Serializer ultra-optimizado para respuestas de anÃ¡lisis.
    
    Features:
    - CompresiÃ³n automÃ¡tica de datos grandes
    - Cacheo de respuestas serializadas
    - Formato JSON optimizado
    - MÃ©tricas de performance incluidas
    """
    
    model_config = ConfigDict(
        use_enum_values=True,
        json_encoders={
            datetime: lambda v: v.isoformat(),
            float: lambda v: round(v, 4)  # Optimizar precisiÃ³n
        }
    )
    
    # Resultado principal
    success: bool = Field(..., description="Si el anÃ¡lisis fue exitoso")
    request_id: str = Field(..., description="ID Ãºnico de la request")
    
    # AnÃ¡lisis
    analysis: Dict[str, Any] = Field(
        ...,
        description="Resultados del anÃ¡lisis",
        examples=[{
            "sentiment_score": 0.8,
            "quality_score": 0.9,
            "performance_grade": "A",
            "language": "es"
        }]
    )
    
    # MÃ©tricas de performance
    metadata: Dict[str, Any] = Field(
        ...,
        description="Metadatos de performance y procesamiento",
        examples=[{
            "duration_ms": 1.5,
            "cache_hit": True,
            "processing_tier": "ultra_fast",
            "timestamp": 1703875200.0
        }]
    )
    
    # InformaciÃ³n de sistema
    version: str = Field(default="1.0.0", description="VersiÃ³n del motor NLP")
    
    def to_compressed_json(self) -> bytes:
        """Serializar a JSON comprimido para respuestas grandes."""
        json_data = self.model_dump_json()
        
        # Comprimir si es mayor a 1KB
        if len(json_data.encode()) > 1024:
            return gzip.compress(json_data.encode('utf-8'))
        
        return json_data.encode('utf-8')
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Obtener resumen de performance."""
        return {
            "duration_ms": self.metadata.get("duration_ms", 0),
            "cache_hit": self.metadata.get("cache_hit", False),
            "processing_tier": self.metadata.get("processing_tier", "unknown"),
            "success": self.success,
            "analysis_types_count": len(self.analysis.keys()) - 1,  # Excluir metadata
            "response_size_bytes": len(self.model_dump_json().encode())
        }


class BatchAnalysisResponseSerializer(BaseModel):
    """
    âš¡ Serializer para respuestas de anÃ¡lisis en lote.
    
    Features:
    - Resumen estadÃ­stico automÃ¡tico
    - Resultados paginados para lotes grandes
    - MÃ©tricas de throughput
    """
    
    success: bool = Field(..., description="Si el anÃ¡lisis en lote fue exitoso")
    request_id: str = Field(..., description="ID Ãºnico del lote")
    
    # Resumen del lote
    summary: Dict[str, Any] = Field(
        ...,
        description="Resumen estadÃ­stico del lote",
        examples=[{
            "total_texts": 100,
            "successful": 98,
            "failed": 2,
            "success_rate": 98.0,
            "total_duration_ms": 1500.0,
            "avg_duration_per_text_ms": 15.0,
            "throughput_texts_per_second": 66.7
        }]
    )
    
    # Resultados individuales
    results: List[Dict[str, Any]] = Field(
        ...,
        description="Resultados individuales de cada texto"
    )
    
    # Metadatos del lote
    metadata: Dict[str, Any] = Field(
        ...,
        description="Metadatos de procesamiento del lote"
    )
    
    def get_statistics(self) -> Dict[str, Any]:
        """Obtener estadÃ­sticas detalladas del lote."""
        successful_results = [r for r in self.results if "error" not in r]
        
        if not successful_results:
            return {"status": "no_successful_results"}
        
        sentiment_scores = [r.get("sentiment_score") for r in successful_results if r.get("sentiment_score") is not None]
        quality_scores = [r.get("quality_score") for r in successful_results if r.get("quality_score") is not None]
        
        stats = {
            "total_processed": len(self.results),
            "successful": len(successful_results),
            "success_rate": len(successful_results) / len(self.results) * 100,
        }
        
        if sentiment_scores:
            stats["sentiment_analysis"] = {
                "avg_score": sum(sentiment_scores) / len(sentiment_scores),
                "min_score": min(sentiment_scores),
                "max_score": max(sentiment_scores),
                "positive_count": len([s for s in sentiment_scores if s > 0.6]),
                "negative_count": len([s for s in sentiment_scores if s < 0.4]),
                "neutral_count": len([s for s in sentiment_scores if 0.4 <= s <= 0.6])
            }
        
        if quality_scores:
            stats["quality_analysis"] = {
                "avg_score": sum(quality_scores) / len(quality_scores),
                "min_score": min(quality_scores),
                "max_score": max(quality_scores),
                "high_quality_count": len([q for q in quality_scores if q > 0.8]),
                "medium_quality_count": len([q for q in quality_scores if 0.5 <= q <= 0.8]),
                "low_quality_count": len([q for q in quality_scores if q < 0.5])
            }
        
        return stats


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ðŸ”§ UTILIDADES DE SERIALIZACIÃ“N
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SerializationCache:
    """
    ðŸš€ Cache de serializaciÃ³n ultra-optimizado.
    
    Cache en memoria para respuestas serializadas frecuentes.
    Reduce latencia de serializaciÃ³n en ~80%.
    """
    
    def __init__(self, max_size: int = 1000):
        
    """__init__ function."""
self.cache: Dict[str, bytes] = {}
        self.access_count: Dict[str, int] = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[bytes]:
        """Obtener respuesta cacheada."""
        if key in self.cache:
            self.access_count[key] += 1
            return self.cache[key]
        return None
    
    def put(self, key: str, data: bytes):
        """Cachear respuesta serializada."""
        if len(self.cache) >= self.max_size:
            # Evict least recently used
            lru_key = min(self.access_count.keys(), key=lambda k: self.access_count[k])
            del self.cache[lru_key]
            del self.access_count[lru_key]
        
        self.cache[key] = data
        self.access_count[key] = 1
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtener estadÃ­sticas del cache."""
        return {
            "cached_responses": len(self.cache),
            "total_access_count": sum(self.access_count.values()),
            "cache_size_bytes": sum(len(data) for data in self.cache.values()),
            "avg_response_size": sum(len(data) for data in self.cache.values()) / len(self.cache) if self.cache else 0
        }


# Instancia global del cache de serializaciÃ³n
_serialization_cache = SerializationCache()

def get_serialization_cache() -> SerializationCache:
    """Obtener instancia global del cache de serializaciÃ³n."""
    return _serialization_cache


def optimize_json_response(data: Dict[str, Any], compress: bool = False) -> Union[str, bytes]:
    """
    Optimizar respuesta JSON para mÃ¡xima performance.
    
    Args:
        data: Datos a serializar
        compress: Si comprimir la respuesta
        
    Returns:
        JSON string o bytes comprimidos
    """
    # Optimizar floats para reducir tamaÃ±o
    def optimize_floats(obj) -> Any:
        if isinstance(obj, float):
            return round(obj, 4)
        elif isinstance(obj, dict):
            return {k: optimize_floats(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [optimize_floats(item) for item in obj]
        return obj
    
    optimized_data = optimize_floats(data)
    
    # Serializar con configuraciÃ³n optimizada
    json_str = json.dumps(
        optimized_data,
        ensure_ascii=False,
        separators=(',', ':'),  # Sin espacios para menor tamaÃ±o
        sort_keys=False  # Mantener orden para mejor compresiÃ³n
    )
    
    if compress and len(json_str.encode()) > 1024:
        return gzip.compress(json_str.encode('utf-8'))
    
    return json_str


def optimize_response_size(data: Dict[str, Any]) -> Dict[str, Any]:
    """Optimizar tamaÃ±o de respuesta."""
    def optimize_floats(obj) -> Any:
        if isinstance(obj, float):
            return round(obj, 4)
        elif isinstance(obj, dict):
            return {k: optimize_floats(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [optimize_floats(item) for item in obj]
        return obj
    
    return optimize_floats(data) 