"""
Ultimate TruthGPT API Features Example
=====================================

This example demonstrates the ULTIMATE features of the TruthGPT API
including advanced architectures, GANs, quantization, pruning, and distributed training.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import truthgpt as tg
import numpy as np
import torch
import time
from typing import Dict, Any, List


def demonstrate_advanced_architectures():
    """Demonstrate advanced architectures."""
    print("\nüèóÔ∏è Advanced Architectures Demonstration")
    print("=" * 50)
    
    # ResNet
    print("1. ResNet Architecture")
    resnet = tg.architectures.ResNet50(num_classes=10)
    print(f"   ResNet-50: {resnet}")
    
    # Test ResNet
    x = torch.randn(4, 3, 224, 224)
    output = resnet(x)
    print(f"   Input shape: {x.shape}")
    print(f"   Output shape: {output.shape}")
    
    # Vision Transformer
    print("\n2. Vision Transformer Architecture")
    vit = tg.architectures.ViT_B16(num_classes=10)
    print(f"   ViT-B16: {vit}")
    
    # Test Vision Transformer
    x = torch.randn(4, 3, 224, 224)
    output = vit(x)
    print(f"   Input shape: {x.shape}")
    print(f"   Output shape: {output.shape}")
    
    print("‚úÖ Advanced architectures demonstrated successfully!")


def demonstrate_gan_training():
    """Demonstrate GAN training."""
    print("\nüé® GAN Training Demonstration")
    print("=" * 50)
    
    # Create GAN
    gan = tg.architectures.GAN(
        latent_dim=100,
        img_channels=3,
        img_size=64,
        hidden_dim=64
    )
    
    print(f"GAN created: {gan}")
    print(f"Generator: {gan.generator}")
    print(f"Discriminator: {gan.discriminator}")
    
    # Test generator
    noise = torch.randn(4, 100)
    fake_images = gan.generate(4, torch.device('cpu'))
    print(f"Generated images shape: {fake_images.shape}")
    
    # Test discriminator
    real_images = torch.randn(4, 3, 64, 64)
    real_validity = gan.discriminate(real_images)
    fake_validity = gan.discriminate(fake_images)
    print(f"Real validity: {real_validity.mean().item():.4f}")
    print(f"Fake validity: {fake_validity.mean().item():.4f}")
    
    print("‚úÖ GAN training demonstrated successfully!")


def demonstrate_quantization():
    """Demonstrate model quantization."""
    print("\n‚ö° Quantization Demonstration")
    print("=" * 50)
    
    # Create a simple model
    model = tg.Sequential([
        tg.layers.Dense(128, activation='relu'),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(64, activation='relu'),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(10, activation='softmax')
    ])
    
    # Generate sample data
    x_sample = torch.randn(1, 10)
    
    # Dynamic quantization
    print("1. Dynamic Quantization")
    dynamic_quantizer = tg.quantization.DynamicQuantization(dtype=torch.qint8)
    quantized_model = dynamic_quantizer.quantize(model)
    
    # Benchmark quantization
    benchmark_results = dynamic_quantizer.benchmark(
        model, quantized_model, x_sample, num_runs=10
    )
    
    print(f"   Speedup: {benchmark_results['speedup']:.2f}x")
    print(f"   Compression: {benchmark_results['compression_ratio']:.2f}x")
    
    print("‚úÖ Quantization demonstrated successfully!")


def demonstrate_pruning():
    """Demonstrate model pruning."""
    print("\n‚úÇÔ∏è Pruning Demonstration")
    print("=" * 50)
    
    # Create a simple model
    model = tg.Sequential([
        tg.layers.Dense(128, activation='relu'),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(64, activation='relu'),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(10, activation='softmax')
    ])
    
    # Generate sample data
    x_sample = torch.randn(1, 10)
    
    # Magnitude-based pruning
    print("1. Magnitude-based Pruning")
    pruner = tg.pruning.MagnitudePruning(sparsity=0.5, global_pruning=True)
    pruned_model = pruner.prune(model)
    
    # Analyze sparsity
    sparsity_analysis = pruner.analyze_sparsity(pruned_model)
    print(f"   Overall sparsity: {sparsity_analysis['overall_sparsity']:.2%}")
    
    # Benchmark pruning
    benchmark_results = pruner.benchmark(
        model, pruned_model, x_sample, num_runs=10
    )
    
    print(f"   Speedup: {benchmark_results['speedup']:.2f}x")
    print(f"   Sparsity: {benchmark_results['sparsity']:.2%}")
    print(f"   Compression: {benchmark_results['compression_ratio']:.2f}x")
    
    print("‚úÖ Pruning demonstrated successfully!")


def demonstrate_distributed_training():
    """Demonstrate distributed training."""
    print("\nüåê Distributed Training Demonstration")
    print("=" * 50)
    
    # Check if distributed training is available
    if not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA not available, skipping distributed training demo")
        return
    
    # Create distributed trainer
    ddp = tg.distributed.DistributedDataParallel(
        backend='nccl',
        world_size=1,  # Single GPU for demo
        rank=0
    )
    
    print(f"Distributed trainer: {ddp}")
    print(f"World size: {ddp.get_world_size()}")
    print(f"Rank: {ddp.get_rank()}")
    print(f"Is master: {ddp.is_master()}")
    
    # Create a simple model
    model = tg.Sequential([
        tg.layers.Dense(64, activation='relu'),
        tg.layers.Dense(10, activation='softmax')
    ])
    
    # Wrap model with DDP
    ddp_model = ddp.wrap_model(model)
    print(f"DDP model created: {type(ddp_model)}")
    
    # Cleanup
    ddp.cleanup()
    
    print("‚úÖ Distributed training demonstrated successfully!")


def demonstrate_comprehensive_pipeline():
    """Demonstrate a comprehensive pipeline with all features."""
    print("\nüöÄ Comprehensive Pipeline Demonstration")
    print("=" * 50)
    
    # Generate data
    x_train = np.random.randn(1000, 20).astype(np.float32)
    y_train = np.random.randint(0, 10, 1000).astype(np.int64)
    x_val = np.random.randn(200, 20).astype(np.float32)
    y_val = np.random.randint(0, 10, 200).astype(np.int64)
    
    # Create advanced model
    model = tg.Sequential([
        tg.layers.Dense(256, activation='relu'),
        tg.layers.BatchNormalization(),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(128, activation='relu'),
        tg.layers.BatchNormalization(),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(64, activation='relu'),
        tg.layers.Dropout(0.2),
        tg.layers.Dense(10, activation='softmax')
    ])
    
    # Compile with advanced optimizer
    model.compile(
        optimizer=tg.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),
        loss=tg.losses.SparseCategoricalCrossentropy(),
        metrics=['accuracy']
    )
    
    # Create callbacks
    callbacks = [
        tg.callbacks.EarlyStopping(monitor='val_loss', patience=5),
        tg.callbacks.ModelCheckpoint('ultimate_model.pth', monitor='val_accuracy', save_best_only=True)
    ]
    
    # Train model
    print("Training comprehensive model...")
    history = model.fit(
        x_train, y_train,
        epochs=5,
        batch_size=64,
        validation_data=(x_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    # Evaluate model
    test_loss, test_accuracy = model.evaluate(x_val, y_val, verbose=1)
    
    # Apply quantization
    print("\nApplying quantization...")
    quantizer = tg.quantization.DynamicQuantization(dtype=torch.qint8)
    quantized_model = quantizer.quantize(model)
    
    # Apply pruning
    print("\nApplying pruning...")
    pruner = tg.pruning.MagnitudePruning(sparsity=0.3, global_pruning=True)
    pruned_model = pruner.prune(model)
    
    # Analyze results
    sparsity_analysis = pruner.analyze_sparsity(pruned_model)
    
    print(f"\nüìä Final Results:")
    print(f"   Test accuracy: {test_accuracy:.4f}")
    print(f"   Quantization speedup: {quantizer._get_compression_ratio():.2f}x")
    print(f"   Pruning sparsity: {sparsity_analysis['overall_sparsity']:.2%}")
    print(f"   Pruning compression: {1 / (1 - sparsity_analysis['overall_sparsity']):.2f}x")
    
    print("‚úÖ Comprehensive pipeline demonstrated successfully!")


def demonstrate_advanced_optimization():
    """Demonstrate advanced optimization techniques."""
    print("\nüîß Advanced Optimization Demonstration")
    print("=" * 50)
    
    # Create a complex model
    model = tg.Sequential([
        tg.layers.Dense(512, activation='relu'),
        tg.layers.BatchNormalization(),
        tg.layers.Dropout(0.4),
        tg.layers.Dense(256, activation='relu'),
        tg.layers.BatchNormalization(),
        tg.layers.Dropout(0.4),
        tg.layers.Dense(128, activation='relu'),
        tg.layers.Dropout(0.3),
        tg.layers.Dense(64, activation='relu'),
        tg.layers.Dropout(0.2),
        tg.layers.Dense(10, activation='softmax')
    ])
    
    # Generate data
    x_train = np.random.randn(2000, 20).astype(np.float32)
    y_train = np.random.randint(0, 10, 2000).astype(np.int64)
    
    # Test different optimization techniques
    optimization_techniques = [
        ('Standard Training', None),
        ('Quantization', 'quantization'),
        ('Pruning', 'pruning'),
        ('Quantization + Pruning', 'both')
    ]
    
    results = {}
    
    for technique_name, technique in optimization_techniques:
        print(f"\n{technique_name}")
        
        # Create model copy
        model_copy = tg.Sequential([
            tg.layers.Dense(512, activation='relu'),
            tg.layers.BatchNormalization(),
            tg.layers.Dropout(0.4),
            tg.layers.Dense(256, activation='relu'),
            tg.layers.BatchNormalization(),
            tg.layers.Dropout(0.4),
            tg.layers.Dense(128, activation='relu'),
            tg.layers.Dropout(0.3),
            tg.layers.Dense(64, activation='relu'),
            tg.layers.Dropout(0.2),
            tg.layers.Dense(10, activation='softmax')
        ])
        
        model_copy.compile(
            optimizer=tg.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),
            loss=tg.losses.SparseCategoricalCrossentropy(),
            metrics=['accuracy']
        )
        
        # Apply optimization technique
        if technique == 'quantization':
            quantizer = tg.quantization.DynamicQuantization(dtype=torch.qint8)
            model_copy = quantizer.quantize(model_copy)
        elif technique == 'pruning':
            pruner = tg.pruning.MagnitudePruning(sparsity=0.3, global_pruning=True)
            model_copy = pruner.prune(model_copy)
        elif technique == 'both':
            quantizer = tg.quantization.DynamicQuantization(dtype=torch.qint8)
            model_copy = quantizer.quantize(model_copy)
            pruner = tg.pruning.MagnitudePruning(sparsity=0.3, global_pruning=True)
            model_copy = pruner.prune(model_copy)
        
        # Train model
        start_time = time.time()
        history = model_copy.fit(x_train, y_train, epochs=3, batch_size=64, verbose=0)
        training_time = time.time() - start_time
        
        # Evaluate model
        test_loss, test_accuracy = model_copy.evaluate(x_train, y_train, verbose=0)
        
        results[technique_name] = {
            'accuracy': test_accuracy,
            'time': training_time,
            'final_loss': history['loss'][-1]
        }
        
        print(f"   Accuracy: {test_accuracy:.4f}")
        print(f"   Training time: {training_time:.2f}s")
        print(f"   Final loss: {history['loss'][-1]:.4f}")
    
    print(f"\nüìä Optimization Results Summary:")
    for technique, metrics in results.items():
        print(f"   {technique}: {metrics['accuracy']:.4f} accuracy, {metrics['time']:.2f}s")
    
    print("‚úÖ Advanced optimization demonstrated successfully!")


def main():
    """Main demonstration function."""
    print("üéØ TruthGPT API ULTIMATE Features Demonstration")
    print("=" * 60)
    
    try:
        # Demonstrate all ultimate features
        demonstrate_advanced_architectures()
        demonstrate_gan_training()
        demonstrate_quantization()
        demonstrate_pruning()
        demonstrate_distributed_training()
        demonstrate_comprehensive_pipeline()
        demonstrate_advanced_optimization()
        
        print("\nüéâ ULTIMATE features demonstration completed successfully!")
        print("TruthGPT API is ready for the most advanced deep learning applications!")
        
    except Exception as e:
        print(f"\n‚ùå Error during demonstration: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)









