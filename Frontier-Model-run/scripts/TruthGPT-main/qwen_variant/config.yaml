qwen_72b:
  vocab_size: 151936
  hidden_size: 8192
  intermediate_size: 49152
  num_hidden_layers: 80
  num_attention_heads: 64
  num_key_value_heads: 64
  max_position_embeddings: 32768
  rope_theta: 1000000.0
  rms_norm_eps: 1e-6
  use_sliding_window: false
  sliding_window: 4096
  max_window_layers: 70
  tie_word_embeddings: false
  dropout: 0.0
  attention_dropout: 0.0
  
  use_flash_attention: true
  use_moe: true
  num_experts: 64
  num_experts_per_tok: 8
  shared_expert_intermediate_size: 1408
  
  enable_quantization: true
  quantization_bits: 8
  enable_gradient_checkpointing: true
  enable_compilation: true

qwen_14b:
  vocab_size: 151936
  hidden_size: 5120
  intermediate_size: 27392
  num_hidden_layers: 40
  num_attention_heads: 40
  num_key_value_heads: 40
  max_position_embeddings: 32768
  rope_theta: 1000000.0
  rms_norm_eps: 1e-6
  use_sliding_window: false
  sliding_window: 4096
  max_window_layers: 35
  tie_word_embeddings: false
  dropout: 0.0
  attention_dropout: 0.0
  
  use_flash_attention: true
  use_moe: true
  num_experts: 64
  num_experts_per_tok: 8
  shared_expert_intermediate_size: 1408
  
  enable_quantization: true
  quantization_bits: 8
  enable_gradient_checkpointing: true
  enable_compilation: true

qwen_7b:
  vocab_size: 151936
  hidden_size: 4096
  intermediate_size: 22016
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 32
  max_position_embeddings: 32768
  rope_theta: 1000000.0
  rms_norm_eps: 1e-6
  use_sliding_window: false
  sliding_window: 4096
  max_window_layers: 28
  tie_word_embeddings: false
  dropout: 0.0
  attention_dropout: 0.0
  
  use_flash_attention: true
  use_moe: true
  num_experts: 64
  num_experts_per_tok: 8
  shared_expert_intermediate_size: 1408
  
  enable_quantization: true
  quantization_bits: 8
  enable_gradient_checkpointing: true
  enable_compilation: true

qwen_test:
  vocab_size: 1000
  hidden_size: 512
  intermediate_size: 2048
  num_hidden_layers: 4
  num_attention_heads: 8
  num_key_value_heads: 8
  max_position_embeddings: 2048
  rope_theta: 10000.0
  rms_norm_eps: 1e-6
  use_sliding_window: false
  sliding_window: 1024
  max_window_layers: 2
  tie_word_embeddings: false
  dropout: 0.0
  attention_dropout: 0.0
  
  use_flash_attention: false
  use_moe: true
  num_experts: 8
  num_experts_per_tok: 2
  shared_expert_intermediate_size: 256
  
  enable_quantization: false
  quantization_bits: 8
  enable_gradient_checkpointing: false
  enable_compilation: false

qwen_optimizations:
  enable_flash_attention: true
  enable_moe_optimization: true
  enable_gradient_checkpointing: true
  enable_quantization: true
  quantization_bits: 8
  enable_compilation: true
  enable_triton_kernels: true
  enable_cuda_kernels: true
  enable_memory_optimization: true
  optimization_level: "aggressive"

qwen_training:
  learning_rate: 0.0001
  batch_size: 8
  gradient_accumulation_steps: 4
  max_steps: 10000
  warmup_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  use_grpo: true
  grpo_beta: 0.1
  grpo_gamma: 0.99
  grpo_eps: 1e-8
  
  use_mixed_precision: true
  use_gradient_checkpointing: true
  
  eval_steps: 500
  save_steps: 1000
  logging_steps: 100
  output_dir: "./qwen_checkpoints"

qwen_benchmarks:
  batch_sizes: [1, 2, 4, 8, 16]
  sequence_lengths: [512, 1024, 2048, 4096]
  num_warmup_runs: 5
  num_benchmark_runs: 20
  measure_memory: true
  measure_throughput: true
  measure_latency: true
  measure_flops: false
  save_results: true
