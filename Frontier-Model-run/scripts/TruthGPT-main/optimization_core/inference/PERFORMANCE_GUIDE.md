# âš¡ Performance Optimization Guide

Complete guide for optimizing inference API performance.

## ðŸ“Š Performance Metrics

### Key Metrics to Monitor

1. **Latency**
   - p50: Median latency
   - p95: 95th percentile (target: <300ms)
   - p99: 99th percentile (target: <500ms)

2. **Throughput**
   - Requests per second (RPS)
   - Tokens per second
   - Batch processing rate

3. **Efficiency**
   - Cache hit rate (target: >50%)
   - Error rate (target: <1%)
   - Queue depth (target: <50)

4. **Resources**
   - CPU usage (target: <80%)
   - Memory usage (target: <8GB)
   - GPU utilization (if applicable)

## ðŸŽ¯ Optimization Strategies

### 1. Batching Configuration

**Problem**: High latency or low throughput

**Solutions**:
```bash
# Increase batch size for better GPU utilization
export BATCH_MAX_SIZE=64

# Reduce flush timeout for lower latency
export BATCH_FLUSH_TIMEOUT_MS=10

# Balance based on load
# - High traffic: Larger batches, shorter timeout
# - Low traffic: Smaller batches, longer timeout
```

**Expected Impact**: 20-40% latency reduction, 2-3x throughput increase

### 2. Caching Strategy

**Problem**: Low cache hit rate

**Solutions**:
```bash
# Enable Redis for distributed caching
export CACHE_BACKEND=redis
export REDIS_URL=redis://localhost:6379/0

# Increase cache TTL
export CACHE_DEFAULT_TTL=7200  # 2 hours

# Normalize prompts for better cache matching
# - Strip whitespace
# - Lowercase (if semantic similarity not needed)
# - Remove variations
```

**Expected Impact**: 50-80% latency reduction for cached requests

### 3. Rate Limiting Tuning

**Problem**: Rate limit errors or resource exhaustion

**Solutions**:
```bash
# Adjust per-minute limit
export RATE_LIMIT_RPM=1000

# Configure per-endpoint limits (in code)
rate_limiter.configure_endpoint("/v1/infer", rpm=500, rph=50000)
```

**Expected Impact**: Better resource utilization, reduced errors

### 4. Circuit Breaker Configuration

**Problem**: Cascading failures

**Solutions**:
```bash
# Adjust failure threshold
export CIRCUIT_BREAKER_FAILURE_THRESHOLD=3

# Reduce timeout for faster recovery
export CIRCUIT_BREAKER_TIMEOUT_SEC=30
```

**Expected Impact**: Improved reliability, faster recovery

### 5. Horizontal Scaling

**Problem**: High queue depth, high CPU/memory

**Solutions**:
```yaml
# Kubernetes HPA
minReplicas: 3
maxReplicas: 10
targetCPUUtilization: 70
targetMemoryUtilization: 80
```

**Expected Impact**: Linear throughput scaling

### 6. Model Optimization

**Problem**: High latency, high memory

**Solutions**:
- Use quantization (int8/int4)
- Enable TensorRT/ONNX Runtime
- Use model pruning
- Implement KV cache optimization

**Expected Impact**: 30-50% latency reduction, 50% memory reduction

## ðŸ› ï¸ Tools for Optimization

### 1. Performance Tuner

```bash
python -m inference.utils.performance_tuner --url http://localhost:8080
```

### 2. Benchmark Tool

```bash
python -m inference.utils.benchmark \
  --url http://localhost:8080 \
  --requests 1000 \
  --concurrency 50
```

### 3. Load Testing

```bash
k6 run tests/load-test.js \
  --env API_URL=http://localhost:8080 \
  --env API_TOKEN=your-token
```

### 4. Metrics Monitoring

```bash
# View Prometheus metrics
curl http://localhost:8080/metrics

# Query specific metrics
curl "http://localhost:9090/api/v1/query?query=inference_request_duration_ms"
```

## ðŸ“ˆ Performance Targets (SLO)

| Metric | Target | Critical |
|--------|--------|----------|
| p95 Latency | <300ms | <600ms |
| p99 Latency | <500ms | <1000ms |
| Error Rate | <0.5% | <2% |
| Cache Hit Rate | >50% | >30% |
| Queue Depth | <50 | <100 |
| CPU Usage | <70% | <90% |
| Memory Usage | <6GB | <8GB |

## ðŸ” Troubleshooting

### High Latency

1. Check queue depth: `inference_queue_depth`
2. Check batch utilization: `inference_active_batches`
3. Check cache hit rate
4. Review model performance
5. Check network latency

### Low Throughput

1. Increase batch size
2. Scale horizontally
3. Optimize model inference
4. Check for bottlenecks (CPU/GPU/Network)

### High Error Rate

1. Check circuit breakers: `/health` endpoint
2. Review error logs
3. Check resource limits
4. Verify model availability

### Memory Issues

1. Reduce batch size
2. Enable model quantization
3. Implement memory-efficient batching
4. Scale vertically

## ðŸ“š Best Practices

1. **Monitor Continuously**: Set up dashboards and alerts
2. **Test Regularly**: Run benchmarks after changes
3. **Tune Iteratively**: Make small changes and measure
4. **Document Changes**: Keep track of what works
5. **Load Test**: Test under realistic conditions
6. **Plan for Scale**: Design for growth

## ðŸŽ¯ Quick Wins

1. âœ… Enable Redis caching (5 minutes)
2. âœ… Increase batch size (2 minutes)
3. âœ… Configure rate limits (5 minutes)
4. âœ… Set up monitoring (15 minutes)
5. âœ… Enable compression (5 minutes)

---

**Version**: 1.0.0  
**Last Updated**: 2025-01-30


