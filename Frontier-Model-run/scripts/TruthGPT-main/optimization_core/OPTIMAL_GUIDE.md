# üéØ GU√çA √ìPTIMA - M√ÅXIMA OPTIMIZACI√ìN

## ‚ö° Configuraci√≥n √ìptima para TruthGPT

### üì¶ Instalaci√≥n √ìptima

```bash
# Instalar versiones exactas optimizadas
pip install -r requirements_optimal.txt

# O desde PyTorch index (CUDA 11.8)
pip install torch==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118
```

## üéØ Stack √ìptimo Recomendado

### 1. **PyTorch + CUDA**
```bash
torch==2.1.0+cu118         # CUDA 11.8 optimizada
torchvision==0.16.0+cu118
torchaudio==2.1.0+cu118
```

### 2. **Optimizaci√≥n GPU**
```bash
flash-attn==2.4.0          # 3x faster attention
xformers==0.0.23           # 2x faster operations
triton==3.0.0              # JIT compilation
cupy-cuda11x==12.3.0       # NumPy GPU
```

### 3. **Entrenamiento Eficiente**
```bash
peft==0.6.0                # LoRA, QLoRA
deepspeed==0.12.0          # ZeRO optimization
fairscale==0.4.13          # Sharded DDP
```

## üöÄ Configuraci√≥n √ìptima de C√≥digo

### Modelo Optimizado
```python
import torch
from transformers import AutoModelForCausalLM
from accelerate import Accelerator
from peft import LoraConfig, get_peft_model
from flash_attn import flash_attn_func

# Configuraci√≥n √≥ptima
config = {
    'model_name': 'gpt2',
    'precision': 'fp16',
    'enable_flash_attention': True,
    'enable_gradient_checkpointing': True,
    'use_peft': True,
}

# Cargar modelo con optimizaciones
model = AutoModelForCausalLM.from_pretrained(
    config['model_name'],
    torch_dtype=torch.float16,  # FP16
    device_map='auto',           # Distributed
)

# PEFT para fine-tuning eficiente
if config['use_peft']:
    lora_config = LoraConfig(
        r=16,                     # Rank
        lora_alpha=32,
        target_modules=["c_attn", "c_proj"],
        lora_dropout=0.1
    )
    model = get_peft_model(model, lora_config)

# Accelerator
accelerator = Accelerator(
    mixed_precision='fp16',
    gradient_accumulation_steps=4
)

model = accelerator.prepare(model)
```

### Entrenamiento Optimizado
```python
from torch.cuda.amp import autocast, GradScaler
import deepspeed

# Mixed precision
scaler = GradScaler()

# Training loop √≥ptimo
for batch in dataloader:
    with autocast():
        outputs = model(**batch)
        loss = outputs.loss
    
    # Backward optimizado
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### DataLoader Optimizado
```python
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,              # Paralelo
    pin_memory=True,             # Transferencia r√°pida
    persistent_workers=True,    # Reutilizar workers
    prefetch_factor=2,          # Prefetch
    drop_last=True,             # Batch size consistente
)
```

## üìä Benchmarks √ìptimos

### Speedup Esperado

| Optimizaci√≥n | Speedup | Descripci√≥n |
|--------------|---------|-------------|
| FP16 Mixed Precision | **1.5x** | Entrenamiento m√°s r√°pido |
| Flash Attention | **3x** | Atenci√≥n GPU |
| XFormers | **2x** | Operaciones eficientes |
| Gradient Checkpointing | **1.3x** | Menos memoria |
| PEFT LoRA | **2x** | Par√°metros reducidos |
| DeepSpeed ZeRO | **2-4x** | Distribuido |
| **COMBINADO** | **15-20x** ‚ö° | Todo junto |

### Uso de Memoria

| Optimizaci√≥n | Reducci√≥n | Descripci√≥n |
|--------------|----------|-------------|
| FP16 | **-50%** | Mitad de memoria |
| Gradient Checkpointing | **-40%** | Activaciones |
| PEFT LoRA | **-90%** | Par√°metros entrenables |
| DeepSpeed ZeRO | **-75%** | Gradients distribuidos |
| **COMBINADO** | **-95%** üíæ | Todo junto |

## üéØ Configuraci√≥n por Caso de Uso

### Caso 1: Single GPU (Fast)
```python
config = {
    'precision': 'fp16',
    'flash_attention': True,
    'gradient_checkpointing': True,
    'use_peft': True,
}
```

### Caso 2: Multi-GPU (Faster)
```python
config = {
    'precision': 'fp16',
    'flash_attention': True,
    'gradient_checkpointing': True,
    'use_peft': True,
    'use_deepspeed': True,
    'num_gpus': 4,
}
```

### Caso 3: Memory Constrained (Optimal)
```python
config = {
    'precision': 'int8',
    'quantization': 'bitsandbytes',
    'gradient_checkpointing': True,
    'use_peft': True,
    'use_flash_attention': True,
    'micro_batch_size': 1,
}
```

## ‚úÖ Checklist √ìptimo

### Instalaci√≥n
- [ ] PyTorch con CUDA correcta
- [ ] Flash Attention instalado
- [ ] XFormers instalado
- [ ] DeepSpeed instalado
- [ ] PEFT instalado

### Configuraci√≥n
- [ ] FP16/BF16 habilitado
- [ ] Flash Attention habilitado
- [ ] Gradient checkpointing ON
- [ ] PEFT configurado
- [ ] DeepSpeed configurado (si multi-GPU)

### Entrenamiento
- [ ] Mixed precision ON
- [ ] Gradient accumulation configurado
- [ ] Learning rate schedule configurado
- [ ] Early stopping configurado
- [ ] Monitoring (WandB) activado

## üîß Verificaci√≥n √ìptima

```python
import torch
from flash_attn import flash_attn_func

# Verificar GPU
print(f"CUDA: {torch.cuda.is_available()}")
print(f"GPUs: {torch.cuda.device_count()}")
print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Verificar Flash Attention
try:
    from flash_attn import flash_attn_func
    print("‚úÖ Flash Attention: OK")
except ImportError:
    print("‚ùå Flash Attention: NOT INSTALLED")

# Verificar XFormers
try:
    import xformers
    print("‚úÖ XFormers: OK")
except ImportError:
    print("‚ùå XFormers: NOT INSTALLED")

# Verificar PEFT
try:
    import peft
    print("‚úÖ PEFT: OK")
except ImportError:
    print("‚ùå PEFT: NOT INSTALLED")

# Verificar DeepSpeed
try:
    import deepspeed
    print("‚úÖ DeepSpeed: OK")
except ImportError:
    print("‚ùå DeepSpeed: NOT INSTALLED")
```

## üéì Best Practices

### 1. Always use FP16 for training
```python
torch_dtype=torch.float16
```

### 2. Enable Flash Attention
```python
attn_implementation="flash_attention_2"
```

### 3. Use Gradient Checkpointing
```python
model.gradient_checkpointing_enable()
```

### 4. Use PEFT for fine-tuning
```python
from peft import LoraConfig, get_peft_model
```

### 5. Monitor with WandB
```python
import wandb
wandb.init(project="truthgpt")
```

## üöÄ Comandos R√°pidos

```bash
# Instalar todo
pip install -r requirements_optimal.txt

# Verificar instalaci√≥n
python -c "import torch; print(torch.cuda.is_available())"

# Benchmark
python -m pytest pytest-benchmark
```

---

**¬°Ahora tienes la configuraci√≥n m√°s optimizada posible!** üéØ‚ö°

