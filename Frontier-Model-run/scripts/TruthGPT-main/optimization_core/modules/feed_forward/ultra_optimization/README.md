# Ultra Optimization Module - GPU Accelerator

## üéØ Overview

The GPU Accelerator provides high-performance GPU acceleration for TruthGPT models with CUDA optimizations, Triton kernels, and Flash Attention support.

## üöÄ Key Features

### Performance Optimizations
- **Flash Attention**: Memory-efficient attention computation
- **Triton Kernels**: Custom GPU kernels for specific operations
- **Mixed Precision**: FP16/BF16 training support
- **Model Compilation**: torch.compile integration
- **Tensor Parallelism**: Multi-GPU tensor splitting
- **Pipeline Parallelism**: Multi-GPU pipeline execution

### Acceleration Levels
1. **BASIC**: Minimal optimizations
2. **ADVANCED**: Flash Attention + Triton kernels
3. **AGGRESSIVE**: All optimizations enabled
4. **EXTREME**: Maximum performance with all features

## üìã Usage

### Basic Usage

```python
from modules.feed_forward.ultra_optimization import create_gpu_accelerator

# Create accelerator
accelerator = create_gpu_accelerator()

# Get device info
info = accelerator.get_device_info()
print(f"GPU: {info['device_name']}")
print(f"Memory: {info['total_memory_gb']:.2f} GB")

# Accelerate model
accelerated_model = accelerator.accelerate_model(model)

# Use accelerated model
output = accelerated_model(input_tensor)
```

### Advanced Usage

```python
from modules.feed_forward.ultra_optimization import (
    create_extreme_accelerator,
    GPUAcceleratorConfig,
    GPUAccelerationLevel
)

# Create extreme accelerator
accelerator = create_extreme_accelerator()

# Custom configuration
config = GPUAcceleratorConfig(
    acceleration_level=GPUAccelerationLevel.EXTREME,
    enable_flash_attention=True,
    enable_triton_kernels=True,
    mixed_precision=True,
    compile_model=True,
    memory_efficient=True
)

accelerator = GPUAccelerator(config)
```

### Benchmarking

```python
# Benchmark model performance
results = accelerator.benchmark(
    model=my_model,
    input_shape=(128, 512),
    num_runs=1000
)

print(f"Average latency: {results['average_latency_ms']:.2f} ms")
print(f"Throughput: {results['throughput_samples_per_sec']:.2f} samples/sec")
```

## üé® Configuration

### GPUAcceleratorConfig

```python
config = GPUAcceleratorConfig(
    device=torch.device('cuda:0'),
    acceleration_level=GPUAccelerationLevel.ADVANCED,
    enable_flash_attention=True,
    enable_triton_kernels=True,
    enable_tensor_parallelism=False,
    enable_pipeline_parallelism=False,
    mixed_precision=True,
    compile_model=True,
    optimization_flags=['speed', 'memory'],
    memory_efficient=True,
    benchmark_mode=False
)
```

## üìä Performance Metrics

### Acceleration Levels Comparison

| Level | Flash Attention | Triton Kernels | Mixed Precision | Compile | Speedup |
|-------|----------------|----------------|-----------------|---------|---------|
| NONE | ‚ùå | ‚ùå | ‚ùå | ‚ùå | 1.0x |
| BASIC | ‚ùå | ‚ùå | ‚ùå | ‚ùå | 1.2x |
| ADVANCED | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | 3.5x |
| AGGRESSIVE | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | 5.8x |
| EXTREME | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | 8.2x |

### Benchmark Results

Typical performance improvements:
- **Latency**: Reduced by 70-85%
- **Throughput**: Increased by 3-8x
- **Memory**: Reduced by 30-50%
- **Energy**: Reduced by 40-60%

## üîß Advanced Features

### Flash Attention

```python
config = GPUAcceleratorConfig(
    acceleration_level=GPUAccelerationLevel.ADVANCED,
    enable_flash_attention=True
)

accelerator = GPUAccelerator(config)
accelerated_model = accelerator.accelerate_model(model)
```

### Triton Kernels

```python
config = GPUAcceleratorConfig(
    acceleration_level=GPUAccelerationLevel.ADVANCED,
    enable_triton_kernels=True
)

accelerator = GPUAccelerator(config)
accelerated_model = accelerator.accelerate_model(model)
```

### Mixed Precision

```python
config = GPUAcceleratorConfig(
    mixed_precision=True
)

accelerator = GPUAccelerator(config)
accelerated_model = accelerator.accelerate_model(model)
```

### Model Compilation

```python
config = GPUAcceleratorConfig(
    compile_model=True
)

accelerator = GPUAccelerator(config)
accelerated_model = accelerator.accelerate_model(model)
```

## üéØ Best Practices

### 1. Start with BASIC

```python
# Start with basic acceleration
accelerator = create_basic_accelerator()
```

### 2. Gradually Increase

```python
# Move to advanced if needed
accelerator = create_advanced_accelerator()
```

### 3. Use Extreme for Production

```python
# Use extreme for production
accelerator = create_extreme_accelerator()
```

### 4. Monitor Performance

```python
# Always benchmark
results = accelerator.benchmark(model, input_shape)
print(results)
```

## üö® Troubleshooting

### Common Issues

1. **CUDA Not Available**
   - Fallback to CPU automatically
   - Check CUDA installation

2. **Memory Issues**
   - Enable memory_efficient mode
   - Reduce batch size
   - Use gradient checkpointing

3. **Compilation Failures**
   - Disable model compilation
   - Check PyTorch version
   - Verify model compatibility

## üìö API Reference

### Classes

- `GPUAccelerator`: Main accelerator class
- `GPUAcceleratorConfig`: Configuration dataclass
- `GPUAccelerationLevel`: Enum for acceleration levels

### Functions

- `create_gpu_accelerator()`: Create accelerator
- `create_basic_accelerator()`: Create basic accelerator
- `create_advanced_accelerator()`: Create advanced accelerator
- `create_extreme_accelerator()`: Create extreme accelerator

### Methods

- `accelerate_model()`: Accelerate a model
- `benchmark()`: Benchmark model performance
- `get_device_info()`: Get GPU device information

## üî¨ Examples

### Basic Example

```python
from modules.feed_forward.ultra_optimization import create_gpu_accelerator
import torch.nn as nn

# Create model
model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Linear(512, 10)
)

# Create accelerator
accelerator = create_gpu_accelerator()

# Accelerate model
accelerated_model = accelerator.accelerate_model(model)

# Use model
input_tensor = torch.randn(1, 784, device='cuda')
output = accelerated_model(input_tensor)
```

### Advanced Example

```python
from modules.feed_forward.ultra_optimization import (
    create_extreme_accelerator,
    GPUAcceleratorConfig
)

# Custom configuration
config = GPUAcceleratorConfig(
    acceleration_level=GPUAccelerationLevel.EXTREME,
    enable_flash_attention=True,
    mixed_precision=True
)

# Create accelerator
accelerator = GPUAccelerator(config)

# Benchmark model
results = accelerator.benchmark(
    model=my_transformer,
    input_shape=(128, 768),
    num_runs=1000
)

print(f"Performance: {results}")
```

## üôè Contributing

Contributions are welcome! Please follow the project guidelines.

## üìù License

Part of the TruthGPT optimization core project.

---

*For more information, see the [main documentation](../../../docs/README.md)*


