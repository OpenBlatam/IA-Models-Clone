# ğŸ—ï¸ TruthGPT Optimization Core - Architecture

## ğŸ“ Arquitectura Modular

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Configuration YAML                        â”‚
â”‚              (configs/llm_default.yaml)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Build System                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ build.py     â”‚  â”‚build_trainer â”‚  â”‚validate_configâ”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Registries (Factories)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚Attention â”‚ â”‚Optimizer â”‚ â”‚Datasets  â”‚ â”‚Callbacks â”‚        â”‚
â”‚  â”‚KV Cache  â”‚ â”‚Memory    â”‚ â”‚Collate   â”‚ â”‚Metrics   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GenericTrainer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ â€¢ Mixed Precision (bf16/fp16)                          â”‚   â”‚
â”‚  â”‚ â€¢ TF32 Acceleration                                    â”‚   â”‚
â”‚  â”‚ â€¢ torch.compile Support                                â”‚   â”‚
â”‚  â”‚ â€¢ Fused AdamW Optimizer                               â”‚   â”‚
â”‚  â”‚ â€¢ EMA Weights                                          â”‚   â”‚
â”‚  â”‚ â€¢ Gradient Clipping                                    â”‚   â”‚
â”‚  â”‚ â€¢ NaN Detection                                        â”‚   â”‚
â”‚  â”‚ â€¢ Periodic Checkpointing                               â”‚   â”‚
â”‚  â”‚ â€¢ Auto-resume                                          â”‚   â”‚
â”‚  â”‚ â€¢ Dynamic Padding + Bucketing                         â”‚   â”‚
â”‚  â”‚ â€¢ Tokens/sec Tracking                                  â”‚   â”‚
â”‚  â”‚ â€¢ Early Stopping                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Output                                    â”‚
â”‚  â€¢ Checkpoints (best.pt, last.pt, step_*.pt)               â”‚
â”‚  â€¢ W&B/TensorBoard Logs                                     â”‚
â”‚  â€¢ Model Artifacts                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Flujo de Datos

```
YAML Config
    â†“
build_components() â†’ [attention, kv_cache, memory, optimizer, ...]
    â†“
build_trainer() â†’ GenericTrainer
    â†“
train() â†’ [Load Data â†’ Forward â†’ Backward â†’ Optimize â†’ Log â†’ Eval â†’ Checkpoint]
    â†“
Output: Checkpoints + Logs
```

## ğŸ“¦ Componentes Modulares

### 1. Attention Backends
- **sdpa**: PyTorch SDPA (default, siempre disponible)
- **flash**: Flash Attention (fallback a sdpa si no disponible)
- **triton**: Triton kernels (fallback a sdpa si no disponible)

### 2. KV Cache
- **none**: Sin cache (para entrenamiento)
- **paged**: PagedKVCache (para inferencia eficiente)

### 3. Memory Management
- **adaptive**: AdvancedMemoryManager con detecciÃ³n GPU
- **static**: ConfiguraciÃ³n estÃ¡tica bÃ¡sica

### 4. Optimizers
- **adamw**: AdamW fused (default)
- **lion**: Lion optimizer (stub, fallback a AdamW)
- **adafactor**: Adafactor (stub, fallback a AdamW)

### 5. Callbacks
- **print**: PrintLogger (siempre disponible)
- **wandb**: Weights & Biases (requiere `pip install wandb`)
- **tensorboard**: TensorBoard (requiere `pip install tensorboard`)

### 6. Datasets
- **hf**: HuggingFace datasets (streaming opcional)
- **jsonl**: JSONL files (iterable)
- **webdataset**: WebDataset (stub para futuro)

### 7. Collate Functions
- **lm**: Language modeling (dynamic padding)
- **cv**: Computer vision (stub)

### 8. Metrics
- **loss**: Validation loss
- **ppl**: Perplexity (exp(loss))

## ğŸ›ï¸ ConfiguraciÃ³n por Capas

```
Layer 1: Model Configuration
â”œâ”€â”€ name_or_path
â”œâ”€â”€ attention.backend
â”œâ”€â”€ kv_cache.type
â”œâ”€â”€ memory.policy
â””â”€â”€ lora.* (opcional)

Layer 2: Training Configuration
â”œâ”€â”€ epochs, batch_size, lr
â”œâ”€â”€ optimizer.type
â”œâ”€â”€ callbacks
â”œâ”€â”€ performance flags (tf32, compile)
â””â”€â”€ early stopping

Layer 3: Data Configuration
â”œâ”€â”€ source (hf/jsonl/webdataset)
â”œâ”€â”€ streaming
â”œâ”€â”€ collate
â””â”€â”€ bucketing

Layer 4: Infrastructure
â”œâ”€â”€ checkpoint.*
â”œâ”€â”€ ema.*
â”œâ”€â”€ resume.*
â”œâ”€â”€ eval.*
â””â”€â”€ logging.*
```

## ğŸ”Œ Extension Points

Para agregar nuevos componentes sin tocar cÃ³digo core:

1. **Nuevo Backend de Attention:**
   ```python
   # En factories/attention.py
   @ATTENTION_BACKENDS.register("mi_backend")
   def build_mi_backend():
       return mi_attention_function
   ```

2. **Nuevo Optimizer:**
   ```python
   # En factories/optimizer.py
   @OPTIMIZERS.register("mi_optimizer")
   def build_mi_optimizer(params, lr, **kwargs):
       return MiOptimizer(params, lr=lr)
   ```

3. **Nuevo Callback:**
   ```python
   # En factories/callbacks.py
   @CALLBACKS.register("mi_callback")
   def build_mi_callback(**kwargs):
       return MiCallback(**kwargs)
   ```

## ğŸš€ Performance Tuning Guide

### Nivel 1: BÃ¡sico
- `allow_tf32: true`
- `mixed_precision: bf16`
- `fused_adamw: true`

### Nivel 2: Intermedio
- `torch_compile: true`
- `compile_mode: reduce-overhead`
- `bucket_by_length: true`

### Nivel 3: Avanzado
- `torch_compile: true`
- `compile_mode: max-autotune`
- `data.prefetch_factor: 4`
- `data.num_workers: 8`

## ğŸ“Š MÃ©tricas y Observabilidad

### Durante Entrenamiento
- `loss` - Loss promedio por log_interval
- `tokens_per_sec` - Throughput en tiempo real
- `lr` - Learning rate actual (vÃ­a callbacks)

### Durante EvaluaciÃ³n
- `val_loss` - Loss de validaciÃ³n
- `ppl` - Perplexity (exp(val_loss))
- `best_metric` - Mejor mÃ©trica segÃºn `select_best_by`

### En W&B/TensorBoard
- Todos los logs automÃ¡ticamente
- GrÃ¡ficas de loss, ppl, tokens/s
- ComparaciÃ³n de runs

## ğŸ”’ Principios de DiseÃ±o

1. **Modularidad**: Componentes intercambiables vÃ­a registries
2. **Configurabilidad**: Todo desde YAML, sin cÃ³digo
3. **Fallbacks**: DegradaciÃ³n elegante si componentes opcionales no disponibles
4. **Performance**: Optimizaciones aplicadas automÃ¡ticamente cuando es posible
5. **Observabilidad**: Logging y mÃ©tricas integradas
6. **Robustez**: Manejo de errores, NaN detection, auto-resume

---

**Para mÃ¡s detalles, ver `README.md` y `QUICK_REFERENCE.md`**


