"""
Advanced Neural Network Multi-Modal AI System for TruthGPT Optimization Core
Complete multi-modal AI with vision, audio, text fusion and cross-modal attention
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union, Callable
import logging
from dataclasses import dataclass, field
from enum import Enum
import time
import json
from pathlib import Path
import pickle
from abc import ABC, abstractmethod
import math
import random
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

class ModalityType(Enum):
    """Modality types"""
    VISION = "vision"
    AUDIO = "audio"
    TEXT = "text"
    VIDEO = "video"
    SENSORY = "sensory"
    MULTIMODAL = "multimodal"

class FusionStrategy(Enum):
    """Fusion strategies"""
    EARLY_FUSION = "early_fusion"
    LATE_FUSION = "late_fusion"
    INTERMEDIATE_FUSION = "intermediate_fusion"
    ATTENTION_FUSION = "attention_fusion"
    CROSS_MODAL_FUSION = "cross_modal_fusion"
    HIERARCHICAL_FUSION = "hierarchical_fusion"

class AttentionType(Enum):
    """Attention types"""
    SELF_ATTENTION = "self_attention"
    CROSS_ATTENTION = "cross_attention"
    MULTI_HEAD_ATTENTION = "multi_head_attention"
    SPATIAL_ATTENTION = "spatial_attention"
    TEMPORAL_ATTENTION = "temporal_attention"
    MODALITY_ATTENTION = "modality_attention"

class MultiModalConfig:
    """Configuration for multi-modal AI system"""
    # Basic settings
    modalities: List[ModalityType] = field(default_factory=lambda: [ModalityType.VISION, ModalityType.AUDIO, ModalityType.TEXT])
    fusion_strategy: FusionStrategy = FusionStrategy.ATTENTION_FUSION
    attention_type: AttentionType = AttentionType.MULTI_HEAD_ATTENTION
    
    # Vision settings
    vision_input_size: Tuple[int, int, int] = (224, 224, 3)
    vision_backbone: str = "resnet50"
    vision_feature_dim: int = 2048
    
    # Audio settings
    audio_sample_rate: int = 16000
    audio_window_size: int = 1024
    audio_hop_size: int = 512
    audio_feature_dim: int = 512
    
    # Text settings
    text_vocab_size: int = 30000
    text_max_length: int = 512
    text_embedding_dim: int = 768
    
    # Fusion settings
    fusion_dim: int = 1024
    num_attention_heads: int = 8
    attention_dropout: float = 0.1
    
    # Training settings
    learning_rate: float = 0.001
    batch_size: int = 32
    num_epochs: int = 100
    
    # Advanced features
    enable_modality_dropout: bool = True
    modality_dropout_rate: float = 0.1
    enable_contrastive_learning: bool = True
    enable_cross_modal_transfer: bool = True
    enable_multimodal_augmentation: bool = True
    
    def __post_init__(self):
        """Validate multi-modal configuration"""
        if not self.modalities:
            raise ValueError("At least one modality must be specified")
        if self.vision_feature_dim <= 0:
            raise ValueError("Vision feature dimension must be positive")
        if self.audio_feature_dim <= 0:
            raise ValueError("Audio feature dimension must be positive")
        if self.text_embedding_dim <= 0:
            raise ValueError("Text embedding dimension must be positive")
        if self.fusion_dim <= 0:
            raise ValueError("Fusion dimension must be positive")
        if self.num_attention_heads <= 0:
            raise ValueError("Number of attention heads must be positive")
        if not (0 <= self.attention_dropout <= 1):
            raise ValueError("Attention dropout must be between 0 and 1")
        if self.learning_rate <= 0:
            raise ValueError("Learning rate must be positive")
        if self.batch_size <= 0:
            raise ValueError("Batch size must be positive")
        if self.num_epochs <= 0:
            raise ValueError("Number of epochs must be positive")
        if not (0 <= self.modality_dropout_rate <= 1):
            raise ValueError("Modality dropout rate must be between 0 and 1")

class VisionProcessor:
    """Vision modality processor"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.vision_backbone = self._create_vision_backbone()
        self.vision_features = []
        logger.info("‚úÖ Vision Processor initialized")
    
    def _create_vision_backbone(self) -> nn.Module:
        """Create vision backbone"""
        if self.config.vision_backbone == "resnet50":
            backbone = nn.Sequential(
                nn.Conv2d(3, 64, 7, 2, 3),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(3, 2, 1),
                # Simplified ResNet blocks
                self._resnet_block(64, 64, 2),
                self._resnet_block(64, 128, 2),
                self._resnet_block(128, 256, 2),
                self._resnet_block(256, 512, 2),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(512, self.config.vision_feature_dim)
            )
        else:
            # Default CNN backbone
            backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, 1, 1),
                nn.BatchNorm2d(64),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, 1, 1),
                nn.BatchNorm2d(128),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(256, self.config.vision_feature_dim)
            )
        
        return backbone
    
    def _resnet_block(self, in_channels: int, out_channels: int, stride: int) -> nn.Module:
        """Create ResNet block"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride, 1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, 1, 1),
            nn.BatchNorm2d(out_channels)
        )
    
    def process_vision(self, vision_data: torch.Tensor) -> torch.Tensor:
        """Process vision data"""
        logger.info("üëÅÔ∏è Processing vision data")
        
        # Process through vision backbone
        vision_features = self.vision_backbone(vision_data)
        
        # Store features
        self.vision_features.append(vision_features)
        
        return vision_features

class AudioProcessor:
    """Audio modality processor"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.audio_backbone = self._create_audio_backbone()
        self.audio_features = []
        logger.info("‚úÖ Audio Processor initialized")
    
    def _create_audio_backbone(self) -> nn.Module:
        """Create audio backbone"""
        backbone = nn.Sequential(
            # Mel spectrogram processing
            nn.Conv1d(1, 64, 3, 1, 1),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(64, 128, 3, 1, 1),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(128, 256, 3, 1, 1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(256, self.config.audio_feature_dim)
        )
        
        return backbone
    
    def process_audio(self, audio_data: torch.Tensor) -> torch.Tensor:
        """Process audio data"""
        logger.info("üéµ Processing audio data")
        
        # Convert to mel spectrogram (simplified)
        mel_spec = self._audio_to_mel_spectrogram(audio_data)
        
        # Process through audio backbone
        audio_features = self.audio_backbone(mel_spec)
        
        # Store features
        self.audio_features.append(audio_features)
        
        return audio_features
    
    def _audio_to_mel_spectrogram(self, audio_data: torch.Tensor) -> torch.Tensor:
        """Convert audio to mel spectrogram"""
        # Simplified mel spectrogram conversion
        # In practice, you would use librosa or torchaudio
        batch_size = audio_data.shape[0]
        mel_spec = torch.randn(batch_size, 1, 128)  # Simplified
        return mel_spec

class TextProcessor:
    """Text modality processor"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.text_backbone = self._create_text_backbone()
        self.text_features = []
        logger.info("‚úÖ Text Processor initialized")
    
    def _create_text_backbone(self) -> nn.Module:
        """Create text backbone"""
        backbone = nn.Sequential(
            nn.Embedding(self.config.text_vocab_size, self.config.text_embedding_dim),
            nn.LSTM(self.config.text_embedding_dim, self.config.text_embedding_dim, 
                   batch_first=True, bidirectional=True),
            nn.Linear(self.config.text_embedding_dim * 2, self.config.text_embedding_dim)
        )
        
        return backbone
    
    def process_text(self, text_data: torch.Tensor) -> torch.Tensor:
        """Process text data"""
        logger.info("üìù Processing text data")
        
        # Process through text backbone
        embedded = self.text_backbone[0](text_data)
        lstm_out, _ = self.text_backbone[1](embedded)
        text_features = self.text_backbone[2](lstm_out[:, -1, :])  # Take last output
        
        # Store features
        self.text_features.append(text_features)
        
        return text_features

class CrossModalAttention:
    """Cross-modal attention mechanism"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.attention_layers = {}
        logger.info("‚úÖ Cross-Modal Attention initialized")
    
    def create_attention_layer(self, query_dim: int, key_dim: int, value_dim: int) -> nn.Module:
        """Create attention layer"""
        attention_layer = nn.MultiheadAttention(
            embed_dim=query_dim,
            num_heads=self.config.num_attention_heads,
            dropout=self.config.attention_dropout,
            batch_first=True
        )
        
        return attention_layer
    
    def apply_cross_modal_attention(self, query: torch.Tensor, key: torch.Tensor, 
                                  value: torch.Tensor) -> torch.Tensor:
        """Apply cross-modal attention"""
        logger.info("üîó Applying cross-modal attention")
        
        # Ensure dimensions match
        if query.shape[-1] != key.shape[-1]:
            # Project to same dimension
            projection = nn.Linear(key.shape[-1], query.shape[-1])
            key = projection(key)
            value = projection(value)
        
        # Create attention layer if not exists
        layer_key = f"{query.shape[-1]}_{key.shape[-1]}"
        if layer_key not in self.attention_layers:
            self.attention_layers[layer_key] = self.create_attention_layer(
                query.shape[-1], key.shape[-1], value.shape[-1]
            )
        
        attention_layer = self.attention_layers[layer_key]
        
        # Apply attention
        attended_output, attention_weights = attention_layer(query, key, value)
        
        return attended_output, attention_weights

class FusionEngine:
    """Multi-modal fusion engine"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        self.fusion_layers = {}
        self.cross_modal_attention = CrossModalAttention(config)
        logger.info("‚úÖ Fusion Engine initialized")
    
    def fuse_modalities(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Fuse modalities based on strategy"""
        logger.info(f"üîó Fusing modalities using {self.config.fusion_strategy.value}")
        
        if self.config.fusion_strategy == FusionStrategy.EARLY_FUSION:
            return self._early_fusion(modality_features)
        elif self.config.fusion_strategy == FusionStrategy.LATE_FUSION:
            return self._late_fusion(modality_features)
        elif self.config.fusion_strategy == FusionStrategy.INTERMEDIATE_FUSION:
            return self._intermediate_fusion(modality_features)
        elif self.config.fusion_strategy == FusionStrategy.ATTENTION_FUSION:
            return self._attention_fusion(modality_features)
        elif self.config.fusion_strategy == FusionStrategy.CROSS_MODAL_FUSION:
            return self._cross_modal_fusion(modality_features)
        else:
            return self._hierarchical_fusion(modality_features)
    
    def _early_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Early fusion strategy"""
        # Concatenate all modality features
        fused_features = torch.cat(list(modality_features.values()), dim=-1)
        
        # Project to fusion dimension
        if fused_features.shape[-1] != self.config.fusion_dim:
            projection = nn.Linear(fused_features.shape[-1], self.config.fusion_dim)
            fused_features = projection(fused_features)
        
        return fused_features
    
    def _late_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Late fusion strategy"""
        # Process each modality separately
        processed_features = []
        
        for modality, features in modality_features.items():
            # Project to fusion dimension
            projection = nn.Linear(features.shape[-1], self.config.fusion_dim)
            processed_features.append(projection(features))
        
        # Concatenate processed features
        fused_features = torch.cat(processed_features, dim=-1)
        
        return fused_features
    
    def _intermediate_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Intermediate fusion strategy"""
        # Process modalities with shared layers
        shared_layer = nn.Linear(self.config.fusion_dim, self.config.fusion_dim)
        
        processed_features = []
        for modality, features in modality_features.items():
            # Project to fusion dimension
            projection = nn.Linear(features.shape[-1], self.config.fusion_dim)
            projected = projection(features)
            
            # Apply shared processing
            processed = shared_layer(projected)
            processed_features.append(processed)
        
        # Average the processed features
        fused_features = torch.stack(processed_features, dim=0).mean(dim=0)
        
        return fused_features
    
    def _attention_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Attention-based fusion strategy"""
        # Create attention weights for each modality
        modality_names = list(modality_features.keys())
        modality_features_list = list(modality_features.values())
        
        # Project all features to same dimension
        projected_features = []
        for features in modality_features_list:
            projection = nn.Linear(features.shape[-1], self.config.fusion_dim)
            projected_features.append(projection(features))
        
        # Stack features for attention
        stacked_features = torch.stack(projected_features, dim=1)  # [batch, num_modalities, fusion_dim]
        
        # Apply self-attention
        attention_layer = nn.MultiheadAttention(
            embed_dim=self.config.fusion_dim,
            num_heads=self.config.num_attention_heads,
            dropout=self.config.attention_dropout,
            batch_first=True
        )
        
        attended_features, attention_weights = attention_layer(
            stacked_features, stacked_features, stacked_features
        )
        
        # Average across modalities
        fused_features = attended_features.mean(dim=1)
        
        return fused_features
    
    def _cross_modal_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Cross-modal fusion strategy"""
        modality_names = list(modality_features.keys())
        modality_features_list = list(modality_features.values())
        
        # Project all features to same dimension
        projected_features = []
        for features in modality_features_list:
            projection = nn.Linear(features.shape[-1], self.config.fusion_dim)
            projected_features.append(projection(features))
        
        # Apply cross-modal attention between all pairs
        cross_modal_features = []
        
        for i, query_features in enumerate(projected_features):
            for j, (key_features, value_features) in enumerate(zip(projected_features, projected_features)):
                if i != j:  # Cross-modal attention
                    attended_output, _ = self.cross_modal_attention.apply_cross_modal_attention(
                        query_features.unsqueeze(1), 
                        key_features.unsqueeze(1), 
                        value_features.unsqueeze(1)
                    )
                    cross_modal_features.append(attended_output.squeeze(1))
        
        # Concatenate cross-modal features
        fused_features = torch.cat(cross_modal_features, dim=-1)
        
        # Project to final fusion dimension
        final_projection = nn.Linear(fused_features.shape[-1], self.config.fusion_dim)
        fused_features = final_projection(fused_features)
        
        return fused_features
    
    def _hierarchical_fusion(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """Hierarchical fusion strategy"""
        # First level: pairwise fusion
        modality_names = list(modality_features.keys())
        pairwise_features = []
        
        for i in range(len(modality_names)):
            for j in range(i + 1, len(modality_names)):
                mod1, mod2 = modality_names[i], modality_names[j]
                feat1, feat2 = modality_features[mod1], modality_features[mod2]
                
                # Concatenate pairwise features
                pairwise_feat = torch.cat([feat1, feat2], dim=-1)
                
                # Project to fusion dimension
                projection = nn.Linear(pairwise_feat.shape[-1], self.config.fusion_dim)
                pairwise_features.append(projection(pairwise_feat))
        
        # Second level: fuse all pairwise features
        if pairwise_features:
            hierarchical_features = torch.stack(pairwise_features, dim=0).mean(dim=0)
        else:
            # Fallback to early fusion
            hierarchical_features = self._early_fusion(modality_features)
        
        return hierarchical_features

class MultiModalAI:
    """Main multi-modal AI system"""
    
    def __init__(self, config: MultiModalConfig):
        self.config = config
        
        # Modality processors
        self.vision_processor = VisionProcessor(config)
        self.audio_processor = AudioProcessor(config)
        self.text_processor = TextProcessor(config)
        
        # Fusion engine
        self.fusion_engine = FusionEngine(config)
        
        # Multi-modal state
        self.multimodal_history = []
        
        logger.info("‚úÖ Multi-Modal AI System initialized")
    
    def process_multimodal_data(self, data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """Process multi-modal data"""
        logger.info("üöÄ Processing multi-modal data")
        
        processing_results = {
            'start_time': time.time(),
            'config': self.config,
            'modality_features': {},
            'fusion_result': None
        }
        
        # Process each modality
        modality_features = {}
        
        for modality in self.config.modalities:
            if modality.value in data:
                if modality == ModalityType.VISION:
                    features = self.vision_processor.process_vision(data[modality.value])
                    modality_features['vision'] = features
                elif modality == ModalityType.AUDIO:
                    features = self.audio_processor.process_audio(data[modality.value])
                    modality_features['audio'] = features
                elif modality == ModalityType.TEXT:
                    features = self.text_processor.process_text(data[modality.value])
                    modality_features['text'] = features
        
        processing_results['modality_features'] = modality_features
        
        # Fuse modalities
        if modality_features:
            fused_features = self.fusion_engine.fuse_modalities(modality_features)
            processing_results['fusion_result'] = fused_features
        
        # Final evaluation
        processing_results['end_time'] = time.time()
        processing_results['total_duration'] = processing_results['end_time'] - processing_results['start_time']
        
        # Store results
        self.multimodal_history.append(processing_results)
        
        logger.info("‚úÖ Multi-modal data processing completed")
        return processing_results
    
    def generate_multimodal_report(self, results: Dict[str, Any]) -> str:
        """Generate multi-modal AI report"""
        report = []
        report.append("=" * 50)
        report.append("MULTI-MODAL AI REPORT")
        report.append("=" * 50)
        
        # Configuration
        report.append("\nMULTI-MODAL CONFIGURATION:")
        report.append("-" * 28)
        report.append(f"Modalities: {[m.value for m in self.config.modalities]}")
        report.append(f"Fusion Strategy: {self.config.fusion_strategy.value}")
        report.append(f"Attention Type: {self.config.attention_type.value}")
        report.append(f"Vision Input Size: {self.config.vision_input_size}")
        report.append(f"Vision Backbone: {self.config.vision_backbone}")
        report.append(f"Vision Feature Dim: {self.config.vision_feature_dim}")
        report.append(f"Audio Sample Rate: {self.config.audio_sample_rate}")
        report.append(f"Audio Window Size: {self.config.audio_window_size}")
        report.append(f"Audio Hop Size: {self.config.audio_hop_size}")
        report.append(f"Audio Feature Dim: {self.config.audio_feature_dim}")
        report.append(f"Text Vocab Size: {self.config.text_vocab_size}")
        report.append(f"Text Max Length: {self.config.text_max_length}")
        report.append(f"Text Embedding Dim: {self.config.text_embedding_dim}")
        report.append(f"Fusion Dim: {self.config.fusion_dim}")
        report.append(f"Number of Attention Heads: {self.config.num_attention_heads}")
        report.append(f"Attention Dropout: {self.config.attention_dropout}")
        report.append(f"Learning Rate: {self.config.learning_rate}")
        report.append(f"Batch Size: {self.config.batch_size}")
        report.append(f"Number of Epochs: {self.config.num_epochs}")
        report.append(f"Modality Dropout: {'Enabled' if self.config.enable_modality_dropout else 'Disabled'}")
        report.append(f"Modality Dropout Rate: {self.config.modality_dropout_rate}")
        report.append(f"Contrastive Learning: {'Enabled' if self.config.enable_contrastive_learning else 'Disabled'}")
        report.append(f"Cross-Modal Transfer: {'Enabled' if self.config.enable_cross_modal_transfer else 'Disabled'}")
        report.append(f"Multimodal Augmentation: {'Enabled' if self.config.enable_multimodal_augmentation else 'Disabled'}")
        
        # Results
        report.append("\nMULTI-MODAL AI RESULTS:")
        report.append("-" * 25)
        report.append(f"Total Duration: {results.get('total_duration', 0):.2f} seconds")
        report.append(f"Start Time: {results.get('start_time', 'Unknown')}")
        report.append(f"End Time: {results.get('end_time', 'Unknown')}")
        
        # Modality features
        if 'modality_features' in results:
            report.append(f"\nMODALITY FEATURES:")
            report.append("-" * 18)
            for modality, features in results['modality_features'].items():
                report.append(f"  {modality}: {features.shape}")
        
        # Fusion result
        if 'fusion_result' in results and results['fusion_result'] is not None:
            report.append(f"\nFUSION RESULT:")
            report.append("-" * 14)
            report.append(f"  Shape: {results['fusion_result'].shape}")
            report.append(f"  Dimension: {results['fusion_result'].shape[-1]}")
        
        return "\n".join(report)
    
    def visualize_multimodal_results(self, save_path: str = None):
        """Visualize multi-modal AI results"""
        if not self.multimodal_history:
            logger.warning("No multi-modal AI history to visualize")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Plot 1: Modality feature dimensions
        modality_dims = []
        modality_names = []
        
        if self.multimodal_history:
            latest_result = self.multimodal_history[-1]
            if 'modality_features' in latest_result:
                for modality, features in latest_result['modality_features'].items():
                    modality_names.append(modality)
                    modality_dims.append(features.shape[-1])
        
        if modality_dims and modality_names:
            axes[0, 0].bar(modality_names, modality_dims, color=['blue', 'green', 'red'])
            axes[0, 0].set_ylabel('Feature Dimension')
            axes[0, 0].set_title('Modality Feature Dimensions')
        
        # Plot 2: Processing duration over time
        durations = [r.get('total_duration', 0) for r in self.multimodal_history]
        axes[0, 1].plot(durations, 'b-', linewidth=2)
        axes[0, 1].set_xlabel('Processing Run')
        axes[0, 1].set_ylabel('Duration (seconds)')
        axes[0, 1].set_title('Processing Duration Over Time')
        axes[0, 1].grid(True)
        
        # Plot 3: Fusion strategy distribution
        fusion_strategies = [self.config.fusion_strategy.value]
        strategy_counts = [1]
        
        axes[1, 0].pie(strategy_counts, labels=fusion_strategies, autopct='%1.1f%%')
        axes[1, 0].set_title('Fusion Strategy')
        
        # Plot 4: Multi-modal configuration
        config_values = [
            len(self.config.modalities),
            self.config.fusion_dim,
            self.config.num_attention_heads,
            self.config.batch_size
        ]
        config_labels = ['Modalities', 'Fusion Dim', 'Attention Heads', 'Batch Size']
        
        axes[1, 1].bar(config_labels, config_values, color=['blue', 'green', 'orange', 'red'])
        axes[1, 1].set_ylabel('Value')
        axes[1, 1].set_title('Multi-Modal Configuration')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        else:
            plt.show()
        
        plt.close()

# Factory functions
def create_multimodal_config(**kwargs) -> MultiModalConfig:
    """Create multi-modal configuration"""
    return MultiModalConfig(**kwargs)

def create_vision_processor(config: MultiModalConfig) -> VisionProcessor:
    """Create vision processor"""
    return VisionProcessor(config)

def create_audio_processor(config: MultiModalConfig) -> AudioProcessor:
    """Create audio processor"""
    return AudioProcessor(config)

def create_text_processor(config: MultiModalConfig) -> TextProcessor:
    """Create text processor"""
    return TextProcessor(config)

def create_cross_modal_attention(config: MultiModalConfig) -> CrossModalAttention:
    """Create cross-modal attention"""
    return CrossModalAttention(config)

def create_fusion_engine(config: MultiModalConfig) -> FusionEngine:
    """Create fusion engine"""
    return FusionEngine(config)

def create_multimodal_ai(config: MultiModalConfig) -> MultiModalAI:
    """Create multi-modal AI system"""
    return MultiModalAI(config)

# Example usage
def example_multimodal_ai():
    """Example of multi-modal AI system"""
    # Create configuration
    config = create_multimodal_config(
        modalities=[ModalityType.VISION, ModalityType.AUDIO, ModalityType.TEXT],
        fusion_strategy=FusionStrategy.ATTENTION_FUSION,
        attention_type=AttentionType.MULTI_HEAD_ATTENTION,
        vision_input_size=(224, 224, 3),
        vision_backbone="resnet50",
        vision_feature_dim=2048,
        audio_sample_rate=16000,
        audio_window_size=1024,
        audio_hop_size=512,
        audio_feature_dim=512,
        text_vocab_size=30000,
        text_max_length=512,
        text_embedding_dim=768,
        fusion_dim=1024,
        num_attention_heads=8,
        attention_dropout=0.1,
        learning_rate=0.001,
        batch_size=32,
        num_epochs=100,
        enable_modality_dropout=True,
        modality_dropout_rate=0.1,
        enable_contrastive_learning=True,
        enable_cross_modal_transfer=True,
        enable_multimodal_augmentation=True
    )
    
    # Create multi-modal AI system
    multimodal_ai = create_multimodal_ai(config)
    
    # Create dummy multi-modal data
    batch_size = 4
    
    vision_data = torch.randn(batch_size, 3, 224, 224)
    audio_data = torch.randn(batch_size, 16000)
    text_data = torch.randint(0, config.text_vocab_size, (batch_size, config.text_max_length))
    
    multimodal_data = {
        'vision': vision_data,
        'audio': audio_data,
        'text': text_data
    }
    
    # Process multi-modal data
    multimodal_results = multimodal_ai.process_multimodal_data(multimodal_data)
    
    # Generate report
    multimodal_report = multimodal_ai.generate_multimodal_report(multimodal_results)
    
    print(f"‚úÖ Multi-Modal AI Example Complete!")
    print(f"üöÄ Multi-Modal AI Statistics:")
    print(f"   Modalities: {[m.value for m in config.modalities]}")
    print(f"   Fusion Strategy: {config.fusion_strategy.value}")
    print(f"   Attention Type: {config.attention_type.value}")
    print(f"   Vision Input Size: {config.vision_input_size}")
    print(f"   Vision Backbone: {config.vision_backbone}")
    print(f"   Vision Feature Dim: {config.vision_feature_dim}")
    print(f"   Audio Sample Rate: {config.audio_sample_rate}")
    print(f"   Audio Window Size: {config.audio_window_size}")
    print(f"   Audio Hop Size: {config.audio_hop_size}")
    print(f"   Audio Feature Dim: {config.audio_feature_dim}")
    print(f"   Text Vocab Size: {config.text_vocab_size}")
    print(f"   Text Max Length: {config.text_max_length}")
    print(f"   Text Embedding Dim: {config.text_embedding_dim}")
    print(f"   Fusion Dim: {config.fusion_dim}")
    print(f"   Number of Attention Heads: {config.num_attention_heads}")
    print(f"   Attention Dropout: {config.attention_dropout}")
    print(f"   Learning Rate: {config.learning_rate}")
    print(f"   Batch Size: {config.batch_size}")
    print(f"   Number of Epochs: {config.num_epochs}")
    print(f"   Modality Dropout: {'Enabled' if config.enable_modality_dropout else 'Disabled'}")
    print(f"   Modality Dropout Rate: {config.modality_dropout_rate}")
    print(f"   Contrastive Learning: {'Enabled' if config.enable_contrastive_learning else 'Disabled'}")
    print(f"   Cross-Modal Transfer: {'Enabled' if config.enable_cross_modal_transfer else 'Disabled'}")
    print(f"   Multimodal Augmentation: {'Enabled' if config.enable_multimodal_augmentation else 'Disabled'}")
    
    print(f"\nüìä Multi-Modal AI Results:")
    print(f"   Multi-Modal History Length: {len(multimodal_ai.multimodal_history)}")
    print(f"   Total Duration: {multimodal_results.get('total_duration', 0):.2f} seconds")
    
    # Show modality features
    if 'modality_features' in multimodal_results:
        print(f"   Modality Features:")
        for modality, features in multimodal_results['modality_features'].items():
            print(f"     {modality}: {features.shape}")
    
    # Show fusion result
    if 'fusion_result' in multimodal_results and multimodal_results['fusion_result'] is not None:
        print(f"   Fusion Result: {multimodal_results['fusion_result'].shape}")
    
    print(f"\nüìã Multi-Modal AI Report:")
    print(multimodal_report)
    
    return multimodal_ai

# Export utilities
__all__ = [
    'ModalityType',
    'FusionStrategy',
    'AttentionType',
    'MultiModalConfig',
    'VisionProcessor',
    'AudioProcessor',
    'TextProcessor',
    'CrossModalAttention',
    'FusionEngine',
    'MultiModalAI',
    'create_multimodal_config',
    'create_vision_processor',
    'create_audio_processor',
    'create_text_processor',
    'create_cross_modal_attention',
    'create_fusion_engine',
    'create_multimodal_ai',
    'example_multimodal_ai'
]

if __name__ == "__main__":
    example_multimodal_ai()
    print("‚úÖ Multi-modal AI example completed successfully!")