# ðŸš€ API Performance Monitoring Guide - Video-OpusClip

## Overview

This comprehensive guide demonstrates how to implement and use API performance monitoring for the Video-OpusClip system, with a focus on the three critical metrics:

1. **Response Time** - Total time from request to response
2. **Latency** - Network and processing overhead time
3. **Throughput** - Requests processed per second

## ðŸŽ¯ Key Performance Metrics

### 1. Response Time
- **Definition**: Total time from when a request is received until the response is sent
- **Measurement**: High-precision timing using `time.time()` or `time.perf_counter()`
- **Target**: < 1000ms for most operations, < 500ms for critical paths
- **Monitoring**: Real-time tracking with percentiles (P50, P95, P99)

### 2. Latency
- **Definition**: Network transmission time + processing overhead
- **Measurement**: Estimated as ~80% of response time (network + processing)
- **Target**: < 500ms for optimal user experience
- **Monitoring**: Separate tracking from response time

### 3. Throughput
- **Definition**: Number of requests processed per second (RPS)
- **Measurement**: Rolling window calculation (default 60 seconds)
- **Target**: > 10 RPS for video processing endpoints
- **Monitoring**: Real-time calculation with trend analysis

## ðŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚    â”‚ Performance     â”‚    â”‚ Metrics Storage â”‚
â”‚                 â”‚    â”‚ Monitor         â”‚    â”‚                 â”‚
â”‚  - Middleware   â”‚â”€â”€â”€â–¶â”‚  - Response     â”‚â”€â”€â”€â–¶â”‚  - History      â”‚
â”‚  - Endpoints    â”‚    â”‚    Time         â”‚    â”‚  - Real-time    â”‚
â”‚  - Decorators   â”‚    â”‚  - Latency      â”‚    â”‚  - Alerts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  - Throughput   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  - Resources    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Monitoring Flow

1. **Request Reception** - Start timing
2. **Processing** - Track resource usage
3. **Response** - End timing and calculate metrics
4. **Storage** - Store metrics in history
5. **Analysis** - Calculate statistics and trends
6. **Alerting** - Check thresholds and trigger alerts

## ðŸ”§ Implementation Guide

### 1. Basic Setup

```python
from api_performance_monitor import APIPerformanceMonitor, get_performance_monitor

# Initialize performance monitor
monitor = APIPerformanceMonitor({
    "enable_high_precision": True,
    "enable_gpu_monitoring": True,
    "enable_distributed_tracing": False
})

# Start monitoring
await monitor.start_monitoring()
```

### 2. FastAPI Integration

```python
from fastapi import FastAPI
from api_performance_monitor import get_performance_monitor

app = FastAPI()

# Add performance monitoring middleware
monitor = get_performance_monitor()
app.middleware("http")(monitor.create_middleware())

# Start monitoring on app startup
@app.on_event("startup")
async def startup_event():
    await monitor.start_monitoring()

@app.on_event("shutdown")
async def shutdown_event():
    await monitor.stop_monitoring()
```

### 3. Endpoint Monitoring

```python
from api_performance_monitor import get_performance_monitor

monitor = get_performance_monitor()

@app.post("/video/process")
@monitor.monitor_performance("video_processing")
async def process_video(request: VideoProcessingRequest):
    """Process video with performance monitoring."""
    
    # Your video processing logic here
    result = await process_video_logic(request)
    
    return {"success": True, "result": result}
```

### 4. Custom Performance Tracking

```python
# Using context managers
with monitor.performance_context("custom_operation"):
    # Your custom operation
    result = perform_custom_operation()

# Using async context managers
async with monitor.async_performance_context("async_operation"):
    # Your async operation
    result = await perform_async_operation()

# Manual tracking
start_time = time.time()
try:
    result = perform_operation()
    end_time = time.time()
    monitor.track_custom_operation("operation_name", start_time, end_time, success=True)
except Exception as e:
    end_time = time.time()
    monitor.track_custom_operation("operation_name", start_time, end_time, success=False, error=str(e))
    raise
```

## ðŸ“Š Performance Metrics Collection

### 1. Response Time Tracking

```python
# High-precision timing
import time

def track_response_time():
    start_time = time.perf_counter()  # High precision
    
    # Process request
    result = process_request()
    
    end_time = time.perf_counter()
    response_time_ms = (end_time - start_time) * 1000
    
    return result, response_time_ms

# Middleware integration
async def performance_middleware(request: Request, call_next):
    start_time = time.perf_counter()
    
    try:
        response = await call_next(request)
        end_time = time.perf_counter()
        
        # Track successful request
        monitor.track_request(request, response, start_time, end_time)
        
        return response
    except Exception as e:
        end_time = time.perf_counter()
        
        # Track failed request
        error_response = JSONResponse(status_code=500, content={"error": str(e)})
        monitor.track_request(request, error_response, start_time, end_time)
        
        raise
```

### 2. Latency Calculation

```python
def calculate_latency(response_time_ms: float) -> float:
    """Calculate latency from response time."""
    # Latency is typically 80% of response time
    # This is a rough estimate - in production, you'd measure actual network latency
    return response_time_ms * 0.8

def calculate_network_latency(request: Request, response: Response) -> float:
    """Calculate actual network latency if possible."""
    # This would require network-level monitoring
    # For now, we use estimation
    return response_time_ms * 0.6  # Assume 60% is network
```

### 3. Throughput Calculation

```python
def calculate_throughput(request_timestamps: List[datetime], window_seconds: int = 60) -> float:
    """Calculate throughput (requests per second) in a rolling window."""
    now = datetime.now()
    window_start = now - timedelta(seconds=window_seconds)
    
    # Count requests in window
    requests_in_window = sum(
        1 for timestamp in request_timestamps
        if timestamp >= window_start
    )
    
    # Calculate throughput
    throughput_rps = requests_in_window / window_seconds
    
    return throughput_rps

# Real-time throughput monitoring
class ThroughputMonitor:
    def __init__(self, window_seconds: int = 60):
        self.window_seconds = window_seconds
        self.request_timestamps = deque(maxlen=10000)
    
    def record_request(self):
        """Record a new request."""
        self.request_timestamps.append(datetime.now())
    
    def get_current_throughput(self) -> float:
        """Get current throughput."""
        return calculate_throughput(list(self.request_timestamps), self.window_seconds)
```

### 4. Resource Monitoring

```python
import psutil
import torch

def collect_resource_metrics() -> Dict[str, float]:
    """Collect system resource metrics."""
    metrics = {}
    
    # CPU usage
    metrics["cpu_percent"] = psutil.cpu_percent(interval=0.1)
    
    # Memory usage
    memory = psutil.virtual_memory()
    metrics["memory_percent"] = memory.percent
    metrics["memory_available_gb"] = memory.available / (1024**3)
    
    # GPU usage (if available)
    if torch.cuda.is_available():
        try:
            metrics["gpu_percent"] = torch.cuda.utilization()
            metrics["gpu_memory_percent"] = torch.cuda.memory_utilization()
        except Exception as e:
            logger.warning(f"GPU monitoring failed: {e}")
    
    # Disk usage
    disk = psutil.disk_usage('/')
    metrics["disk_percent"] = disk.percent
    
    # Network I/O
    network = psutil.net_io_counters()
    metrics["network_bytes_sent"] = network.bytes_sent
    metrics["network_bytes_recv"] = network.bytes_recv
    
    return metrics
```

## ðŸ“ˆ Performance Analysis

### 1. Statistical Analysis

```python
import statistics
from typing import List

def analyze_response_times(response_times: List[float]) -> Dict[str, float]:
    """Analyze response time statistics."""
    if not response_times:
        return {}
    
    return {
        "count": len(response_times),
        "mean": statistics.mean(response_times),
        "median": statistics.median(response_times),
        "p50": statistics.median(response_times),
        "p95": percentile(response_times, 95),
        "p99": percentile(response_times, 99),
        "min": min(response_times),
        "max": max(response_times),
        "std_dev": statistics.stdev(response_times) if len(response_times) > 1 else 0
    }

def percentile(data: List[float], percentile: int) -> float:
    """Calculate percentile of data."""
    if not data:
        return 0.0
    
    sorted_data = sorted(data)
    index = (percentile / 100) * (len(sorted_data) - 1)
    
    if index.is_integer():
        return sorted_data[int(index)]
    else:
        lower = sorted_data[int(index)]
        upper = sorted_data[int(index) + 1]
        return lower + (upper - lower) * (index - int(index))
```

### 2. Trend Analysis

```python
def analyze_performance_trends(metrics_history: List[PerformanceMetrics], 
                             time_window: timedelta) -> Dict[str, Any]:
    """Analyze performance trends over time."""
    cutoff_time = datetime.now() - time_window
    recent_metrics = [m for m in metrics_history if m.timestamp > cutoff_time]
    
    if not recent_metrics:
        return {}
    
    # Group by time intervals
    intervals = defaultdict(list)
    interval_size = time_window.total_seconds() / 10  # 10 intervals
    
    for metric in recent_metrics:
        interval_index = int((metric.timestamp - cutoff_time).total_seconds() / interval_size)
        intervals[interval_index].append(metric)
    
    # Calculate trends
    trends = {
        "response_time_trend": [],
        "latency_trend": [],
        "throughput_trend": [],
        "error_rate_trend": []
    }
    
    for interval in sorted(intervals.keys()):
        interval_metrics = intervals[interval]
        
        if interval_metrics:
            trends["response_time_trend"].append(statistics.mean([m.response_time_ms for m in interval_metrics]))
            trends["latency_trend"].append(statistics.mean([m.latency_ms for m in interval_metrics]))
            trends["throughput_trend"].append(statistics.mean([m.throughput_rps for m in interval_metrics if m.throughput_rps > 0]))
            trends["error_rate_trend"].append(statistics.mean([m.error_rate for m in interval_metrics]))
    
    return trends
```

### 3. Bottleneck Identification

```python
def identify_performance_bottlenecks(metrics: List[PerformanceMetrics]) -> List[Dict[str, Any]]:
    """Identify performance bottlenecks."""
    bottlenecks = []
    
    # Check response time bottlenecks
    response_times = [m.response_time_ms for m in metrics]
    avg_response_time = statistics.mean(response_times)
    
    if avg_response_time > 1000:  # > 1 second
        bottlenecks.append({
            "type": "HIGH_RESPONSE_TIME",
            "severity": "HIGH" if avg_response_time > 2000 else "MEDIUM",
            "description": f"Average response time is {avg_response_time:.2f}ms",
            "recommendation": "Optimize processing logic or increase resources"
        })
    
    # Check throughput bottlenecks
    throughputs = [m.throughput_rps for m in metrics if m.throughput_rps > 0]
    if throughputs:
        avg_throughput = statistics.mean(throughputs)
        if avg_throughput < 10:  # < 10 RPS
            bottlenecks.append({
                "type": "LOW_THROUGHPUT",
                "severity": "HIGH" if avg_throughput < 5 else "MEDIUM",
                "description": f"Average throughput is {avg_throughput:.2f} RPS",
                "recommendation": "Scale horizontally or optimize processing"
            })
    
    # Check resource bottlenecks
    cpu_usage = [m.cpu_usage_percent for m in metrics]
    if cpu_usage:
        avg_cpu = statistics.mean(cpu_usage)
        if avg_cpu > 80:
            bottlenecks.append({
                "type": "HIGH_CPU_USAGE",
                "severity": "HIGH" if avg_cpu > 90 else "MEDIUM",
                "description": f"Average CPU usage is {avg_cpu:.1f}%",
                "recommendation": "Optimize CPU-intensive operations or scale up"
            })
    
    memory_usage = [m.memory_usage_percent for m in metrics]
    if memory_usage:
        avg_memory = statistics.mean(memory_usage)
        if avg_memory > 85:
            bottlenecks.append({
                "type": "HIGH_MEMORY_USAGE",
                "severity": "HIGH" if avg_memory > 95 else "MEDIUM",
                "description": f"Average memory usage is {avg_memory:.1f}%",
                "recommendation": "Optimize memory usage or increase memory"
            })
    
    return bottlenecks
```

## ðŸš¨ Alerting System

### 1. Performance Thresholds

```python
@dataclass
class PerformanceThresholds:
    """Performance thresholds for alerting."""
    response_time_ms: float = 1000.0  # 1 second
    latency_ms: float = 500.0         # 500ms
    throughput_rps: float = 10.0      # 10 requests per second
    error_rate: float = 0.05          # 5% error rate
    cpu_usage_percent: float = 80.0   # 80% CPU usage
    memory_usage_percent: float = 85.0 # 85% memory usage
    gpu_usage_percent: float = 90.0   # 90% GPU usage

# Set custom thresholds
thresholds = PerformanceThresholds(
    response_time_ms=500.0,  # Stricter for video processing
    throughput_rps=5.0,      # Lower for heavy operations
    cpu_usage_percent=70.0   # More conservative
)

monitor.set_thresholds(thresholds)
```

### 2. Alert Callbacks

```python
def performance_alert_callback(alert: PerformanceAlert):
    """Handle performance alerts."""
    logger.warning(
        f"Performance alert: {alert.alert_type}",
        severity=alert.severity,
        message=alert.message,
        current_value=alert.current_value,
        threshold=alert.threshold
    )
    
    # Send to monitoring system (e.g., Prometheus, DataDog)
    if alert.severity == "CRITICAL":
        send_critical_alert(alert)
    else:
        send_warning_alert(alert)

# Register alert callback
monitor.add_alert_callback(performance_alert_callback)
```

### 3. Custom Alert Rules

```python
def check_custom_alerts(metrics: List[PerformanceMetrics]) -> List[PerformanceAlert]:
    """Check custom alert conditions."""
    alerts = []
    
    # Check for sudden performance degradation
    if len(metrics) >= 10:
        recent_metrics = metrics[-10:]
        older_metrics = metrics[-20:-10]
        
        recent_avg = statistics.mean([m.response_time_ms for m in recent_metrics])
        older_avg = statistics.mean([m.response_time_ms for m in older_metrics])
        
        if recent_avg > older_avg * 2:  # 2x degradation
            alerts.append(PerformanceAlert(
                timestamp=datetime.now(),
                alert_type="PERFORMANCE_DEGRADATION",
                severity="HIGH",
                message=f"Response time increased from {older_avg:.2f}ms to {recent_avg:.2f}ms",
                metrics=recent_metrics[-1],
                threshold=older_avg * 2,
                current_value=recent_avg
            ))
    
    # Check for error rate spikes
    if len(metrics) >= 5:
        recent_metrics = metrics[-5:]
        error_rate = statistics.mean([m.error_rate for m in recent_metrics])
        
        if error_rate > 0.1:  # 10% error rate
            alerts.append(PerformanceAlert(
                timestamp=datetime.now(),
                alert_type="HIGH_ERROR_RATE",
                severity="CRITICAL",
                message=f"Error rate is {error_rate:.1%}",
                metrics=recent_metrics[-1],
                threshold=0.1,
                current_value=error_rate
            ))
    
    return alerts
```

## ðŸ“Š Real-time Monitoring

### 1. Live Dashboard

```python
@app.get("/metrics/live")
async def get_live_metrics():
    """Get live performance metrics."""
    monitor = get_performance_monitor()
    
    current_metrics = monitor.get_current_metrics()
    summary = monitor.get_performance_summary(timedelta(minutes=5))
    recent_alerts = monitor.get_alerts(timedelta(minutes=5))
    
    return {
        "timestamp": datetime.now().isoformat(),
        "current_metrics": current_metrics.__dict__ if current_metrics else None,
        "recent_summary": summary,
        "recent_alerts": [alert.__dict__ for alert in recent_alerts],
        "system_resources": {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "gpu_percent": torch.cuda.utilization() if torch.cuda.is_available() else None
        }
    }
```

### 2. Performance Endpoints

```python
@app.get("/metrics/response-time")
async def get_response_time_metrics(time_window: str = "1h"):
    """Get response time metrics."""
    monitor = get_performance_monitor()
    
    # Parse time window
    window_map = {
        "1h": timedelta(hours=1),
        "6h": timedelta(hours=6),
        "24h": timedelta(hours=24),
        "7d": timedelta(days=7)
    }
    
    window = window_map.get(time_window, timedelta(hours=1))
    metrics = list(monitor.metrics_history)
    
    # Filter by time window
    cutoff_time = datetime.now() - window
    filtered_metrics = [m for m in metrics if m.timestamp > cutoff_time]
    
    if not filtered_metrics:
        return {"message": "No metrics available for the specified time window"}
    
    response_times = [m.response_time_ms for m in filtered_metrics]
    
    return {
        "time_window": time_window,
        "total_requests": len(filtered_metrics),
        "response_time": {
            "mean": statistics.mean(response_times),
            "median": statistics.median(response_times),
            "p95": percentile(response_times, 95),
            "p99": percentile(response_times, 99),
            "min": min(response_times),
            "max": max(response_times)
        }
    }

@app.get("/metrics/throughput")
async def get_throughput_metrics():
    """Get throughput metrics."""
    monitor = get_performance_monitor()
    
    # Get current throughput
    current_throughput = monitor.get_current_metrics().throughput_rps if monitor.get_current_metrics() else 0
    
    # Get historical throughput
    metrics = list(monitor.metrics_history)
    throughputs = [m.throughput_rps for m in metrics if m.throughput_rps > 0]
    
    return {
        "current_throughput_rps": current_throughput,
        "historical_throughput": {
            "mean": statistics.mean(throughputs) if throughputs else 0,
            "max": max(throughputs) if throughputs else 0,
            "min": min(throughputs) if throughputs else 0
        },
        "total_requests": len(metrics)
    }

@app.get("/metrics/latency")
async def get_latency_metrics():
    """Get latency metrics."""
    monitor = get_performance_monitor()
    
    metrics = list(monitor.metrics_history)
    latencies = [m.latency_ms for m in metrics]
    
    return {
        "latency": {
            "mean": statistics.mean(latencies) if latencies else 0,
            "median": statistics.median(latencies) if latencies else 0,
            "p95": percentile(latencies, 95) if latencies else 0,
            "p99": percentile(latencies, 99) if latencies else 0
        },
        "total_requests": len(metrics)
    }
```

## ðŸ”§ Performance Optimization

### 1. Response Time Optimization

```python
# Use async/await for I/O operations
@app.post("/video/process/optimized")
async def process_video_optimized(request: VideoProcessingRequest):
    """Optimized video processing with performance monitoring."""
    
    # Parallel processing
    with monitor.performance_context("video_processing"):
        # Process video frames in parallel
        frame_tasks = [
            process_frame_async(frame) 
            for frame in request.frames
        ]
        
        processed_frames = await asyncio.gather(*frame_tasks)
    
    return {"success": True, "frames": processed_frames}

# Use connection pooling
async def get_optimized_database():
    """Get database with connection pooling."""
    container = get_dependency_container()
    async with container.db_manager.get_session() as session:
        yield session

# Use caching
@monitor.monitor_performance("cached_operation")
async def cached_operation(key: str):
    """Cached operation for better performance."""
    cache = await get_cache_client()
    
    # Check cache first
    cached_result = await cache.get(key)
    if cached_result:
        return cached_result
    
    # Perform operation
    result = await perform_expensive_operation()
    
    # Cache result
    await cache.set(key, result, ttl=300)
    
    return result
```

### 2. Throughput Optimization

```python
# Batch processing
@app.post("/video/process/batch")
async def process_video_batch(requests: List[VideoProcessingRequest]):
    """Batch video processing for higher throughput."""
    
    # Process in batches
    batch_size = 10
    results = []
    
    for i in range(0, len(requests), batch_size):
        batch = requests[i:i + batch_size]
        
        with monitor.performance_context(f"batch_processing_{i//batch_size}"):
            batch_results = await asyncio.gather(*[
                process_video_async(req) for req in batch
            ])
            results.extend(batch_results)
    
    return {"success": True, "results": results}

# Load balancing
@app.post("/video/process/load_balanced")
async def process_video_load_balanced(request: VideoProcessingRequest):
    """Load-balanced video processing."""
    
    # Get available workers
    workers = await get_available_workers()
    
    # Select least loaded worker
    selected_worker = min(workers, key=lambda w: w.current_load)
    
    # Process on selected worker
    result = await selected_worker.process_video(request)
    
    return result
```

### 3. Latency Optimization

```python
# Pre-warming models
async def pre_warm_models():
    """Pre-warm models to reduce latency."""
    monitor = get_performance_monitor()
    
    with monitor.performance_context("model_pre_warming"):
        models = ["video_processor", "caption_generator", "quality_analyzer"]
        
        for model_name in models:
            # Load model in background
            asyncio.create_task(load_model_async(model_name))

# Connection pooling
class ConnectionPool:
    def __init__(self, max_connections: int = 20):
        self.max_connections = max_connections
        self.connections = []
        self.lock = asyncio.Lock()
    
    async def get_connection(self):
        """Get connection from pool."""
        async with self.lock:
            if self.connections:
                return self.connections.pop()
            else:
                return await create_new_connection()
    
    async def return_connection(self, connection):
        """Return connection to pool."""
        async with self.lock:
            if len(self.connections) < self.max_connections:
                self.connections.append(connection)
            else:
                await connection.close()

# Use connection pool
connection_pool = ConnectionPool()

async def optimized_database_operation():
    """Optimized database operation with connection pooling."""
    connection = await connection_pool.get_connection()
    
    try:
        result = await connection.execute("SELECT * FROM videos")
        return result
    finally:
        await connection_pool.return_connection(connection)
```

## ðŸ“Š Reporting and Analytics

### 1. Performance Reports

```python
@app.get("/reports/performance")
async def generate_performance_report(
    start_time: datetime,
    end_time: datetime,
    format: str = "json"
):
    """Generate performance report for specified time period."""
    monitor = get_performance_monitor()
    
    # Filter metrics by time period
    metrics = [
        m for m in monitor.metrics_history
        if start_time <= m.timestamp <= end_time
    ]
    
    if not metrics:
        return {"message": "No metrics available for the specified time period"}
    
    # Generate report
    report = {
        "period": {
            "start": start_time.isoformat(),
            "end": end_time.isoformat()
        },
        "summary": {
            "total_requests": len(metrics),
            "successful_requests": len([m for m in metrics if m.error_rate == 0]),
            "failed_requests": len([m for m in metrics if m.error_rate > 0])
        },
        "performance": {
            "response_time": analyze_response_times([m.response_time_ms for m in metrics]),
            "latency": analyze_response_times([m.latency_ms for m in metrics]),
            "throughput": {
                "average": statistics.mean([m.throughput_rps for m in metrics if m.throughput_rps > 0]),
                "peak": max([m.throughput_rps for m in metrics if m.throughput_rps > 0])
            }
        },
        "resources": {
            "cpu": {
                "average": statistics.mean([m.cpu_usage_percent for m in metrics]),
                "peak": max([m.cpu_usage_percent for m in metrics])
            },
            "memory": {
                "average": statistics.mean([m.memory_usage_percent for m in metrics]),
                "peak": max([m.memory_usage_percent for m in metrics])
            }
        },
        "bottlenecks": identify_performance_bottlenecks(metrics)
    }
    
    if format.lower() == "csv":
        return export_to_csv(report)
    else:
        return report
```

### 2. Trend Analysis

```python
@app.get("/analytics/trends")
async def analyze_performance_trends(time_window: str = "24h"):
    """Analyze performance trends."""
    monitor = get_performance_monitor()
    
    # Parse time window
    window_map = {
        "1h": timedelta(hours=1),
        "6h": timedelta(hours=6),
        "24h": timedelta(hours=24),
        "7d": timedelta(days=7)
    }
    
    window = window_map.get(time_window, timedelta(hours=24))
    trends = analyze_performance_trends(list(monitor.metrics_history), window)
    
    return {
        "time_window": time_window,
        "trends": trends,
        "insights": generate_trend_insights(trends)
    }

def generate_trend_insights(trends: Dict[str, List[float]]) -> List[str]:
    """Generate insights from trend data."""
    insights = []
    
    # Analyze response time trend
    if trends.get("response_time_trend"):
        response_times = trends["response_time_trend"]
        if len(response_times) >= 2:
            trend = response_times[-1] - response_times[0]
            if trend > 100:
                insights.append("Response time is increasing significantly")
            elif trend < -100:
                insights.append("Response time is improving")
    
    # Analyze throughput trend
    if trends.get("throughput_trend"):
        throughputs = trends["throughput_trend"]
        if len(throughputs) >= 2:
            trend = throughputs[-1] - throughputs[0]
            if trend < -2:
                insights.append("Throughput is decreasing")
            elif trend > 2:
                insights.append("Throughput is improving")
    
    return insights
```

## ðŸŽ¯ Best Practices

### 1. Monitoring Strategy

- **Set Realistic Thresholds** - Base thresholds on actual performance requirements
- **Monitor Key Metrics** - Focus on response time, latency, and throughput
- **Use Percentiles** - P95 and P99 are more important than averages
- **Track Trends** - Monitor performance over time, not just current values
- **Correlate Metrics** - Understand relationships between different metrics

### 2. Performance Optimization

- **Profile First** - Identify bottlenecks before optimizing
- **Optimize Critical Paths** - Focus on the most important operations
- **Use Caching** - Cache frequently accessed data
- **Implement Connection Pooling** - Reuse database and network connections
- **Use Async Operations** - Avoid blocking operations
- **Scale Horizontally** - Add more instances for better throughput

### 3. Alerting Strategy

- **Avoid Alert Fatigue** - Set meaningful thresholds
- **Use Different Severities** - Distinguish between warnings and critical alerts
- **Include Context** - Provide actionable information in alerts
- **Test Alerts** - Ensure alerts work correctly
- **Review and Adjust** - Regularly review and adjust thresholds

### 4. Data Management

- **Retention Policy** - Define how long to keep metrics data
- **Data Compression** - Compress old metrics data
- **Regular Cleanup** - Remove old data to prevent storage issues
- **Backup Strategy** - Backup important metrics data
- **Data Export** - Provide ways to export metrics for analysis

## ðŸ“‹ Summary

This comprehensive API performance monitoring system provides:

1. **Real-time Monitoring** - Live tracking of response time, latency, and throughput
2. **Statistical Analysis** - Detailed analysis with percentiles and trends
3. **Alerting System** - Configurable alerts for performance issues
4. **Resource Monitoring** - CPU, memory, and GPU usage tracking
5. **Bottleneck Identification** - Automatic detection of performance issues
6. **Optimization Tools** - Tools and patterns for performance improvement
7. **Reporting** - Comprehensive performance reports and analytics

The system prioritizes the three critical metrics (response time, latency, throughput) while providing comprehensive monitoring and optimization capabilities for the Video-OpusClip system. 