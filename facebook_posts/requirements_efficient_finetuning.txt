# Efficient Fine-tuning System Requirements
# Core deep learning and transformers dependencies

# PyTorch ecosystem
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Transformers and Hugging Face
transformers>=4.30.0
tokenizers>=0.13.0
datasets>=2.12.0
accelerate>=0.20.0
peft>=0.4.0  # Parameter-Efficient Fine-Tuning library

# Efficient fine-tuning libraries
loralib>=0.1.1
adapterhub>=3.2.0

# Scientific computing
numpy>=1.24.0
scipy>=1.10.0
pandas>=2.0.0

# Visualization and monitoring
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.14.0
tensorboard>=2.13.0
wandb>=0.15.0

# Progress bars and utilities
tqdm>=4.65.0
rich>=13.3.0

# Configuration and serialization
PyYAML>=6.0
omegaconf>=2.3.0
hydra-core>=1.3.0

# Image processing (for multimodal models)
Pillow>=9.5.0
opencv-python>=4.7.0

# Machine learning utilities
scikit-learn>=1.2.0
einops>=0.6.0
timm>=0.8.0  # PyTorch Image Models

# Performance and optimization
flash-attn>=2.0.0  # Flash Attention (optional, requires specific setup)
xformers>=0.0.20  # Memory-efficient transformers (optional)
bitsandbytes>=0.39.0  # 8-bit optimizers and quantization

# Development and testing
pytest>=7.3.0
pytest-cov>=4.1.0
black>=23.3.0
isort>=5.12.0
flake8>=6.0.0
mypy>=1.3.0

# Jupyter and interactive development
jupyter>=1.0.0
ipywidgets>=8.0.0
notebook>=6.5.0

# Data processing
jsonlines>=3.1.0
pyarrow>=12.0.0

# Model serving and deployment (optional)
fastapi>=0.95.0
uvicorn>=0.22.0
gradio>=3.32.0

# Memory profiling and debugging
memory-profiler>=0.60.0
py-spy>=0.3.14

# Additional utilities
click>=8.1.0
requests>=2.31.0
filelock>=3.12.0
packaging>=23.1

# Optional: For specific model architectures
sentence-transformers>=2.2.0  # For embedding models
diffusers>=0.17.0  # For diffusion models






