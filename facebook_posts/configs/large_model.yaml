name: large_model
version: "1.0.0"
description: "Large model for production use"
created_at: "2024-01-01T00:00:00"
updated_at: "2024-01-01T00:00:00"

model:
  model_type: "transformer"
  input_dim: 768
  hidden_dim: 512
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  activation: "gelu"
  vocab_size: 50000
  max_seq_length: 512
  embedding_dim: 768
  layer_norm_eps: 1e-12
  attention_dropout: 0.1
  hidden_dropout: 0.1
  initializer_range: 0.02
  use_bias: true
  use_relative_position: false
  use_absolute_position: true
  position_embedding_type: "learned"

training:
  batch_size: 64
  learning_rate: 1e-4
  weight_decay: 1e-5
  num_epochs: 100
  warmup_steps: 1000
  max_grad_norm: 1.0
  optimizer: "adamw"
  scheduler: "cosine"
  momentum: 0.9
  beta1: 0.9
  beta2: 0.999
  epsilon: 1e-8
  lr_scheduler_type: "cosine"
  lr_warmup_ratio: 0.1
  lr_decay_ratio: 0.1
  min_lr: 1e-6
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  fp16: true
  bf16: false
  dataloader_pin_memory: true
  dataloader_num_workers: 8

data:
  dataset_name: "facebook_posts"
  dataset_size: 10000
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_length: 512
  truncation: true
  padding: "max_length"
  return_tensors: "pt"
  use_augmentation: true
  augmentation_prob: 0.5
  augmentation_methods:
    - "random_mask"
    - "synonym_replacement"
    - "back_translation"
  batch_size: 64
  shuffle: true
  num_workers: 8
  pin_memory: true
  drop_last: true
  cache_dir: "cache"
  use_cache: true
  cache_size: 5000

performance:
  use_mixed_precision: true
  dtype: "float16"
  scaler_enabled: true
  memory_efficient_attention: true
  gradient_checkpointing: true
  memory_pool: true
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4
  compile_model: true
  use_torch_compile: true
  optimize_for_inference: false
  use_distributed: false
  world_size: 1
  rank: 0
  backend: "nccl"

logging:
  log_level: "INFO"
  log_file: "logs/large_model_training.log"
  log_interval: 10
  use_tensorboard: true
  use_wandb: true
  tensorboard_dir: "runs/large_model"
  wandb_project: "facebook-posts-ai"
  wandb_entity: ""
  save_dir: "checkpoints/large_model"
  save_interval: 100
  save_best_only: true
  save_last: true
  max_checkpoints: 10
  eval_interval: 100
  log_metrics:
    - "loss"
    - "accuracy"
    - "learning_rate"
    - "gradient_norm"
    - "memory_usage"

system:
  device: "cuda"
  seed: 42
  deterministic: false
  base_dir: "."
  data_dir: "data"
  models_dir: "models"
  logs_dir: "logs"
  cache_dir: "cache"
  num_threads: 8
  use_multiprocessing: true
  max_workers: 8
  debug: false
  verbose: false
  profile: true

custom:
  experiment_type: "production"
  model_size: "large"
  expected_training_time: "2-4 hours"
  gpu_memory_required: "8GB+"
  optimization_level: "high" 