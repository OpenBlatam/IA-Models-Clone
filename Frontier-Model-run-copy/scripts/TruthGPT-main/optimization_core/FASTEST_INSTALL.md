# âš¡ INSTALACIÃ“N MÃS RÃPIDA - MÃXIMA VELOCIDAD

## ğŸš€ InstalaciÃ³n Ultra RÃ¡pida

### 1ï¸âƒ£ **Una LÃ­nea - Todo AutomÃ¡tico**
```bash
pip install -r requirements_fastest.txt
```

### 2ï¸âƒ£ **InstalaciÃ³n por Partes**

#### ğŸ”´ CrÃ­tico (MÃ­nimo para funcionar)
```bash
pip install torch>=2.1.0 transformers>=4.35.0 accelerate>=0.25.0
```

#### ğŸŸ  GPU (MÃ¡xima velocidad)
```bash
pip install flash-attn xformers triton
```

#### ğŸŸ¡ OptimizaciÃ³n
```bash
pip install peft>=0.6.0 deepspeed>=0.12.0 bitsandbytes>=0.41.0
```

## âš¡ Speedup Esperado

| OptimizaciÃ³n | Speedup | DescripciÃ³n |
|--------------|---------|-------------|
| Flash Attention | **3x** | AtenciÃ³n GPU optimizada |
| XFormers | **2x** | Operaciones eficientes |
| Mixed Precision | **2x** | FP16 entrenamiento |
| DeepSpeed | **2-4x** | Distributed training |
| **COMBINADO** | **10-20x** âš¡ | Todo junto |

## ğŸ“¦ LibrerÃ­as MÃ­nimas

Si necesitas solo lo esencial:
```bash
pip install torch transformers accelerate flash-attn wandb gradio
```

## ğŸ”§ OptimizaciÃ³n GPU

### Verificar GPU
```bash
python -c "import torch; print(torch.cuda.is_available())"
```

### Instalar Flash Attention
```bash
pip install flash-attn --no-build-isolation
```

### Optimizar XFormers
```bash
pip install xformers -U
```

## âœ… VerificaciÃ³n RÃ¡pida

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
print(f"GPUs: {torch.cuda.device_count()}")

import transformers
print(f"Transformers: {transformers.__version__}")

try:
    import flash_attn
    print("âœ… Flash Attention instalado")
except:
    print("âŒ Flash Attention no instalado")
```

## ğŸ¯ Para Desarrollo TruthGPT

### Solo lo esencial:
```bash
pip install torch transformers accelerate gradio tqdm
```

### Con optimizaciones:
```bash
pip install -r requirements_fastest.txt
```

## ğŸ“Š Tiempo de InstalaciÃ³n

- **MÃ­nimo**: 2-3 minutos
- **Recomendado**: 5-7 minutos
- **Completo**: 10-15 minutos

## ğŸš¨ Troubleshooting

### Flash Attention falla
```bash
pip install flash-attn==2.3.6 --no-build-isolation
```

### CUDA version mismatch
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### InstalaciÃ³n limpia
```bash
pip uninstall -y torch transformers
pip install -r requirements_fastest.txt
```

## âœ… Ready!

Una vez instalado, verifica con:
```bash
python -c "import torch; from flash_attn import flash_attn_func; print('âœ… Todo listo!')"
```

**Â¡Ahora eres 10-20x mÃ¡s rÃ¡pido!** âš¡ğŸš€

