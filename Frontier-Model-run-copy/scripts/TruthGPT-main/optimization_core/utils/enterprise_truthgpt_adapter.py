"""
Enterprise TruthGPT Adapter
Ultra-optimized adapter for TruthGPT with enterprise features
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Any, Tuple
import logging
from dataclasses import dataclass
from enum import Enum

class AdapterMode(Enum):
    """Adapter mode enum."""
    OPTIMIZATION = "optimization"
    TRAINING = "training"
    INFERENCE = "inference"
    ENTERPRISE = "enterprise"

@dataclass
class AdapterConfig:
    """Adapter configuration."""
    mode: AdapterMode = AdapterMode.ENTERPRISE
    attention_heads: int = 16
    hidden_size: int = 512
    num_layers: int = 12
    vocab_size: int = 50257
    max_position_embeddings: int = 2048
    dropout: float = 0.1
    activation_dropout: float = 0.0
    layer_norm_eps: float = 1e-5
    
    # Enterprise features
    use_flash_attention: bool = True
    use_gradient_checkpointing: bool = True
    use_mixed_precision: bool = True
    use_data_parallel: bool = True
    use_quantization: bool = True

class EnterpriseTruthGPTAdapter(nn.Module):
    """Enterprise TruthGPT adapter with advanced optimizations."""
    
    def __init__(self, config: AdapterConfig):
        super().__init__()
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Initialize components
        self.embedding = self._create_embedding()
        self.layers = nn.ModuleList([
            self._create_transformer_layer(i)
            for i in range(config.num_layers)
        ])
        self.output = self._create_output_layer()
        
        # Enterprise optimizations
        self._apply_enterprise_optimizations()
    
    def _create_embedding(self) -> nn.Module:
        """Create embedding layer."""
        return nn.Sequential(
            nn.Embedding(
                self.config.vocab_size,
                self.config.hidden_size,
                max_norm=1.0
            ),
            nn.Dropout(self.config.dropout)
        )
    
    def _create_transformer_layer(self, layer_idx: int) -> nn.Module:
        """Create transformer layer."""
        return nn.ModuleDict({
            'attention': self._create_attention_layer(),
            'feed_forward': self._create_feed_forward_layer(),
            'layer_norm1': nn.LayerNorm(
                self.config.hidden_size,
                eps=self.config.layer_norm_eps
            ),
            'layer_norm2': nn.LayerNorm(
                self.config.hidden_size,
                eps=self.config.layer_norm_eps
            ),
            'dropout1': nn.Dropout(self.config.dropout),
            'dropout2': nn.Dropout(self.config.activation_dropout)
        })
    
    def _create_attention_layer(self) -> nn.Module:
        """Create attention layer with enterprise optimizations."""
        if self.config.use_flash_attention:
            # Use Flash Attention for better performance
            return self._create_flash_attention()
        else:
            return self._create_standard_attention()
    
    def _create_flash_attention(self) -> nn.Module:
        """Create flash attention layer."""
        # Import flash attention if available
        try:
            from flash_attn import flash_attn_func
            return nn.Module()
        except ImportError:
            self.logger.warning("Flash Attention not available, using standard attention")
            return self._create_standard_attention()
    
    def _create_standard_attention(self) -> nn.Module:
        """Create standard attention layer."""
        return nn.MultiheadAttention(
            embed_dim=self.config.hidden_size,
            num_heads=self.config.attention_heads,
            dropout=self.config.dropout,
            batch_first=False
        )
    
    def _create_feed_forward_layer(self) -> nn.Module:
        """Create feed-forward layer."""
        return nn.Sequential(
            nn.Linear(
                self.config.hidden_size,
                self.config.hidden_size * 4
            ),
            nn.GELU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(
                self.config.hidden_size * 4,
                self.config.hidden_size
            )
        )
    
    def _create_output_layer(self) -> nn.Module:
        """Create output layer."""
        return nn.Sequential(
            nn.LayerNorm(
                self.config.hidden_size,
                eps=self.config.layer_norm_eps
            ),
            nn.Linear(
                self.config.hidden_size,
                self.config.vocab_size,
                bias=False
            )
        )
    
    def _apply_enterprise_optimizations(self):
        """Apply enterprise optimizations."""
        # Mixed precision
        if self.config.use_mixed_precision:
            self.half()
        
        # Data parallel
        if self.config.use_data_parallel and torch.cuda.device_count() > 1:
            self = nn.DataParallel(self)
        
        # Quantization
        if self.config.use_quantization:
            self = torch.quantization.quantize_dynamic(
                self,
                {nn.Linear},
                dtype=torch.qint8
            )
    
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass with enterprise optimizations."""
        # Get embeddings
        x = self.embedding(input_ids)
        
        # Apply transformer layers
        for layer in self.layers:
            # Self-attention
            residual = x
            x = layer['layer_norm1'](x)
            if self.config.use_flash_attention:
                # Flash attention implementation
                x, _ = layer['attention'](x, x, x)
            else:
                x, _ = layer['attention'](x, x, x)
            x = layer['dropout1'](x)
            x = x + residual
            
            # Feed-forward
            residual = x
            x = layer['layer_norm2'](x)
            x = layer['feed_forward'](x)
            x = layer['dropout2'](x)
            x = x + residual
        
        # Apply output layer
        x = self.output(x)
        
        return x
    
    def optimize_for_inference(self):
        """Optimize model for inference."""
        self.eval()
        self.requires_grad_(False)
        
        # Apply further optimizations
        torch.jit.optimize_for_inference(self)
        
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information."""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "attention_heads": self.config.attention_heads,
            "hidden_size": self.config.hidden_size,
            "num_layers": self.config.num_layers,
            "vocab_size": self.config.vocab_size,
            "max_position_embeddings": self.config.max_position_embeddings,
            "use_flash_attention": self.config.use_flash_attention,
            "use_gradient_checkpointing": self.config.use_gradient_checkpointing,
            "use_mixed_precision": self.config.use_mixed_precision,
            "use_data_parallel": self.config.use_data_parallel,
            "use_quantization": self.config.use_quantization
        }

# Factory function
def create_enterprise_adapter(config: Optional[AdapterConfig] = None) -> EnterpriseTruthGPTAdapter:
    """Create enterprise TruthGPT adapter."""
    if config is None:
        config = AdapterConfig()
    return EnterpriseTruthGPTAdapter(config)

# Example usage
if __name__ == "__main__":
    # Create enterprise adapter
    config = AdapterConfig(
        mode=AdapterMode.ENTERPRISE,
        attention_heads=32,
        hidden_size=1024,
        num_layers=24,
        vocab_size=50257,
        use_flash_attention=True,
        use_gradient_checkpointing=True,
        use_mixed_precision=True,
        use_data_parallel=True,
        use_quantization=True
    )
    
    adapter = create_enterprise_adapter(config)
    
    # Get model info
    info = adapter.get_model_info()
    print("Model Info:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    # Forward pass
    batch_size = 4
    sequence_length = 512
    input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_length))
    
    output = adapter(input_ids)
    print(f"Output shape: {output.shape}")
    
    # Optimize for inference
    adapter.optimize_for_inference()
    print("Model optimized for inference")
