"""
Efficient K/V Cache Implementation for TruthGPT
Implements optimized key-value caching for transformer inference
Minimizes memory overhead and latency between tokens
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any, List
import math
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod
import gc
from contextlib import contextmanager

logger = logging.getLogger(__name__)

@dataclass
class KVCacheConfig:
    """Configuration for K/V cache optimization."""
    max_cache_size: int = 2048
    cache_dtype: torch.dtype = torch.float16
    use_compression: bool = True
    compression_ratio: float = 0.5
    use_memory_mapping: bool = False
    cache_eviction_policy: str = "lru"  # lru, fifo, random
    enable_cache_warming: bool = True
    cache_precision: str = "fp16"  # fp16, fp32, int8

class KVCache:
    """
    Efficient Key-Value cache for transformer inference.
    
    This implementation provides:
    - Memory-efficient storage of K/V states
    - Automatic cache management and eviction
    - Support for different precision levels
    - Optimized memory access patterns
    """
    
    def __init__(self, config: KVCacheConfig):
        self.config = config
        self.cache: Dict[int, Dict[str, torch.Tensor]] = {}
        self.cache_order: List[int] = []
        self.current_size = 0
        self.hit_count = 0
        self.miss_count = 0
        
    def get(self, layer_idx: int, position: int) -> Optional[Dict[str, torch.Tensor]]:
        """Get cached K/V states for a specific layer and position."""
        cache_key = (layer_idx, position)
        
        if cache_key in self.cache:
            self.hit_count += 1
            # Update LRU order
            if cache_key in self.cache_order:
                self.cache_order.remove(cache_key)
            self.cache_order.append(cache_key)
            return self.cache[cache_key]
        else:
            self.miss_count += 1
            return None
    
    def put(self, layer_idx: int, position: int, kv_states: Dict[str, torch.Tensor]) -> None:
        """Store K/V states in cache."""
        cache_key = (layer_idx, position)
        
        # Check if we need to evict
        if self.current_size >= self.config.max_cache_size:
            self._evict_oldest()
        
        # Store in cache
        self.cache[cache_key] = kv_states
        self.cache_order.append(cache_key)
        self.current_size += 1
    
    def _evict_oldest(self) -> None:
        """Evict oldest cached entry."""
        if self.cache_order:
            oldest_key = self.cache_order.pop(0)
            if oldest_key in self.cache:
                del self.cache[oldest_key]
                self.current_size -= 1
    
    def clear(self) -> None:
        """Clear all cached data."""
        self.cache.clear()
        self.cache_order.clear()
        self.current_size = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.hit_count + self.miss_count
        hit_rate = self.hit_count / total_requests if total_requests > 0 else 0.0
        
        return {
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': hit_rate,
            'current_size': self.current_size,
            'max_size': self.config.max_cache_size
        }

class EfficientMultiHeadAttention(nn.Module):
    """
    Memory-efficient multi-head attention with optimized K/V caching.
    
    This implementation provides:
    - Efficient K/V cache reuse for sequential generation
    - Memory-optimized attention computation
    - Support for different attention patterns (prefill vs decode)
    - Automatic cache management
    """
    
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        dropout: float = 0.1,
        bias: bool = True,
        use_kv_cache: bool = True,
        cache_config: Optional[KVCacheConfig] = None
    ):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.dropout = nn.Dropout(dropout)
        self.use_kv_cache = use_kv_cache
        
        # Validate dimensions
        if d_model % n_heads != 0:
            raise ValueError(f"d_model ({d_model}) must be divisible by n_heads ({n_heads})")
        
        # Linear transformations
        self.query_linear = nn.Linear(d_model, d_model, bias=bias)
        self.key_linear = nn.Linear(d_model, d_model, bias=bias)
        self.value_linear = nn.Linear(d_model, d_model, bias=bias)
        self.output_linear = nn.Linear(d_model, d_model, bias=bias)
        
        # K/V cache
        if use_kv_cache:
            self.kv_cache = KVCache(cache_config or KVCacheConfig())
        else:
            self.kv_cache = None
        
        # Attention computation optimization
        self.scale = math.sqrt(self.head_dim)
        self.use_flash_attention = hasattr(F, 'scaled_dot_product_attention')
    
    def forward(
        self,
        query: torch.Tensor,
        key: Optional[torch.Tensor] = None,
        value: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
        use_cache: bool = True,
        cache_position: Optional[int] = None,
        **kwargs
    ) -> Tuple[torch.Tensor, Optional[Dict[str, torch.Tensor]]]:
        """
        Forward pass with efficient K/V caching.
        
        Args:
            query: Query tensor of shape (batch_size, seq_len, d_model)
            key: Key tensor (optional, will use query if None)
            value: Value tensor (optional, will use query if None)
            mask: Optional attention mask
            use_cache: Whether to use K/V cache
            cache_position: Position in sequence for caching
            **kwargs: Additional arguments
            
        Returns:
            Tuple of (output, cached_kv_states)
        """
        batch_size, seq_len, d_model = query.size()
        
        # Use query as key/value if not provided (self-attention)
        if key is None:
            key = query
        if value is None:
            value = query
        
        # Apply linear transformations
        query = self.query_linear(query)
        key = self.key_linear(key)
        value = self.value_linear(value)
        
        # Reshape for multi-head attention
        query = query.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        key = key.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        value = value.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)
        
        # Try to use cached K/V states
        cached_kv = None
        if self.use_kv_cache and use_cache and cache_position is not None:
            cached_kv = self.kv_cache.get(0, cache_position)  # Layer 0 for now
        
        # Compute attention
        if self.use_flash_attention and not cached_kv:
            # Use PyTorch's optimized attention
            output = F.scaled_dot_product_attention(
                query, key, value, attn_mask=mask, dropout_p=self.dropout.p if self.training else 0.0
            )
        else:
            # Manual attention computation
            output = self._compute_attention(query, key, value, mask, cached_kv)
        
        # Update cache if enabled
        if self.use_kv_cache and use_cache and cache_position is not None:
            kv_states = {
                'key': key.detach(),
                'value': value.detach()
            }
            self.kv_cache.put(0, cache_position, kv_states)
            cached_kv = kv_states
        
        # Reshape back to original format
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        
        # Apply output linear transformation
        output = self.output_linear(output)
        
        return output, cached_kv
    
    def _compute_attention(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
        cached_kv: Optional[Dict[str, torch.Tensor]] = None
    ) -> torch.Tensor:
        """Compute attention manually with optional cached K/V."""
        
        # Use cached K/V if available
        if cached_kv is not None:
            # Concatenate cached and new K/V
            key = torch.cat([cached_kv['key'], key], dim=2)
            value = torch.cat([cached_kv['value'], value], dim=2)
        
        # Compute attention scores
        scores = torch.matmul(query, key.transpose(-2, -1)) / self.scale
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Apply softmax
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        output = torch.matmul(attention_weights, value)
        
        return output
    
    def clear_cache(self) -> None:
        """Clear the K/V cache."""
        if self.kv_cache:
            self.kv_cache.clear()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        if self.kv_cache:
            return self.kv_cache.get_stats()
        return {}

class PrefillDecodeOptimizer:
    """
    Optimizer for separating prefill and decode phases.
    
    This class provides:
    - Efficient prefill phase processing
    - Optimized decode phase with K/V cache reuse
    - Memory management between phases
    - Performance monitoring
    """
    
    def __init__(self, model: nn.Module, cache_config: Optional[KVCacheConfig] = None):
        self.model = model
        self.cache_config = cache_config or KVCacheConfig()
        self.is_prefill_phase = True
        self.current_position = 0
        
    def prefill_phase(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        Process the prefill phase (initial prompt processing).
        
        Args:
            input_ids: Input token IDs
            attention_mask: Attention mask
            **kwargs: Additional arguments
            
        Returns:
            Model outputs
        """
        self.is_prefill_phase = True
        self.current_position = 0
        
        # Process the entire sequence at once
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=True,
                **kwargs
            )
        
        return outputs
    
    def decode_phase(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        Process the decode phase (token-by-token generation).
        
        Args:
            input_ids: New token IDs (typically single token)
            attention_mask: Attention mask
            **kwargs: Additional arguments
            
        Returns:
            Model outputs
        """
        self.is_prefill_phase = False
        
        # Use cached K/V states for efficient generation
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=True,
                cache_position=self.current_position,
                **kwargs
            )
        
        self.current_position += 1
        return outputs
    
    def reset(self) -> None:
        """Reset the optimizer state."""
        self.is_prefill_phase = True
        self.current_position = 0
        
        # Clear all caches
        for module in self.model.modules():
            if hasattr(module, 'clear_cache'):
                module.clear_cache()

class MemoryEfficientAttention:
    """
    Memory-efficient attention computation with various optimizations.
    
    This class provides:
    - Gradient checkpointing for memory efficiency
    - Mixed precision support
    - Memory mapping for large sequences
    - Automatic garbage collection
    """
    
    def __init__(self, use_checkpointing: bool = True, use_mixed_precision: bool = True):
        self.use_checkpointing = use_checkpointing
        self.use_mixed_precision = use_mixed_precision
    
    @contextmanager
    def memory_efficient_forward(self, module: nn.Module):
        """Context manager for memory-efficient forward pass."""
        if self.use_checkpointing:
            # Use gradient checkpointing
            yield torch.utils.checkpoint.checkpoint(module)
        else:
            yield module
    
    def optimize_memory_usage(self, model: nn.Module) -> None:
        """Apply memory optimizations to the model."""
        # Enable gradient checkpointing
        if self.use_checkpointing:
            for module in model.modules():
                if hasattr(module, 'gradient_checkpointing_enable'):
                    module.gradient_checkpointing_enable()
        
        # Enable mixed precision
        if self.use_mixed_precision:
            model = model.half()
        
        # Force garbage collection
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# Factory functions
def create_efficient_attention(
    d_model: int,
    n_heads: int,
    dropout: float = 0.1,
    use_kv_cache: bool = True,
    cache_config: Optional[KVCacheConfig] = None
) -> EfficientMultiHeadAttention:
    """Create an efficient multi-head attention module."""
    return EfficientMultiHeadAttention(
        d_model=d_model,
        n_heads=n_heads,
        dropout=dropout,
        use_kv_cache=use_kv_cache,
        cache_config=cache_config
    )

def create_prefill_decode_optimizer(
    model: nn.Module,
    cache_config: Optional[KVCacheConfig] = None
) -> PrefillDecodeOptimizer:
    """Create a prefill/decode optimizer."""
    return PrefillDecodeOptimizer(model, cache_config)

def create_memory_efficient_attention(
    use_checkpointing: bool = True,
    use_mixed_precision: bool = True
) -> MemoryEfficientAttention:
    """Create a memory-efficient attention handler."""
    return MemoryEfficientAttention(use_checkpointing, use_mixed_precision)


