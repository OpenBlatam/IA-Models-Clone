"""
Test suite for Ultra Enhanced Optimization Core
Tests the most advanced optimization techniques for the optimization_core module
"""

import torch
import torch.nn as nn
import logging
import time
import numpy as np
from optimization_core import (
    create_ultra_enhanced_optimization_core,
    NeuralCodeOptimizer,
    AdaptiveAlgorithmSelector,
    PredictiveOptimizer,
    SelfEvolvingKernel,
    RealTimeProfiler,
    UltraEnhancedOptimizationCore
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_neural_code_optimizer():
    """Test neural code optimization capabilities."""
    logger.info("üß™ Testing Neural Code Optimizer...")
    
    try:
        from optimization_core.ultra_enhanced_optimization_core import UltraEnhancedOptimizationConfig
        
        config = UltraEnhancedOptimizationConfig(enable_neural_code_optimization=True)
        neural_optimizer = NeuralCodeOptimizer(config)
        
        operation_signature = "Linear_layer1"
        tensor_shapes = [(32, 512), (512, 256)]
        
        code_features = neural_optimizer.encode_code_pattern(operation_signature, tensor_shapes)
        
        strategy = neural_optimizer.predict_optimization_strategy(code_features)
        performance_gain = neural_optimizer.predict_performance_gain(code_features)
        
        logger.info(f"‚úÖ Code features shape: {code_features.shape}")
        logger.info(f"‚úÖ Strategy prediction: {len(strategy)} strategies")
        logger.info(f"‚úÖ Performance gain prediction: {performance_gain:.4f}")
        logger.info(f"‚úÖ Top strategy: {max(strategy.items(), key=lambda x: x[1])}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Neural code optimizer test failed: {e}")
        return False

def test_adaptive_algorithm_selector():
    """Test adaptive algorithm selection."""
    logger.info("üß™ Testing Adaptive Algorithm Selector...")
    
    try:
        from optimization_core.ultra_enhanced_optimization_core import UltraEnhancedOptimizationConfig
        
        config = UltraEnhancedOptimizationConfig(enable_adaptive_algorithm_selection=True)
        selector = AdaptiveAlgorithmSelector(config)
        
        def fast_algorithm(*args, **kwargs):
            return "fast_result"
        
        def accurate_algorithm(*args, **kwargs):
            return "accurate_result"
        
        selector.register_algorithm("fast", fast_algorithm, lambda *args: 0.5)
        selector.register_algorithm("accurate", accurate_algorithm, lambda *args: 1.0)
        
        test_tensor = torch.randn(10, 20)
        algorithm_name, algorithm = selector.select_algorithm("linear", test_tensor)
        
        selector.record_performance("fast", "test_context", 0.1, 100.0, 0.95)
        selector.record_performance("accurate", "test_context", 0.2, 150.0, 0.98)
        
        logger.info(f"‚úÖ Selected algorithm: {algorithm_name}")
        logger.info(f"‚úÖ Algorithm performance history: {len(selector.algorithm_performance)}")
        logger.info(f"‚úÖ Selection history: {len(selector.selection_history)}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Adaptive algorithm selector test failed: {e}")
        return False

def test_predictive_optimizer():
    """Test predictive optimization capabilities."""
    logger.info("üß™ Testing Predictive Optimizer...")
    
    try:
        from optimization_core.ultra_enhanced_optimization_core import UltraEnhancedOptimizationConfig
        
        config = UltraEnhancedOptimizationConfig(
            enable_predictive_optimization=True,
            prediction_horizon=10
        )
        predictor = PredictiveOptimizer(config)
        
        for i in range(15):
            predictor.record_operation(
                f"operation_{i % 3}",
                [(32, 64), (64, 32)],
                {"batch_size": 32, "step": i}
            )
        
        predictions = predictor.predict_next_operations(3)
        
        predictor.preoptimize_predicted_operations(predictions)
        
        logger.info(f"‚úÖ Operation sequence length: {len(predictor.operation_sequence)}")
        logger.info(f"‚úÖ Predictions made: {len(predictions)}")
        logger.info(f"‚úÖ Preoptimized cache size: {len(predictor.preoptimized_cache)}")
        
        if predictions:
            logger.info(f"‚úÖ Sample prediction: {predictions[0]['operation_type']}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Predictive optimizer test failed: {e}")
        return False

def test_self_evolving_kernel():
    """Test self-evolving kernel capabilities."""
    logger.info("üß™ Testing Self-Evolving Kernel...")
    
    try:
        from optimization_core.ultra_enhanced_optimization_core import UltraEnhancedOptimizationConfig
        
        config = UltraEnhancedOptimizationConfig(
            enable_self_evolving_kernels=True,
            evolution_generations=5
        )
        
        def base_linear(x):
            return torch.matmul(x, torch.randn(x.shape[-1], 64))
        
        evolving_kernel = SelfEvolvingKernel(base_linear, config)
        
        test_input = torch.randn(10, 32)
        
        results = []
        for i in range(12):  # Trigger evolution at generation 10
            result = evolving_kernel(test_input)
            results.append(result.shape)
        
        logger.info(f"‚úÖ Evolving kernel generation: {evolving_kernel.generation}")
        logger.info(f"‚úÖ Genetic variants: {len(evolving_kernel.genetic_variants)}")
        logger.info(f"‚úÖ Performance history: {len(evolving_kernel.performance_history)}")
        logger.info(f"‚úÖ All results consistent: {all(r == results[0] for r in results)}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Self-evolving kernel test failed: {e}")
        return False

def test_real_time_profiler():
    """Test real-time profiling capabilities."""
    logger.info("üß™ Testing Real-Time Profiler...")
    
    try:
        from optimization_core.ultra_enhanced_optimization_core import UltraEnhancedOptimizationConfig
        
        config = UltraEnhancedOptimizationConfig(
            enable_real_time_profiling=True,
            profiling_window_size=20
        )
        profiler = RealTimeProfiler(config)
        
        profiler.start_profiling()
        
        operations = ["linear", "conv2d", "layernorm", "attention"]
        for i in range(25):
            op_name = operations[i % len(operations)]
            execution_time = np.random.uniform(0.01, 0.1)
            memory_usage = np.random.uniform(100, 1000)
            gpu_util = np.random.uniform(0.3, 0.9)
            
            profiler.record_operation(op_name, execution_time, memory_usage, gpu_util)
            time.sleep(0.001)  # Small delay
        
        recommendations = profiler.get_optimization_recommendations()
        
        profiler.stop_profiling()
        
        logger.info(f"‚úÖ Profiling data points: {len(profiler.profiling_data)}")
        logger.info(f"‚úÖ Current metrics: {len(profiler.current_metrics)}")
        logger.info(f"‚úÖ Optimization recommendations: {len(recommendations)}")
        
        if recommendations:
            logger.info(f"‚úÖ Sample recommendation: {recommendations[0]['type']}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Real-time profiler test failed: {e}")
        return False

def test_ultra_enhanced_optimization_core():
    """Test the complete ultra-enhanced optimization core."""
    logger.info("üß™ Testing Ultra Enhanced Optimization Core...")
    
    try:
        class TestModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear1 = nn.Linear(128, 256)
                self.norm1 = nn.LayerNorm(256)
                self.linear2 = nn.Linear(256, 128)
                self.norm2 = nn.LayerNorm(128)
                self.conv = nn.Conv2d(3, 16, 3)
            
            def forward(self, x):
                if len(x.shape) == 2:  # Linear path
                    x = self.linear1(x)
                    x = self.norm1(x)
                    x = torch.relu(x)
                    x = self.linear2(x)
                    x = self.norm2(x)
                    return x
                else:  # Conv path
                    return self.conv(x)
        
        model = TestModel()
        
        ultra_optimizer = create_ultra_enhanced_optimization_core({
            'enable_neural_code_optimization': True,
            'enable_adaptive_algorithm_selection': True,
            'enable_predictive_optimization': True,
            'enable_self_evolving_kernels': True,
            'enable_real_time_profiling': True,
            'enable_cross_layer_optimization': True,
            'optimization_learning_rate': 0.01,
            'prediction_horizon': 20,
            'evolution_generations': 5
        })
        
        optimized_model, stats = ultra_optimizer.ultra_optimize_module(model)
        
        test_input = torch.randn(4, 128)
        output = optimized_model(test_input)
        
        conv_input = torch.randn(2, 3, 32, 32)
        conv_output = optimized_model(conv_input)
        
        logger.info(f"‚úÖ Ultra optimized model working: {output.shape}")
        logger.info(f"‚úÖ Conv output shape: {conv_output.shape}")
        logger.info(f"‚úÖ Total ultra optimizations: {stats['ultra_optimizations_applied']}")
        logger.info(f"‚úÖ Neural optimizations: {stats['neural_optimizations']}")
        logger.info(f"‚úÖ Adaptive algorithms: {stats['adaptive_algorithms']}")
        logger.info(f"‚úÖ Predictive optimizations: {stats['predictive_optimizations']}")
        logger.info(f"‚úÖ Evolved kernels: {stats['evolved_kernels']}")
        logger.info(f"‚úÖ Cross-layer optimizations: {stats['cross_layer_optimizations']}")
        logger.info(f"‚úÖ Optimization time: {stats['optimization_time']:.4f}s")
        
        report = ultra_optimizer.get_ultra_optimization_report()
        logger.info(f"‚úÖ Ultra optimization report keys: {list(report.keys())}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Ultra enhanced optimization core test failed: {e}")
        return False

def test_integration_with_existing_optimizations():
    """Test integration with existing optimization_core components."""
    logger.info("üß™ Testing Integration with Existing Optimizations...")
    
    try:
        from optimization_core import (
            create_enhanced_optimization_core,
            OptimizedLayerNorm,
            create_quantum_optimization_core,
            create_nas_optimization_core
        )
        
        class IntegratedModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.optimized_norm = OptimizedLayerNorm(64)
                self.linear = nn.Linear(64, 32)
            
            def forward(self, x):
                x = self.optimized_norm(x)
                x = self.linear(x)
                return x
        
        model = IntegratedModel()
        
        enhanced_optimizer = create_enhanced_optimization_core({
            'enable_adaptive_precision': True,
            'enable_dynamic_kernel_fusion': True
        })
        model, enhanced_stats = enhanced_optimizer.enhance_optimization_module(model)
        
        ultra_optimizer = create_ultra_enhanced_optimization_core({
            'enable_neural_code_optimization': True,
            'enable_predictive_optimization': True
        })
        model, ultra_stats = ultra_optimizer.ultra_optimize_module(model)
        
        quantum_optimizer = create_quantum_optimization_core({
            'enable_quantum_superposition': True,
            'enable_quantum_entanglement': True
        })
        model, quantum_stats = quantum_optimizer.optimize_model(model)
        
        test_input = torch.randn(3, 10, 64)
        output = model(test_input)
        
        logger.info(f"‚úÖ Multi-optimized model output: {output.shape}")
        logger.info(f"‚úÖ Enhanced optimizations: {enhanced_stats['optimizations_applied']}")
        logger.info(f"‚úÖ Ultra optimizations: {ultra_stats['ultra_optimizations_applied']}")
        logger.info(f"‚úÖ Quantum optimizations: {quantum_stats['quantum_optimizations_applied']}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Integration test failed: {e}")
        return False

def test_performance_benchmarking():
    """Test performance improvements from ultra optimizations."""
    logger.info("üß™ Testing Performance Benchmarking...")
    
    try:
        class BenchmarkModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layers = nn.Sequential(
                    nn.Linear(256, 512),
                    nn.LayerNorm(512),
                    nn.ReLU(),
                    nn.Linear(512, 256),
                    nn.LayerNorm(256),
                    nn.ReLU(),
                    nn.Linear(256, 128)
                )
            
            def forward(self, x):
                return self.layers(x)
        
        baseline_model = BenchmarkModel()
        
        optimized_model = BenchmarkModel()
        ultra_optimizer = create_ultra_enhanced_optimization_core({
            'enable_neural_code_optimization': True,
            'enable_adaptive_algorithm_selection': True,
            'enable_predictive_optimization': True,
            'enable_real_time_profiling': True
        })
        optimized_model, stats = ultra_optimizer.ultra_optimize_module(optimized_model)
        
        test_input = torch.randn(32, 256)
        num_runs = 10
        
        baseline_times = []
        for _ in range(num_runs):
            start_time = time.time()
            _ = baseline_model(test_input)
            baseline_times.append(time.time() - start_time)
        
        optimized_times = []
        for _ in range(num_runs):
            start_time = time.time()
            _ = optimized_model(test_input)
            optimized_times.append(time.time() - start_time)
        
        baseline_avg = np.mean(baseline_times)
        optimized_avg = np.mean(optimized_times)
        speedup = baseline_avg / optimized_avg if optimized_avg > 0 else 1.0
        
        logger.info(f"‚úÖ Baseline average time: {baseline_avg:.6f}s")
        logger.info(f"‚úÖ Optimized average time: {optimized_avg:.6f}s")
        logger.info(f"‚úÖ Speedup ratio: {speedup:.2f}x")
        logger.info(f"‚úÖ Ultra optimizations applied: {stats['ultra_optimizations_applied']}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Performance benchmarking test failed: {e}")
        return False

def main():
    """Run all ultra enhanced optimization core tests."""
    logger.info("üöÄ Ultra Enhanced Optimization Core Test Suite")
    logger.info("=" * 60)
    
    tests = [
        ("Neural Code Optimizer", test_neural_code_optimizer),
        ("Adaptive Algorithm Selector", test_adaptive_algorithm_selector),
        ("Predictive Optimizer", test_predictive_optimizer),
        ("Self-Evolving Kernel", test_self_evolving_kernel),
        ("Real-Time Profiler", test_real_time_profiler),
        ("Ultra Enhanced Optimization Core", test_ultra_enhanced_optimization_core),
        ("Integration with Existing Optimizations", test_integration_with_existing_optimizations),
        ("Performance Benchmarking", test_performance_benchmarking)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\nüß™ Testing {test_name}...")
        try:
            if test_func():
                logger.info(f"‚úÖ {test_name}: PASS")
                passed += 1
            else:
                logger.info(f"‚ùå {test_name}: FAIL")
        except Exception as e:
            logger.error(f"‚ùå {test_name}: ERROR - {e}")
    
    logger.info(f"\nüìä Test Results:")
    for i, (test_name, _) in enumerate(tests):
        status = "‚úÖ PASS" if i < passed else "‚ùå FAIL"
        logger.info(f"  {test_name}: {status}")
    
    logger.info(f"\nüéØ Summary: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All ultra enhanced optimization core tests passed!")
        logger.info("üöÄ Ultra optimization capabilities are fully functional!")
    else:
        logger.info(f"‚ö†Ô∏è {total - passed} tests failed")
        logger.info("üîß Some ultra optimization features may need attention")

if __name__ == "__main__":
    main()
