#!/usr/bin/env python3
"""
SEO-Specific Evaluation Metrics System
Comprehensive evaluation framework for SEO deep learning models
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass, field
import logging
import time
import json
import os
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, roc_auc_score, 
    confusion_matrix, classification_report, mean_squared_error, 
    mean_absolute_error, r2_score, jaccard_score, f1_score,
    precision_score, recall_score, cohen_kappa_score, matthews_corrcoef,
    silhouette_score, calinski_harabasz_score, davies_bouldin_score
)
from sklearn.preprocessing import label_binarize
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
import warnings
import asyncio
from typing_extensions import Literal, TypedDict

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

# Constants
MAX_CONNECTIONS = 1000
MAX_RETRIES = 100

# =============================================================================
# SEO-SPECIFIC EVALUATION METRICS IMPLEMENTATION
# =============================================================================

@dataclass
class SEOMetricsConfig:
    """Configuration for SEO-specific evaluation metrics."""
    # Core SEO metrics
    ranking_metrics: bool = True
    content_quality_metrics: bool = True
    user_engagement_metrics: bool = True
    technical_seo_metrics: bool = True
    
    # Ranking evaluation
    ndcg_k_values: List[int] = field(default_factory=lambda: [1, 3, 5, 10])
    map_k_values: List[int] = field(default_factory=lambda: [1, 3, 5, 10])
    
    # Content quality thresholds
    min_content_length: int = 300
    max_keyword_density: float = 0.03
    min_readability_score: float = 60.0
    
    # User engagement thresholds
    min_time_on_page: float = 30.0
    min_click_through_rate: float = 0.01
    max_bounce_rate: float = 0.7

@dataclass
class ClassificationMetricsConfig:
    """Configuration for classification metrics."""
    average: str = "weighted"  # micro, macro, weighted, binary
    zero_division: int = 0
    sample_weight: Optional[np.ndarray] = None

@dataclass
class RegressionMetricsConfig:
    """Configuration for regression metrics."""
    multioutput: str = "uniform_average"  # raw_values, uniform_average
    sample_weight: Optional[np.ndarray] = None

@dataclass
class RankingMetricsConfig:
    """Configuration for ranking metrics."""
    k_values: List[int] = field(default_factory=lambda: [1, 3, 5, 10])
    relevance_threshold: float = 0.5

class SEOSpecificMetrics:
    """SEO-specific evaluation metrics for deep learning models."""
    
    def __init__(self, config: SEOMetricsConfig):
        self.config = config
    
    def calculate_ranking_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                                relevance_scores: Optional[np.ndarray] = None) -> Dict[str, float]:
        """Calculate SEO ranking metrics."""
        metrics = {}
        
        # Normalized Discounted Cumulative Gain (NDCG)
        for k in self.config.ndcg_k_values:
            ndcg_score = self._calculate_ndcg(y_true, y_pred, k, relevance_scores)
            metrics[f'ndcg_at_{k}'] = ndcg_score
        
        # Mean Average Precision (MAP)
        for k in self.config.map_k_values:
            map_score = self._calculate_map(y_true, y_pred, k)
            metrics[f'map_at_{k}'] = map_score
        
        # Mean Reciprocal Rank (MRR)
        metrics['mrr'] = self._calculate_mrr(y_true, y_pred)
        
        # Precision at K
        for k in self.config.ndcg_k_values:
            precision_k = self._calculate_precision_at_k(y_true, y_pred, k)
            metrics[f'precision_at_{k}'] = precision_k
        
        # Recall at K
        for k in self.config.ndcg_k_values:
            recall_k = self._calculate_recall_at_k(y_true, y_pred, k)
            metrics[f'recall_at_{k}'] = recall_k
        
        return metrics
    
    def _calculate_ndcg(self, y_true: np.ndarray, y_pred: np.ndarray, 
                        k: int, relevance_scores: Optional[np.ndarray] = None) -> float:
        """Calculate NDCG at k."""
        if relevance_scores is None:
            relevance_scores = y_true
        
        # Sort predictions by relevance
        sorted_indices = np.argsort(y_pred)[::-1][:k]
        
        # Calculate DCG
        dcg = 0.0
        for i, idx in enumerate(sorted_indices):
            dcg += relevance_scores[idx] / np.log2(i + 2)
        
        # Calculate IDCG (ideal DCG)
        ideal_indices = np.argsort(relevance_scores)[::-1][:k]
        idcg = 0.0
        for i, idx in enumerate(ideal_indices):
            idcg += relevance_scores[idx] / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def _calculate_map(self, y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
        """Calculate MAP at k."""
        sorted_indices = np.argsort(y_pred)[::-1][:k]
        
        ap_sum = 0.0
        relevant_count = 0
        
        for i, idx in enumerate(sorted_indices):
            if y_true[idx] > 0:  # Relevant item
                relevant_count += 1
                precision_at_i = relevant_count / (i + 1)
                ap_sum += precision_at_i
        
        return ap_sum / np.sum(y_true > 0) if np.sum(y_true > 0) > 0 else 0.0
    
    def _calculate_mrr(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """Calculate Mean Reciprocal Rank."""
        sorted_indices = np.argsort(y_pred)[::-1]
        
        rr_sum = 0.0
        for i, idx in enumerate(sorted_indices):
            if y_true[idx] > 0:  # First relevant item
                rr_sum += 1.0 / (i + 1)
                break
        
        return rr_sum
    
    def _calculate_precision_at_k(self, y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
        """Calculate precision at k."""
        sorted_indices = np.argsort(y_pred)[::-1][:k]
        relevant_count = np.sum(y_true[sorted_indices] > 0)
        return relevant_count / k
    
    def _calculate_recall_at_k(self, y_true: np.ndarray, y_pred: np.ndarray, k: int) -> float:
        """Calculate recall at k."""
        sorted_indices = np.argsort(y_pred)[::-1][:k]
        relevant_count = np.sum(y_true[sorted_indices] > 0)
        total_relevant = np.sum(y_true > 0)
        return relevant_count / total_relevant if total_relevant > 0 else 0.0
    
    def calculate_content_quality_metrics(self, content_data: Dict[str, Any]) -> Dict[str, float]:
        """Calculate content quality metrics."""
        metrics = {}
        
        # Content length
        content_length = content_data.get('content_length', 0)
        metrics['content_length'] = content_length
        metrics['content_length_score'] = min(1.0, content_length / self.config.min_content_length)
        
        # Keyword density
        keyword_density = content_data.get('keyword_density', 0.0)
        metrics['keyword_density'] = keyword_density
        metrics['keyword_density_score'] = 1.0 - min(1.0, keyword_density / self.config.max_keyword_density)
        
        # Readability score
        readability_score = content_data.get('readability_score', 0.0)
        metrics['readability_score'] = readability_score
        metrics['readability_score_normalized'] = max(0.0, readability_score / 100.0)
        
        # Overall content quality score
        metrics['overall_content_quality'] = (
            metrics['content_length_score'] * 0.4 +
            metrics['keyword_density_score'] * 0.3 +
            metrics['readability_score_normalized'] * 0.3
        )
        
        return metrics
    
    def calculate_user_engagement_metrics(self, engagement_data: Dict[str, Any]) -> Dict[str, float]:
        """Calculate user engagement metrics."""
        metrics = {}
        
        # Time on page
        time_on_page = engagement_data.get('time_on_page', 0.0)
        metrics['time_on_page'] = time_on_page
        metrics['time_on_page_score'] = min(1.0, time_on_page / self.config.min_time_on_page)
        
        # Click-through rate
        ctr = engagement_data.get('click_through_rate', 0.0)
        metrics['click_through_rate'] = ctr
        metrics['ctr_score'] = min(1.0, ctr / self.config.min_click_through_rate)
        
        # Bounce rate
        bounce_rate = engagement_data.get('bounce_rate', 1.0)
        metrics['bounce_rate'] = bounce_rate
        metrics['bounce_rate_score'] = max(0.0, 1.0 - (bounce_rate / self.config.max_bounce_rate))
        
        # Overall engagement score
        metrics['overall_engagement_score'] = (
            metrics['time_on_page_score'] * 0.4 +
            metrics['ctr_score'] * 0.4 +
            metrics['bounce_rate_score'] * 0.2
        )
        
        return metrics
    
    def calculate_technical_seo_metrics(self, technical_data: Dict[str, Any]) -> Dict[str, float]:
        """Calculate technical SEO metrics."""
        metrics = {}
        
        # Page load speed
        load_speed = technical_data.get('page_load_speed', 0.0)
        metrics['page_load_speed'] = load_speed
        metrics['load_speed_score'] = max(0.0, 1.0 - (load_speed / 3.0))  # 3s threshold
        
        # Mobile friendliness
        mobile_score = technical_data.get('mobile_friendliness', 0.0)
        metrics['mobile_friendliness'] = mobile_score
        metrics['mobile_score_normalized'] = mobile_score / 100.0
        
        # Core Web Vitals
        lcp = technical_data.get('largest_contentful_paint', 0.0)
        metrics['lcp'] = lcp
        metrics['lcp_score'] = 1.0 if lcp <= 2.5 else max(0.0, 1.0 - (lcp - 2.5) / 2.5)
        
        fid = technical_data.get('first_input_delay', 0.0)
        metrics['fid'] = fid
        metrics['fid_score'] = 1.0 if fid <= 100 else max(0.0, 1.0 - (fid - 100) / 100)
        
        cls = technical_data.get('cumulative_layout_shift', 0.0)
        metrics['cls'] = cls
        metrics['cls_score'] = 1.0 if cls <= 0.1 else max(0.0, 1.0 - (cls - 0.1) / 0.1)
        
        # Overall technical score
        metrics['overall_technical_score'] = (
            metrics['load_speed_score'] * 0.2 +
            metrics['mobile_score_normalized'] * 0.2 +
            metrics['lcp_score'] * 0.2 +
            metrics['fid_score'] * 0.2 +
            metrics['cls_score'] * 0.2
        )
        
        return metrics

class ClassificationMetrics:
    """Comprehensive classification metrics for SEO models."""
    
    def __init__(self, config: ClassificationMetricsConfig):
        self.config = config
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, 
                         y_prob: Optional[np.ndarray] = None) -> Dict[str, float]:
        """Calculate all classification metrics."""
        metrics = {}
        
        # Basic accuracy
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        
        # Precision, Recall, F1
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, 
            average=self.config.average, 
            zero_division=self.config.zero_division,
            sample_weight=self.config.sample_weight
        )
        metrics['precision'] = precision
        metrics['recall'] = recall
        metrics['f1_score'] = f1
        metrics['support'] = support
        
        # Individual class metrics
        if len(np.unique(y_true)) == 2:  # Binary classification
            metrics['precision_binary'] = precision_score(
                y_true, y_pred, average='binary', zero_division=self.config.zero_division
            )
            metrics['recall_binary'] = recall_score(
                y_true, y_pred, average='binary', zero_division=self.config.zero_division
            )
            metrics['f1_binary'] = f1_score(
                y_true, y_pred, average='binary', zero_division=self.config.zero_division
            )
        
        # ROC AUC (if probabilities provided)
        if y_prob is not None:
            try:
                if len(np.unique(y_true)) == 2:
                    metrics['roc_auc'] = roc_auc_score(y_true, y_prob[:, 1])
                else:
                    metrics['roc_auc'] = roc_auc_score(y_true, y_prob, multi_class='ovr')
            except Exception as e:
                print(f"‚ö†Ô∏è ROC AUC calculation failed: {e}")
        
        # Additional metrics
        metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)
        metrics['matthews_corrcoef'] = matthews_corrcoef(y_true, y_pred)
        
        return metrics

class RegressionMetrics:
    """Comprehensive regression metrics for SEO models."""
    
    def __init__(self, config: RegressionMetricsConfig):
        self.config = config
    
    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate all regression metrics."""
        metrics = {}
        
        # Mean Squared Error
        metrics['mse'] = mean_squared_error(
            y_true, y_pred, 
            sample_weight=self.config.sample_weight
        )
        
        # Root Mean Squared Error
        metrics['rmse'] = np.sqrt(metrics['mse'])
        
        # Mean Absolute Error
        metrics['mae'] = mean_absolute_error(
            y_true, y_pred, 
            sample_weight=self.config.sample_weight
        )
        
        # R-squared
        metrics['r2_score'] = r2_score(
            y_true, y_pred, 
            sample_weight=self.config.sample_weight
        )
        
        # Mean Absolute Percentage Error
        metrics['mape'] = np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))) * 100
        
        # Symmetric Mean Absolute Percentage Error
        metrics['smape'] = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))
        
        # Huber Loss
        delta = 1.0
        huber_loss = np.where(
            np.abs(y_true - y_pred) <= delta,
            0.5 * (y_true - y_pred) ** 2,
            delta * np.abs(y_true - y_pred) - 0.5 * delta ** 2
        )
        metrics['huber_loss'] = np.mean(huber_loss)
        
        return metrics

class SEOModelEvaluator:
    """Comprehensive SEO model evaluator."""
    
    def __init__(self, seo_config: SEOMetricsConfig,
                 classification_config: ClassificationMetricsConfig,
                 regression_config: RegressionMetricsConfig):
        self.seo_config = seo_config
        self.classification_config = classification_config
        self.regression_config = regression_config
        
        # Initialize metric calculators
        self.seo_metrics = SEOSpecificMetrics(seo_config)
        self.classification_metrics = ClassificationMetrics(classification_config)
        self.regression_metrics = RegressionMetrics(regression_config)
        
        # Results storage
        self.evaluation_results = {}
    
    async def evaluate_seo_model(self, model: nn.Module, 
                                test_data: Dict[str, Any],
                                task_type: str = "ranking") -> Dict[str, Any]:
        """Evaluate SEO model comprehensively."""
        print(f"üöÄ Evaluating SEO model for task: {task_type}")
        
        try:
            # Extract data
            y_true = test_data.get('y_true')
            y_pred = test_data.get('y_pred')
            y_prob = test_data.get('y_prob')
            
            if y_true is None or y_pred is None:
                raise ValueError("Missing required data: y_true and y_pred")
            
            # Convert to numpy if needed
            if torch.is_tensor(y_true):
                y_true = y_true.cpu().numpy()
            if torch.is_tensor(y_pred):
                y_pred = y_pred.cpu().numpy()
            if y_prob is not None and torch.is_tensor(y_prob):
                y_prob = y_prob.cpu().numpy()
            
            # Calculate task-specific metrics
            if task_type == "classification":
                results = self.classification_metrics.calculate_metrics(y_true, y_pred, y_prob)
            elif task_type == "regression":
                results = self.regression_metrics.calculate_metrics(y_true, y_pred)
            elif task_type == "ranking":
                results = self.seo_metrics.calculate_ranking_metrics(y_true, y_pred, y_prob)
            else:
                raise ValueError(f"Unknown task type: {task_type}")
            
            # Add SEO-specific metrics if available
            if 'content_data' in test_data:
                content_metrics = self.seo_metrics.calculate_content_quality_metrics(
                    test_data['content_data']
                )
                results.update(content_metrics)
            
            if 'engagement_data' in test_data:
                engagement_metrics = self.seo_metrics.calculate_user_engagement_metrics(
                    test_data['engagement_data']
                )
                results.update(engagement_metrics)
            
            if 'technical_data' in test_data:
                technical_metrics = self.seo_metrics.calculate_technical_seo_metrics(
                    test_data['technical_data']
                )
                results.update(technical_metrics)
            
            # Store results
            self.evaluation_results[task_type] = results
            
            print(f"‚úÖ {task_type.capitalize()} evaluation completed")
            return results
            
        except Exception as e:
            print(f"‚ùå Error in {task_type} evaluation: {e}")
            raise
    
    def get_evaluation_summary(self) -> Dict[str, Any]:
        """Get comprehensive evaluation summary."""
        return {
            'task_results': self.evaluation_results,
            'overall_score': self._calculate_overall_score(),
            'recommendations': self._generate_recommendations()
        }
    
    def _calculate_overall_score(self) -> float:
        """Calculate overall model performance score."""
        if not self.evaluation_results:
            return 0.0
        
        scores = []
        for task, results in self.evaluation_results.items():
            if task == "classification":
                # Use F1 score for classification
                score = results.get('f1_score', 0.0)
            elif task == "regression":
                # Use R¬≤ score for regression
                score = results.get('r2_score', 0.0)
            elif task == "ranking":
                # Use NDCG@5 for ranking
                score = results.get('ndcg_at_5', 0.0)
            else:
                score = 0.0
            
            scores.append(score)
        
        return np.mean(scores) if scores else 0.0
    
    def _generate_recommendations(self) -> List[str]:
        """Generate improvement recommendations based on metrics."""
        recommendations = []
        
        for task, results in self.evaluation_results.items():
            if task == "classification":
                if results.get('f1_score', 0.0) < 0.7:
                    recommendations.append("Classification F1 score is low. Consider data augmentation or model tuning.")
                
                if results.get('precision', 0.0) < results.get('recall', 0.0):
                    recommendations.append("High false positives. Consider threshold adjustment or feature engineering.")
            
            elif task == "regression":
                if results.get('r2_score', 0.0) < 0.6:
                    recommendations.append("Regression R¬≤ score is low. Consider feature selection or model complexity.")
                
                if results.get('mape', 100.0) > 20.0:
                    recommendations.append("High MAPE. Consider log transformation or robust loss functions.")
            
            elif task == "ranking":
                if results.get('ndcg_at_5', 0.0) < 0.6:
                    recommendations.append("Ranking NDCG@5 is low. Consider learning-to-rank approaches.")
                
                if results.get('mrr', 0.0) < 0.3:
                    recommendations.append("Low MRR. Consider improving top-k ranking accuracy.")
        
        # SEO-specific recommendations
        if 'overall_content_quality' in self.evaluation_results.get('ranking', {}):
            content_score = self.evaluation_results['ranking']['overall_content_quality']
            if content_score < 0.6:
                recommendations.append("Content quality score is low. Focus on content optimization.")
        
        if 'overall_engagement_score' in self.evaluation_results.get('ranking', {}):
            engagement_score = self.evaluation_results['ranking']['overall_engagement_score']
            if engagement_score < 0.5:
                recommendations.append("User engagement is low. Improve user experience and content relevance.")
        
        return recommendations
    
    def save_evaluation_results(self, filepath: str):
        """Save evaluation results to file."""
        try:
            results = self.get_evaluation_summary()
            
            with open(filepath, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"‚úÖ Evaluation results saved to {filepath}")
            
        except Exception as e:
            print(f"‚ùå Error saving results: {e}")
    
    def plot_evaluation_metrics(self, save_path: Optional[str] = None):
        """Plot evaluation metrics visualization."""
        if not self.evaluation_results:
            print("‚ö†Ô∏è No evaluation results to plot")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('SEO Model Evaluation Metrics', fontsize=16)
        
        # Task performance comparison
        if len(self.evaluation_results) > 1:
            tasks = list(self.evaluation_results.keys())
            overall_scores = [self._calculate_overall_score() for _ in tasks]
            
            axes[0, 0].bar(tasks, overall_scores)
            axes[0, 0].set_title('Overall Performance by Task')
            axes[0, 0].set_ylabel('Score')
            axes[0, 0].set_ylim(0, 1)
        
        # Metric breakdown for first task
        if self.evaluation_results:
            first_task = list(self.evaluation_results.keys())[0]
            task_results = self.evaluation_results[first_task]
            
            # Filter numeric metrics
            numeric_metrics = {k: v for k, v in task_results.items() 
                             if isinstance(v, (int, float)) and not k.endswith('_score')}
            
            if numeric_metrics:
                metric_names = list(numeric_metrics.keys())[:8]  # Limit to 8 metrics
                metric_values = [numeric_metrics[name] for name in metric_names]
                
                axes[0, 1].barh(metric_names, metric_values)
                axes[0, 1].set_title(f'{first_task.capitalize()} Metrics')
                axes[0, 1].set_xlabel('Value')
        
        # Learning curves (if available)
        if 'training_history' in self.evaluation_results:
            history = self.evaluation_results['training_history']
            if 'train_loss' in history and 'val_loss' in history:
                axes[1, 0].plot(history['train_loss'], label='Train Loss')
                axes[1, 0].plot(history['val_loss'], label='Validation Loss')
                axes[1, 0].set_title('Training History')
                axes[1, 0].set_xlabel('Epoch')
                axes[1, 0].set_ylabel('Loss')
                axes[1, 0].legend()
                axes[1, 0].grid(True)
        
        # SEO-specific metrics
        seo_metrics = {}
        for task_results in self.evaluation_results.values():
            for key, value in task_results.items():
                if key.endswith('_score') and isinstance(value, (int, float)):
                    seo_metrics[key] = value
        
        if seo_metrics:
            metric_names = list(seo_metrics.keys())[:6]  # Limit to 6 metrics
            metric_values = [seo_metrics[name] for name in metric_names]
            
            axes[1, 1].pie(metric_values, labels=metric_names, autopct='%1.1f%%')
            axes[1, 1].set_title('SEO Quality Metrics Distribution')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ Evaluation plots saved to {save_path}")
        
        plt.show()

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

async def evaluate_seo_model_async(model: nn.Module, 
                                  test_data: Dict[str, Any],
                                  task_type: str = "ranking",
                                  config: Optional[SEOMetricsConfig] = None) -> Dict[str, Any]:
    """Async wrapper for SEO model evaluation."""
    if config is None:
        config = SEOMetricsConfig()
    
    evaluator = SEOModelEvaluator(
        seo_config=config,
        classification_config=ClassificationMetricsConfig(),
        regression_config=RegressionMetricsConfig()
    )
    
    return await evaluator.evaluate_seo_model(model, test_data, task_type)

def create_seo_test_data(n_samples: int = 1000, n_classes: int = 3) -> Dict[str, Any]:
    """Create sample SEO test data for demonstration."""
    np.random.seed(42)
    
    # Generate sample data
    y_true = np.random.randint(0, n_classes, n_samples)
    y_pred = np.random.randint(0, n_classes, n_samples)
    y_prob = np.random.rand(n_samples, n_classes)
    y_prob = y_prob / y_prob.sum(axis=1, keepdims=True)
    
    # SEO-specific data
    content_data = {
        'content_length': np.random.randint(200, 2000, n_samples),
        'keyword_density': np.random.uniform(0.01, 0.05, n_samples),
        'readability_score': np.random.uniform(40, 90, n_samples)
    }
    
    engagement_data = {
        'time_on_page': np.random.uniform(10, 300, n_samples),
        'click_through_rate': np.random.uniform(0.005, 0.05, n_samples),
        'bounce_rate': np.random.uniform(0.3, 0.9, n_samples)
    }
    
    technical_data = {
        'page_load_speed': np.random.uniform(0.5, 5.0, n_samples),
        'mobile_friendliness': np.random.uniform(60, 100, n_samples),
        'largest_contentful_paint': np.random.uniform(1.0, 4.0, n_samples),
        'first_input_delay': np.random.uniform(50, 200, n_samples),
        'cumulative_layout_shift': np.random.uniform(0.05, 0.2, n_samples)
    }
    
    return {
        'y_true': y_true,
        'y_pred': y_pred,
        'y_prob': y_prob,
        'content_data': content_data,
        'engagement_data': engagement_data,
        'technical_data': technical_data
    }

# =============================================================================
# DEMONSTRATION FUNCTION
# =============================================================================

async def demonstrate_seo_evaluation():
    """Demonstrate SEO evaluation metrics system."""
    print("üöÄ SEO Evaluation Metrics System Demonstration")
    
    # Create sample model (placeholder)
    class SampleSEOModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(64, 10)
        
        def forward(self, x):
            return self.linear(x)
    
    model = SampleSEOModel()
    
    # Create test data
    test_data = create_seo_test_data(1000, 3)
    
    # Configuration
    seo_config = SEOMetricsConfig(
        ranking_metrics=True,
        content_quality_metrics=True,
        user_engagement_metrics=True,
        technical_seo_metrics=True
    )
    
    # Create evaluator
    evaluator = SEOModelEvaluator(
        seo_config=seo_config,
        classification_config=ClassificationMetricsConfig(),
        regression_config=RegressionMetricsConfig()
    )
    
    # Evaluate different task types
    task_types = ["classification", "ranking"]
    
    for task_type in task_types:
        print(f"\nüìä Evaluating {task_type} task...")
        
        try:
            results = await evaluator.evaluate_seo_model(model, test_data, task_type)
            
            # Print key metrics
            if task_type == "classification":
                print(f"  Accuracy: {results.get('accuracy', 0):.4f}")
                print(f"  F1 Score: {results.get('f1_score', 0):.4f}")
                print(f"  Precision: {results.get('precision', 0):.4f}")
                print(f"  Recall: {results.get('recall', 0):.4f}")
            
            elif task_type == "ranking":
                print(f"  NDCG@5: {results.get('ndcg_at_5', 0):.4f}")
                print(f"  MAP@5: {results.get('map_at_5', 0):.4f}")
                print(f"  MRR: {results.get('mrr', 0):.4f}")
                print(f"  Content Quality: {results.get('overall_content_quality', 0):.4f}")
                print(f"  User Engagement: {results.get('overall_engagement_score', 0):.4f}")
                print(f"  Technical Score: {results.get('overall_technical_score', 0):.4f}")
        
        except Exception as e:
            print(f"  ‚ùå {task_type} evaluation failed: {e}")
    
    # Get evaluation summary
    summary = evaluator.get_evaluation_summary()
    print(f"\nüéØ Overall Model Score: {summary['overall_score']:.4f}")
    
    # Print recommendations
    if summary['recommendations']:
        print(f"\nüí° Improvement Recommendations:")
        for i, rec in enumerate(summary['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    # Save results
    evaluator.save_evaluation_results("seo_evaluation_results.json")
    
    # Create visualizations
    evaluator.plot_evaluation_metrics("seo_evaluation_plots.png")
    
    print("\n‚úÖ SEO evaluation demonstration completed!")

if __name__ == "__main__":
    # Run demonstration
    asyncio.run(demonstrate_seo_evaluation()) 