#!/usr/bin/env python3
"""
Interactive Demo Showcase for Advanced LLM SEO Engine
Demonstrates all interactive features including SEO analysis, diffusion models, 
batch processing, and performance monitoring
"""

import asyncio
import time
import json
from pathlib import Path
import sys
import os
import numpy as np
import torch

# Add the current directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from advanced_llm_seo_engine import (
    SEOConfig, AdvancedLLMSEOEngine
)

class InteractiveDemoShowcase:
    """Comprehensive showcase of all interactive demo features."""
    
    def __init__(self):
        """Initialize the demo showcase."""
        self.config = SEOConfig(
            batch_size=16,
            learning_rate=2e-5,
            use_mixed_precision=True,
            use_diffusion=True,
            save_checkpoints=True,
            checkpoint_dir="./demo_checkpoints"
        )
        
        self.engine = None
        self.demo_results = {}
        
    async def initialize(self):
        """Initialize the SEO engine."""
        print("üöÄ Initializing Advanced LLM SEO Engine...")
        
        try:
            self.engine = AdvancedLLMSEOEngine(self.config)
            await self.engine.initialize_models()
            print("‚úÖ Engine initialized successfully!")
            return True
        except Exception as e:
            print(f"‚ùå Failed to initialize engine: {e}")
            return False
    
    def showcase_seo_analysis_demo(self):
        """Showcase SEO analysis demo features."""
        print("\nüéØ SEO Analysis Demo Showcase")
        print("=" * 50)
        
        sample_texts = {
            "comprehensive": """
            SEO optimization is the cornerstone of digital marketing success. 
            This comprehensive guide covers essential strategies including keyword research, 
            content optimization, technical SEO best practices, and performance monitoring. 
            Implementing these proven techniques will significantly improve your search engine 
            rankings and drive sustainable organic traffic growth.
            """,
            "keyword_density": """
            Digital marketing strategies focus on content creation and SEO optimization. 
            Marketing professionals use various tools for keyword research and content analysis. 
            SEO tools help marketers optimize their content for better search rankings.
            """,
            "readability": """
            This is a simple sentence. It contains basic words. The structure is clear. 
            Each statement is short. The meaning is easy to understand. Complex concepts 
            are broken down. Reading becomes effortless.
            """,
            "sentiment": """
            This amazing product exceeded all expectations! The incredible features and 
            outstanding performance make it absolutely wonderful. I'm thrilled with the 
            excellent quality and fantastic results.
            """,
            "technical_seo": """
            <title>Comprehensive SEO Guide - Digital Marketing Excellence</title>
            <meta name="description" content="Learn essential SEO strategies including 
            keyword research, content optimization, and technical best practices for 
            improved search rankings and organic traffic growth.">
            <h1>SEO Optimization Guide</h1>
            <h2>Keyword Research</h2>
            <h3>Content Strategy</h3>
            """
        }
        
        for analysis_type, text in sample_texts.items():
            print(f"\nüìä Testing {analysis_type.upper()} Analysis:")
            print(f"Text: {text[:100]}...")
            
            try:
                # Mock the demo analysis (since we're not in Gradio context)
                results = self._mock_seo_analysis(text, analysis_type, "en")
                
                if "error" not in results:
                    print(f"  ‚úÖ Analysis successful!")
                    print(f"  üìù Word Count: {results.get('word_count', 'N/A')}")
                    print(f"  ‚è±Ô∏è Processing Time: {results.get('processing_time', 'N/A')}")
                    
                    if analysis_type == "comprehensive":
                        print(f"  üéØ SEO Score: {results.get('seo_score', 'N/A'):.1f}")
                        print(f"  üîë Keyword Density: {results.get('keyword_density', 'N/A'):.1f}%")
                        print(f"  üìñ Readability: {results.get('readability_score', 'N/A'):.1f}")
                        print(f"  üòä Sentiment: {results.get('sentiment_score', 'N/A'):.2f}")
                        print(f"  ‚öôÔ∏è Technical SEO: {results.get('technical_seo_score', 'N/A'):.1f}")
                    
                    self.demo_results[f"seo_analysis_{analysis_type}"] = results
                else:
                    print(f"  ‚ùå Analysis failed: {results['error']}")
                    
            except Exception as e:
                print(f"  ‚ùå Analysis error: {e}")
    
    def showcase_diffusion_demo(self):
        """Showcase diffusion model demo features."""
        print("\nüé® Diffusion Model Demo Showcase")
        print("=" * 50)
        
        if not self.engine.diffusion_generator:
            print("‚ö†Ô∏è Diffusion models not enabled, skipping demo")
            return
        
        test_prompts = [
            {
                "prompt": "A serene mountain landscape at sunset with golden light, high quality, detailed, professional photography",
                "negative_prompt": "blurry, low quality, distorted, amateur, dark",
                "steps": 50,
                "guidance_scale": 7.5
            },
            {
                "prompt": "A futuristic city skyline with neon lights and flying cars, cyberpunk style, ultra-detailed",
                "negative_prompt": "old, traditional, low resolution, simple",
                "steps": 75,
                "guidance_scale": 8.5
            },
            {
                "prompt": "A cute cat playing with colorful yarn balls, soft lighting, adorable, high quality",
                "negative_prompt": "scary, aggressive, dark, low quality",
                "steps": 40,
                "guidance_scale": 6.5
            }
        ]
        
        for i, test_case in enumerate(test_prompts, 1):
            print(f"\nüé≠ Testing Diffusion Generation {i}:")
            print(f"Prompt: {test_case['prompt'][:60]}...")
            print(f"Negative: {test_case['negative_prompt']}")
            print(f"Steps: {test_case['steps']}, Guidance: {test_case['guidance_scale']}")
            
            try:
                # Mock the diffusion demo
                image, metadata = self._mock_diffusion_demo(
                    test_case["prompt"],
                    test_case["negative_prompt"],
                    test_case["steps"],
                    test_case["guidance_scale"],
                    -1
                )
                
                if image is not None and "error" not in metadata:
                    print(f"  ‚úÖ Image generation successful!")
                    print(f"  üé® Model: {metadata['model']}")
                    print(f"  ‚è±Ô∏è Generation Time: {metadata['generation_time']}")
                    print(f"  üîß Steps: {metadata['steps']}")
                    print(f"  üìè Guidance Scale: {metadata['guidance_scale']}")
                    print(f"  üé≤ Seed: {metadata['seed']}")
                    
                    self.demo_results[f"diffusion_demo_{i}"] = metadata
                else:
                    print(f"  ‚ùå Generation failed: {metadata.get('error', 'Unknown error')}")
                    
            except Exception as e:
                print(f"  ‚ùå Generation error: {e}")
    
    def showcase_batch_processing_demo(self):
        """Showcase batch processing demo features."""
        print("\n‚ö° Batch Processing Demo Showcase")
        print("=" * 50)
        
        batch_texts = """
        SEO optimization requires strategic keyword placement and content structure.
        Content marketing focuses on creating valuable, relevant content for target audiences.
        Technical SEO involves optimizing website infrastructure and performance.
        Link building strategies help establish domain authority and credibility.
        User experience optimization improves engagement and conversion rates.
        """
        
        processing_types = [
            "seo_optimization",
            "content_generation", 
            "keyword_extraction",
            "sentiment_analysis"
        ]
        
        for processing_type in processing_types:
            print(f"\nüîÑ Testing {processing_type.replace('_', ' ').title()}:")
            
            try:
                results = self._mock_batch_processing(batch_texts, processing_type)
                
                if "error" not in results:
                    print(f"  ‚úÖ Batch processing successful!")
                    print(f"  üìä Total texts processed: {results['total_texts']}")
                    print(f"  üìù Sample results:")
                    
                    for i, result in enumerate(results['results'][:3], 1):
                        if processing_type == "seo_optimization":
                            print(f"    Text {i}: SEO Score {result.get('seo_score', 'N/A'):.1f}")
                        elif processing_type == "keyword_extraction":
                            keywords = result.get('extracted_keywords', [])
                            print(f"    Text {i}: {len(keywords)} keywords extracted")
                        elif processing_type == "sentiment_analysis":
                            sentiment = result.get('sentiment', 'N/A')
                            confidence = result.get('confidence', 'N/A')
                            print(f"    Text {i}: {sentiment} (confidence: {confidence:.2f})")
                    
                    self.demo_results[f"batch_processing_{processing_type}"] = results
                else:
                    print(f"  ‚ùå Processing failed: {results['error']}")
                    
            except Exception as e:
                print(f"  ‚ùå Processing error: {e}")
    
    def showcase_performance_monitoring(self):
        """Showcase performance monitoring features."""
        print("\nüìä Performance Monitoring Showcase")
        print("=" * 50)
        
        try:
            # Mock performance metrics
            system_metrics, model_metrics = self._mock_performance_metrics()
            
            print("üñ•Ô∏è System Metrics:")
            if "error" not in system_metrics:
                print(f"  üíª CPU Usage: {system_metrics.get('cpu_percent', 'N/A')}%")
                print(f"  üíæ Memory Usage: {system_metrics.get('memory_percent', 'N/A')}%")
                print(f"  üíø Disk Usage: {system_metrics.get('disk_usage', 'N/A')}%")
                
                if 'gpu_memory_allocated' in system_metrics:
                    print(f"  üéÆ GPU Memory Allocated: {system_metrics['gpu_memory_allocated']}")
                    print(f"  üéÆ GPU Memory Cached: {system_metrics['gpu_memory_cached']}")
                
                self.demo_results["system_metrics"] = system_metrics
            else:
                print(f"  ‚ùå System metrics failed: {system_metrics['error']}")
            
            print("\nüß† Model Metrics:")
            if "error" not in model_metrics:
                print(f"  üß† Model Loaded: {model_metrics.get('model_loaded', 'N/A')}")
                print(f"  üé® Diffusion Loaded: {model_metrics.get('diffusion_loaded', 'N/A')}")
                print(f"  üì± Device: {model_metrics.get('current_device', 'N/A')}")
                print(f"  ‚ö° Mixed Precision: {model_metrics.get('mixed_precision', 'N/A')}")
                print(f"  üì¶ Batch Size: {model_metrics.get('batch_size', 'N/A')}")
                
                if 'last_epoch' in model_metrics:
                    print(f"  üîÑ Last Epoch: {model_metrics['last_epoch']}")
                    print(f"  üìâ Last Train Loss: {model_metrics['last_train_loss']:.4f}")
                    print(f"  üìâ Last Val Loss: {model_metrics['last_val_loss']:.4f}")
                    print(f"  üìà Learning Rate: {model_metrics['learning_rate']:.2e}")
                
                self.demo_results["model_metrics"] = model_metrics
            else:
                print(f"  ‚ùå Model metrics failed: {model_metrics['error']}")
            
            # Test custom metrics
            print("\nüìä Custom Metrics:")
            custom_metrics = [
                ("demo_accuracy", 0.92),
                ("demo_f1_score", 0.88),
                ("demo_precision", 0.91),
                ("demo_recall", 0.85)
            ]
            
            for name, value in custom_metrics:
                result = self._mock_add_custom_metric(name, value)
                if "error" not in result:
                    print(f"  ‚úÖ Added metric '{name}': {value}")
                else:
                    print(f"  ‚ùå Failed to add metric '{name}': {result['error']}")
            
            # Get metrics history
            history = self._mock_get_metrics_history()
            if "error" not in history:
                print(f"  üìã Metrics History: {history['total_metrics']} entries")
            
        except Exception as e:
            print(f"‚ùå Performance monitoring error: {e}")
    
    def showcase_training_visualization(self):
        """Showcase training visualization features."""
        print("\nüìà Training Visualization Showcase")
        print("=" * 50)
        
        try:
            # Mock training history
            self.engine.training_history = [
                {"epoch": 1, "train_loss": 0.85, "val_loss": 0.92, "learning_rate": 2e-5},
                {"epoch": 2, "train_loss": 0.72, "val_loss": 0.78, "learning_rate": 1.8e-5},
                {"epoch": 3, "train_loss": 0.61, "val_loss": 0.65, "learning_rate": 1.6e-5},
                {"epoch": 4, "train_loss": 0.53, "val_loss": 0.58, "learning_rate": 1.4e-5},
                {"epoch": 5, "train_loss": 0.47, "val_loss": 0.52, "learning_rate": 1.2e-5}
            ]
            
            print("üìä Training Progress Data:")
            print(f"  üìà Total Epochs: {len(self.engine.training_history)}")
            print(f"  üéØ Best Train Loss: {min(h['train_loss'] for h in self.engine.training_history):.4f}")
            print(f"  üéØ Best Val Loss: {min(h['val_loss'] for h in self.engine.training_history):.4f}")
            print(f"  üìâ Final Learning Rate: {self.engine.training_history[-1]['learning_rate']:.2e}")
            
            # Show training curve data
            epochs = [h['epoch'] for h in self.engine.training_history]
            train_losses = [h['train_loss'] for h in self.engine.training_history]
            val_losses = [h['val_loss'] for h in self.engine.training_history]
            
            print("\nüìâ Training Curve Data:")
            for i, (epoch, train_loss, val_loss) in enumerate(zip(epochs, train_losses, val_losses)):
                print(f"  Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}")
            
            self.demo_results["training_visualization"] = {
                "epochs": epochs,
                "train_losses": train_losses,
                "val_losses": val_losses,
                "total_epochs": len(epochs)
            }
            
            print("‚úÖ Training visualization data prepared successfully!")
            
        except Exception as e:
            print(f"‚ùå Training visualization error: {e}")
    
    def export_demo_results(self):
        """Export all demo results to JSON file."""
        print("\nüíæ Exporting Demo Results...")
        
        try:
            export_data = {
                "demo_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                "engine_config": {
                    "model_name": self.config.model_name,
                    "batch_size": self.config.batch_size,
                    "learning_rate": self.config.learning_rate,
                    "use_mixed_precision": self.config.use_mixed_precision,
                    "use_diffusion": self.config.use_diffusion
                },
                "demo_results": self.demo_results,
                "summary": {
                    "total_demos": len(self.demo_results),
                    "successful_demos": len([r for r in self.demo_results.values() if "error" not in str(r)]),
                    "failed_demos": len([r for r in self.demo_results.values() if "error" in str(r)])
                }
            }
            
            export_file = f"demo_showcase_results_{int(time.time())}.json"
            with open(export_file, 'w') as f:
                json.dump(export_data, f, indent=2)
            
            print(f"‚úÖ Demo results exported to: {export_file}")
            print(f"üìä Summary: {export_data['summary']}")
            
        except Exception as e:
            print(f"‚ùå Export failed: {e}")
    
    # Mock functions for demo purposes
    def _mock_seo_analysis(self, text, analysis_type, language):
        """Mock SEO analysis function."""
        import numpy as np
        
        results = {
            "text_length": len(text),
            "word_count": len(text.split()),
            "analysis_type": analysis_type,
            "language": language,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "processing_time": f"{np.random.uniform(0.1, 0.5):.3f}s"
        }
        
        if analysis_type == "comprehensive":
            results.update({
                "seo_score": np.random.uniform(65, 95),
                "keyword_density": np.random.uniform(1.5, 4.0),
                "readability_score": np.random.uniform(70, 90),
                "sentiment_score": np.random.uniform(-0.3, 0.8),
                "technical_seo_score": np.random.uniform(75, 95)
            })
        elif analysis_type == "keyword_density":
            words = text.lower().split()
            common_keywords = ["seo", "optimization", "content", "marketing", "digital"]
            keyword_count = sum(1 for word in words if word in common_keywords)
            results["keyword_density"] = (keyword_count / len(words)) * 100 if words else 0
        elif analysis_type == "readability":
            sentences = text.split('.')
            avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
            results["readability_score"] = max(0, 100 - avg_sentence_length * 2)
        elif analysis_type == "sentiment":
            positive_words = ["good", "great", "excellent", "amazing", "wonderful", "amazing", "incredible"]
            negative_words = ["bad", "terrible", "awful", "horrible", "worst"]
            pos_count = sum(1 for word in text.lower().split() if word in positive_words)
            neg_count = sum(1 for word in text.lower().split() if word in negative_words)
            results["sentiment_score"] = (pos_count - neg_count) / max(len(text.split()), 1)
        elif analysis_type == "technical_seo":
            results.update({
                "title_length": np.random.uniform(30, 60),
                "meta_description_length": np.random.uniform(120, 160),
                "heading_structure": "H1, H2, H3" if np.random.random() > 0.3 else "H1, H2",
                "internal_links": np.random.randint(0, 10),
                "external_links": np.random.randint(0, 5)
            })
        
        return results
    
    def _mock_diffusion_demo(self, prompt, negative_prompt, steps, guidance_scale, seed):
        """Mock diffusion demo function."""
        if not prompt.strip():
            return None, {"error": "No prompt provided"}
        
        if not self.engine.diffusion_generator:
            return None, {"error": "Diffusion models not enabled"}
        
        # Simulate generation time
        generation_time = time.time() + np.random.uniform(2.0, 8.0)
        time.sleep(0.1)  # Simulate processing
        
        metadata = {
            "prompt": prompt,
            "negative_prompt": negative_prompt,
            "steps": steps,
            "guidance_scale": guidance_scale,
            "seed": seed if seed != -1 else "random",
            "generation_time": f"{np.random.uniform(2.0, 8.0):.2f}s",
            "model": "MockStableDiffusionPipeline",
            "device": str(self.engine.device)
        }
        
        return "mock_image", metadata
    
    def _mock_batch_processing(self, texts, processing_type):
        """Mock batch processing function."""
        if not texts.strip():
            return {"error": "No texts provided"}
        
        text_list = [t.strip() for t in texts.split('\n') if t.strip()]
        if not text_list:
            return {"error": "No valid texts found"}
        
        results = {
            "processing_type": processing_type,
            "total_texts": len(text_list),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": []
        }
        
        for i, text in enumerate(text_list):
            if processing_type == "seo_optimization":
                result = {
                    "text_id": i + 1,
                    "text_preview": text[:50] + "..." if len(text) > 50 else text,
                    "seo_score": np.random.uniform(60, 95),
                    "suggestions": [
                        "Add more relevant keywords",
                        "Improve heading structure",
                        "Optimize meta description"
                    ][:np.random.randint(1, 4)]
                }
            elif processing_type == "content_generation":
                result = {
                    "text_id": i + 1,
                    "original_length": len(text),
                    "generated_content": f"Enhanced version of: {text[:30]}...",
                    "improvement_score": np.random.uniform(20, 60)
                }
            elif processing_type == "keyword_extraction":
                words = text.lower().split()
                keywords = [word for word in words if len(word) > 4 and word.isalpha()]
                result = {
                    "text_id": i + 1,
                    "extracted_keywords": keywords[:5],
                    "keyword_count": len(keywords),
                    "relevance_score": np.random.uniform(0.5, 1.0)
                }
            elif processing_type == "sentiment_analysis":
                sentiment = np.random.choice(["positive", "neutral", "negative"])
                result = {
                    "text_id": i + 1,
                    "sentiment": sentiment,
                    "confidence": np.random.uniform(0.6, 0.95),
                    "key_phrases": [text.split()[i] for i in np.random.choice(len(text.split()), 3, replace=False)]
                }
            
            results["results"].append(result)
        
        return results
    
    def _mock_performance_metrics(self):
        """Mock performance metrics function."""
        import psutil
        
        system_metrics = {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        if torch.cuda.is_available():
            system_metrics.update({
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated() / 1024**3:.2f} GB",
                "gpu_memory_cached": f"{torch.cuda.memory_reserved() / 1024**3:.2f} GB",
                "gpu_utilization": "N/A"
            })
        
        model_metrics = {
            "model_loaded": self.engine.seo_model is not None,
            "diffusion_loaded": self.engine.diffusion_generator is not None,
            "current_device": str(self.engine.device),
            "mixed_precision": self.config.use_mixed_precision,
            "batch_size": self.config.batch_size
        }
        
        if hasattr(self.engine, 'training_history') and self.engine.training_history:
            latest = self.engine.training_history[-1]
            model_metrics.update({
                "last_epoch": latest.get('epoch', 0),
                "last_train_loss": latest.get('train_loss', 0),
                "last_val_loss": latest.get('val_loss', 0),
                "learning_rate": latest.get('learning_rate', 0)
            })
        
        return system_metrics, model_metrics
    
    def _mock_add_custom_metric(self, name, value):
        """Mock custom metric function."""
        if not hasattr(self.engine, 'custom_metrics'):
            self.engine.custom_metrics = {}
        
        if not hasattr(self.engine, 'custom_metrics_history'):
            self.engine.custom_metrics_history = []
        
        self.engine.custom_metrics[name] = value
        self.engine.custom_metrics_history.append({
            "metric": name,
            "value": value,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        })
        
        return {
            "success": True,
            "metric_added": name,
            "value": value,
            "total_metrics": len(self.engine.custom_metrics)
        }
    
    def _mock_get_metrics_history(self):
        """Mock metrics history function."""
        if not hasattr(self.engine, 'custom_metrics_history'):
            return {"metrics": []}
        
        return {
            "total_metrics": len(self.engine.custom_metrics_history),
            "metrics": self.engine.custom_metrics_history[-20:]
        }
    
    async def run_complete_showcase(self):
        """Run the complete interactive demo showcase."""
        print("üéâ Welcome to the Advanced LLM SEO Engine Interactive Demo Showcase!")
        print("=" * 80)
        
        # Initialize engine
        if not await self.initialize():
            print("‚ùå Failed to initialize engine. Exiting showcase.")
            return
        
        # Run all showcases
        self.showcase_seo_analysis_demo()
        self.showcase_diffusion_demo()
        self.showcase_batch_processing_demo()
        self.showcase_performance_monitoring()
        self.showcase_training_visualization()
        
        # Export results
        self.export_demo_results()
        
        print("\nüéä Interactive Demo Showcase Completed Successfully!")
        print("=" * 80)
        print("üì± To use the full interactive Gradio interface, run:")
        print("   python advanced_llm_seo_engine.py")
        print("\nüß™ To run the test suite, execute:")
        print("   python test_interactive_demos.py")

async def main():
    """Main function to run the demo showcase."""
    showcase = InteractiveDemoShowcase()
    await showcase.run_complete_showcase()

if __name__ == "__main__":
    asyncio.run(main())
