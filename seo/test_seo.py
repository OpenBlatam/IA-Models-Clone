from typing_extensions import Literal, TypedDict
from typing import Any, List, Dict, Optional, Union, Tuple
# Constants
MAX_RETRIES = 100

import os
import sys
import json
from pathlib import Path
from features.seo.service import SEOService
from features.seo.models import SEOScrapeRequest
        import traceback
from typing import Any, List, Dict, Optional
import logging
import asyncio
#!/usr/bin/env python3
"""
Script de prueba para el servicio de an√°lisis SEO con LangChain
"""


# Agregar el directorio padre al path para importar los m√≥dulos
sys.path.append(str(Path(__file__).parent.parent.parent))


def test_seo_analysis():
    """Prueba el an√°lisis SEO con una URL de ejemplo"""
    
    # Configurar API key de OpenAI (opcional para pruebas b√°sicas)
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ö†Ô∏è  OPENAI_API_KEY no configurada. El an√°lisis ser√° b√°sico.")
    
    # URL de prueba
    test_url = "https://www.google.com"
    
    print(f"üîç Analizando SEO de: {test_url}")
    print("=" * 50)
    
    try:
        # Crear request
        request = SEOScrapeRequest(url=test_url)
        
        # Realizar an√°lisis
        response = SEOService.scrape(request)
        
        if response.success:
            data = response.data
            
            print("‚úÖ An√°lisis completado exitosamente!")
            print()
            
            # Mostrar resultados principales
            print("üìä RESULTADOS DEL AN√ÅLISIS SEO")
            print("-" * 30)
            print(f"T√≠tulo: {data.title}")
            print(f"Meta descripci√≥n: {data.meta_description[:100]}...")
            print(f"Puntuaci√≥n SEO: {data.seo_score}/100")
            print(f"Tiempo de carga: {data.load_time:.2f}s")
            print(f"Velocidad: {data.page_speed}")
            print(f"Compatible m√≥vil: {'‚úÖ' if data.mobile_friendly else '‚ùå'}")
            print()
            
            # Headers
            print("üìù ESTRUCTURA DE HEADERS")
            print("-" * 30)
            print(f"H1 tags: {len(data.h1_tags)}")
            for h1 in data.h1_tags[:3]:  # Mostrar solo los primeros 3
                print(f"  - {h1}")
            print(f"H2 tags: {len(data.h2_tags)}")
            print(f"H3 tags: {len(data.h3_tags)}")
            print()
            
            # Im√°genes y enlaces
            print("üñºÔ∏è  CONTENIDO MULTIMEDIA")
            print("-" * 30)
            print(f"Im√°genes: {len(data.images)}")
            images_with_alt = len([img for img in data.images if img.get('alt')])
            print(f"  - Con alt text: {images_with_alt}")
            print(f"Enlaces: {len(data.links)}")
            internal_links = len([link for link in data.links if link.get('is_internal')])
            print(f"  - Internos: {internal_links}")
            print(f"  - Externos: {len(data.links) - internal_links}")
            print()
            
            # Palabras clave
            if data.keywords:
                print("üîë PALABRAS CLAVE")
                print("-" * 30)
                print(f"Keywords: {', '.join(data.keywords)}")
                print()
            
            # Recomendaciones
            if data.recommendations:
                print("üí° RECOMENDACIONES")
                print("-" * 30)
                for i, rec in enumerate(data.recommendations, 1):
                    print(f"{i}. {rec}")
                print()
            
            # Problemas t√©cnicos
            if data.technical_issues:
                print("‚ö†Ô∏è  PROBLEMAS T√âCNICOS")
                print("-" * 30)
                for i, issue in enumerate(data.technical_issues, 1):
                    print(f"{i}. {issue}")
                print()
            
            # Resumen del an√°lisis
            if response.analysis_summary:
                print("üìã RESUMEN DEL AN√ÅLISIS")
                print("-" * 30)
                print(response.analysis_summary)
                print()
            
            # Tags de redes sociales
            if data.social_media_tags:
                print("üì± TAGS DE REDES SOCIALES")
                print("-" * 30)
                for key, value in data.social_media_tags.items():
                    print(f"{key}: {value[:50]}...")
                print()
            
            print("=" * 50)
            print("üéâ An√°lisis SEO completado!")
            
        else:
            print(f"‚ùå Error en el an√°lisis: {response.error}")
            
    except Exception as e:
        print(f"‚ùå Error durante la prueba: {str(e)}")
        traceback.print_exc()

def test_multiple_urls():
    """Prueba el an√°lisis con m√∫ltiples URLs"""
    
    test_urls = [
        "https://www.google.com",
        "https://www.github.com",
        "https://www.stackoverflow.com"
    ]
    
    print("üîç AN√ÅLISIS DE M√öLTIPLES URLs")
    print("=" * 50)
    
    for url in test_urls:
        print(f"\nüìä Analizando: {url}")
        print("-" * 30)
        
        try:
            request = SEOScrapeRequest(url=url)
            response = SEOService.scrape(request)
            
            if response.success:
                data = response.data
                print(f"‚úÖ Puntuaci√≥n SEO: {data.seo_score}/100")
                print(f"‚è±Ô∏è  Tiempo de carga: {data.load_time:.2f}s")
                print(f"üì± M√≥vil: {'‚úÖ' if data.mobile_friendly else '‚ùå'}")
                print(f"üìù T√≠tulo: {data.title[:50]}...")
            else:
                print(f"‚ùå Error: {response.error}")
                
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")

if __name__ == "__main__":
    print("üöÄ Iniciando pruebas del servicio SEO con LangChain")
    print()
    
    # Prueba individual
    test_seo_analysis()
    
    print("\n" + "="*50 + "\n")
    
    # Prueba m√∫ltiple (comentada para evitar demasiadas llamadas)
    # test_multiple_urls()
    
    print("‚úÖ Pruebas completadas!") 